#!/usr/bin/env python3
"""
Create a fully integrated Google Colab notebook for Multi-Agent Soccer Game
This script combines all .py files into a single executable notebook
"""

import json
import os

def create_integrated_notebook():
    """Create the complete integrated notebook"""
    
    # Read all Python files
    python_files = {
        'config.py': open('/home/user/webapp/config.py', 'r').read(),
        'physics.py': open('/home/user/webapp/physics.py', 'r').read(),
        'spaces.py': open('/home/user/webapp/spaces.py', 'r').read(),
        'rewards.py': open('/home/user/webapp/rewards.py', 'r').read(),
        'renderer.py': open('/home/user/webapp/renderer.py', 'r').read(),
        'soccer_env.py': open('/home/user/webapp/soccer_env.py', 'r').read(),
        'agents.py': open('/home/user/webapp/agents.py', 'r').read(),
        'trainers.py': open('/home/user/webapp/trainers.py', 'r').read(),
        'test_environment.py': open('/home/user/webapp/test_environment.py', 'r').read()
    }
    
    # Create notebook structure
    notebook = {
        "nbformat": 4,
        "nbformat_minor": 0,
        "metadata": {
            "colab": {
                "provenance": [],
                "gpuType": "T4",
                "collapsed_sections": []
            },
            "kernelspec": {
                "name": "python3",
                "display_name": "Python 3"
            },
            "language_info": {
                "name": "python"
            },
            "accelerator": "GPU"
        },
        "cells": []
    }
    
    # Add title and overview
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "title_cell"},
        "source": [
            "# ğŸ® Multi-Agent Soccer Game with Deep Reinforcement Learning\n",
            "## Google Colab å®Œå…¨çµ±åˆç‰ˆ\n\n",
            "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯æ·±å±¤å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ãŸãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚µãƒƒã‚«ãƒ¼ã‚²ãƒ¼ãƒ ã®å®Œå…¨çµ±åˆç‰ˆã§ã™ã€‚\n",
            "ã™ã¹ã¦ã®å¿…è¦ãªã‚³ãƒ¼ãƒ‰ãŒå˜ä¸€ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã«å«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€Google Colabã§ç›´æ¥å®Ÿè¡Œã§ãã¾ã™ã€‚\n\n",
            "### ğŸ“‹ å®Ÿè£…å†…å®¹\n",
            "- **ç’°å¢ƒ**: 2v2ã‚µãƒƒã‚«ãƒ¼ã‚²ãƒ¼ãƒ ï¼ˆPettingZooäº’æ›ï¼‰\n",
            "- **ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãªè¡çªæ¤œå‡ºã¨ãƒœãƒ¼ãƒ«ç‰©ç†\n",
            "- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**: Random, DQN, MADDPG\n",
            "- **è¦³æ¸¬ç©ºé–“**: 28æ¬¡å…ƒï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½ç½®ã€ãƒœãƒ¼ãƒ«çŠ¶æ…‹ã€ã‚²ãƒ¼ãƒ æƒ…å ±ï¼‰\n",
            "- **è¡Œå‹•ç©ºé–“**: 5æ¬¡å…ƒé€£ç¶šè¡Œå‹•ï¼ˆç§»å‹•+ã‚­ãƒƒã‚¯ï¼‰\n",
            "- **å ±é…¬ã‚·ã‚¹ãƒ†ãƒ **: å¤šç›®çš„å ±é…¬é–¢æ•°\n\n",
            "### âœ¨ ç‰¹å¾´\n",
            "1. å®Œå…¨ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
            "2. è¤‡æ•°ã®å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…\n",
            "3. å‰µç™ºçš„è¡Œå‹•ã®åˆ†æ\n",
            "4. ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n\n",
            "### ğŸ“Š å®Ÿè£…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n",
            "è©³ç´°ãªå®Ÿè£…å†…å®¹ã¯ã€Œãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚µãƒƒã‚«ãƒ¼ã‚²ãƒ¼ãƒ å®Ÿè£…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ.pdfã€ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚"
        ]
    })
    
    # Add dependency installation
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "install_header"},
        "source": ["## ğŸ“¦ 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "install_deps"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# Google Colabç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
            "!pip install -q gymnasium\n",
            "!pip install -q pettingzoo\n",
            "!pip install -q pygame\n",
            "!pip install -q torch torchvision\n",
            "!pip install -q matplotlib seaborn\n",
            "!pip install -q numpy\n",
            "!pip install -q opencv-python\n\n",
            "print(\"âœ… All dependencies installed successfully!\")"
        ]
    })
    
    # Add imports
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "import_header"},
        "source": ["## ğŸ”§ 2. åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "imports"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
            "import numpy as np\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "import torch.nn.functional as F\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "from typing import Dict, List, Tuple, Optional, Any, Union\n",
            "import json\n",
            "from collections import defaultdict, deque\n",
            "import random\n",
            "from dataclasses import dataclass\n",
            "from abc import ABC, abstractmethod\n",
            "import time\n",
            "import os\n\n",
            "# Gymnasium and PettingZoo\n",
            "import gymnasium as gym\n",
            "from gymnasium import spaces\n",
            "from pettingzoo import AECEnv\n",
            "from pettingzoo.utils import agent_selector, wrappers\n\n",
            "# Pygame (optional for rendering)\n",
            "try:\n",
            "    import pygame\n",
            "    PYGAME_AVAILABLE = True\n",
            "except ImportError:\n",
            "    PYGAME_AVAILABLE = False\n",
            "    print(\"âš ï¸ Pygame not available. Rendering disabled.\")\n\n",
            "# Set random seeds\n",
            "np.random.seed(42)\n",
            "torch.manual_seed(42)\n",
            "if torch.cuda.is_available():\n",
            "    torch.cuda.manual_seed(42)\n\n",
            "# Set matplotlib style\n",
            "plt.style.use('seaborn-v0_8-darkgrid')\n",
            "sns.set_palette(\"husl\")\n\n",
            "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
            "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
            "print(f\"âœ… Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
        ]
    })
    
    # Add configuration code
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "config_header"},
        "source": ["## âš™ï¸ 3. è¨­å®šã‚¯ãƒ©ã‚¹ (Configuration)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "config_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['config.py'] + "\n\nprint(\"âœ… Configuration classes defined successfully!\")"
    })
    
    # Add physics engine
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "physics_header"},
        "source": ["## ğŸ¯ 4. ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ (Physics Engine)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "physics_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['physics.py'] + "\n\nprint(\"âœ… Physics engine implemented successfully!\")"
    })
    
    # Add observation and action spaces
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "spaces_header"},
        "source": ["## ğŸ® 5. è¦³æ¸¬ãƒ»è¡Œå‹•ç©ºé–“ (Observation and Action Spaces)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "spaces_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['spaces.py'] + "\n\nprint(\"âœ… Observation and action spaces defined successfully!\")"
    })
    
    # Add reward system
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "rewards_header"},
        "source": ["## ğŸ† 6. å ±é…¬ã‚·ã‚¹ãƒ†ãƒ  (Reward System)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "rewards_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['rewards.py'] + "\n\nprint(\"âœ… Reward system implemented successfully!\")"
    })
    
    # Add renderer
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "renderer_header"},
        "source": ["## ğŸ¨ 7. ãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼ (Renderer)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "renderer_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['renderer.py'] + "\n\nprint(\"âœ… Renderer implemented successfully!\")"
    })
    
    # Add main environment
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "env_header"},
        "source": ["## ğŸŒ 8. ãƒ¡ã‚¤ãƒ³ç’°å¢ƒ (Main Soccer Environment)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "env_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['soccer_env.py'] + "\n\nprint(\"âœ… Soccer environment implemented successfully!\")"
    })
    
    # Add agents
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "agents_header"},
        "source": ["## ğŸ¤– 9. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£… (Agents)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "agents_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['agents.py'] + "\n\nprint(\"âœ… Agents implemented successfully!\")"
    })
    
    # Add trainers
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "trainers_header"},
        "source": ["## ğŸ“š 10. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ (Training Frameworks)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "trainers_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['trainers.py'] + "\n\nprint(\"âœ… Training frameworks implemented successfully!\")"
    })
    
    # Add test functions
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "test_header"},
        "source": ["## ğŸ§ª 11. ãƒ†ã‚¹ãƒˆé–¢æ•° (Test Functions)"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "test_code"},
        "execution_count": None,
        "outputs": [],
        "source": python_files['test_environment.py'] + "\n\nprint(\"âœ… Test functions implemented successfully!\")"
    })
    
    # Add execution section
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "execution_header"},
        "source": [
            "## ğŸš€ 12. å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (Execution Section)\n\n",
            "ä»¥ä¸‹ã®ã‚»ãƒ«ã§ç’°å¢ƒã‚’ãƒ†ã‚¹ãƒˆã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã§ãã¾ã™ã€‚"
        ]
    })
    
    # Test environment
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "test_env_header"},
        "source": ["### 12.1 ç’°å¢ƒãƒ†ã‚¹ãƒˆ"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "test_env"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# ç’°å¢ƒã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ\n",
            "print(\"ğŸ§ª Running environment tests...\")\n",
            "print(\"=\" * 60)\n\n",
            "test_passed = run_all_tests()\n\n",
            "if test_passed:\n",
            "    print(\"\\nâœ… All environment tests passed! Ready to proceed with training.\")\n",
            "else:\n",
            "    print(\"\\nâŒ Some tests failed. Please check the implementation.\")"
        ]
    })
    
    # Create environment and run baseline
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "baseline_header"},
        "source": ["### 12.2 ç’°å¢ƒä½œæˆã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "baseline_code"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# ç’°å¢ƒã®ä½œæˆ\n",
            "config = SoccerEnvironmentConfig()\n",
            "env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n",
            "print(\"ğŸ“Š Environment Configuration:\")\n",
            "print(f\"  Field Size: {config.FIELD_SIZE}\")\n",
            "print(f\"  Max Steps: {config.MAX_STEPS}\")\n",
            "print(f\"  Players per Team: {config.NUM_PLAYERS_PER_TEAM}\")\n",
            "print(f\"  Agents: {env.agents}\")\n",
            "print(f\"  Observation space: {env.observation_spaces[env.agents[0]].shape}\")\n",
            "print(f\"  Action space: {env.action_spaces[env.agents[0]].shape}\")\n\n",
            "# ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\n",
            "print(\"\\nğŸ² Running baseline with random agents...\")\n",
            "print(\"=\" * 60)\n\n",
            "random_agents = {}\n",
            "for i, agent_name in enumerate(env.agents):\n",
            "    random_agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n\n",
            "# 5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ\n",
            "episode_stats = []\n",
            "for episode in range(5):\n",
            "    env.reset()\n",
            "    episode_reward = {agent: 0 for agent in env.agents}\n",
            "    steps = 0\n",
            "    \n",
            "    while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
            "        for agent in env.agents:\n",
            "            if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n",
            "                obs = env.observe(agent)\n",
            "                action = random_agents[agent].select_action(obs, training=False)\n",
            "                env.step(action)\n",
            "                episode_reward[agent] += env.rewards.get(agent, 0)\n",
            "                steps += 1\n",
            "                \n",
            "                if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n",
            "                    break\n",
            "    \n",
            "    print(f\"Episode {episode + 1}: {steps} steps, Scores: {env.scores}, \"\n",
            "          f\"Total Reward: {sum(episode_reward.values()):.2f}\")\n",
            "    \n",
            "    episode_stats.append({\n",
            "        'episode': episode,\n",
            "        'steps': steps,\n",
            "        'scores': env.scores.copy(),\n",
            "        'total_reward': sum(episode_reward.values())\n",
            "    })\n\n",
            "# çµ±è¨ˆè¡¨ç¤º\n",
            "avg_steps = np.mean([stat['steps'] for stat in episode_stats])\n",
            "avg_reward = np.mean([stat['total_reward'] for stat in episode_stats])\n",
            "team_0_wins = sum(1 for stat in episode_stats if stat['scores'][0] > stat['scores'][1])\n",
            "team_1_wins = sum(1 for stat in episode_stats if stat['scores'][1] > stat['scores'][0])\n",
            "draws = len(episode_stats) - team_0_wins - team_1_wins\n\n",
            "print(f\"\\nğŸ“ˆ Baseline Statistics (Random Agents):\")\n",
            "print(f\"  Average Steps per Episode: {avg_steps:.1f}\")\n",
            "print(f\"  Average Total Reward: {avg_reward:.2f}\")\n",
            "print(f\"  Team 0 (Blue) Wins: {team_0_wins}\")\n",
            "print(f\"  Team 1 (Red) Wins: {team_1_wins}\")\n",
            "print(f\"  Draws: {draws}\")"
        ]
    })
    
    # DQN Training example
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "dqn_header"},
        "source": ["### 12.3 DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨“ç·´ä¾‹"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "dqn_training"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨“ç·´ä¾‹ï¼ˆçŸ­ç¸®ç‰ˆï¼‰\n",
            "print(\"ğŸ¤– Training DQN agents (demo version - 100 episodes)...\")\n",
            "print(\"=\" * 60)\n\n",
            "# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ\n",
            "training_config = TrainingConfig()\n",
            "agent_configs = {\n",
            "    'obs_dim': 28,\n",
            "    'action_dim': 9,  # é›¢æ•£è¡Œå‹•\n",
            "    'hidden_dims': (256, 128),\n",
            "    'lr': 1e-3,\n",
            "    'gamma': 0.99,\n",
            "    'epsilon': 1.0,\n",
            "    'epsilon_decay': 0.995,\n",
            "    'epsilon_min': 0.01,\n",
            "    'buffer_size': 10000,\n",
            "    'batch_size': 64\n",
            "}\n\n",
            "trainer = IndependentLearningTrainer(\n",
            "    env_config=config,\n",
            "    training_config=training_config,\n",
            "    agent_type=\"dqn\",\n",
            "    agent_configs=agent_configs\n",
            ")\n\n",
            "# 100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è¨“ç·´ã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\n",
            "results = trainer.train(num_episodes=100)\n\n",
            "# çµæœã®å¯è¦–åŒ–\n",
            "plt.figure(figsize=(12, 4))\n\n",
            "# Episode rewards\n",
            "plt.subplot(1, 3, 1)\n",
            "plt.plot(results['episode_rewards'])\n",
            "plt.title('Episode Rewards')\n",
            "plt.xlabel('Episode')\n",
            "plt.ylabel('Total Reward')\n",
            "plt.grid(True)\n\n",
            "# Episode lengths\n",
            "plt.subplot(1, 3, 2)\n",
            "plt.plot(results['episode_lengths'])\n",
            "plt.title('Episode Lengths')\n",
            "plt.xlabel('Episode')\n",
            "plt.ylabel('Steps')\n",
            "plt.grid(True)\n\n",
            "# Team scores\n",
            "plt.subplot(1, 3, 3)\n",
            "team_0_scores = [score[0] for score in results['scores_history']]\n",
            "team_1_scores = [score[1] for score in results['scores_history']]\n",
            "plt.plot(team_0_scores, label='Team 0 (Blue)', alpha=0.7)\n",
            "plt.plot(team_1_scores, label='Team 1 (Red)', alpha=0.7)\n",
            "plt.title('Team Scores')\n",
            "plt.xlabel('Episode')\n",
            "plt.ylabel('Score')\n",
            "plt.legend()\n",
            "plt.grid(True)\n\n",
            "plt.tight_layout()\n",
            "plt.show()\n\n",
            "print(\"\\nâœ… Training completed!\")"
        ]
    })
    
    # MADDPG Training example
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "maddpg_header"},
        "source": ["### 12.4 MADDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨“ç·´ä¾‹"]
    })
    
    notebook['cells'].append({
        "cell_type": "code",
        "metadata": {"id": "maddpg_training"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# MADDPGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨“ç·´ä¾‹ï¼ˆçŸ­ç¸®ç‰ˆï¼‰\n",
            "print(\"ğŸ¤– Training MADDPG agents (demo version - 100 episodes)...\")\n",
            "print(\"=\" * 60)\n\n",
            "# MADDPGè¨­å®š\n",
            "maddpg_config = MADDPGConfig()\n\n",
            "# MADDPGãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆ\n",
            "maddpg_trainer = MADDPGTrainer(\n",
            "    env_config=config,\n",
            "    training_config=training_config,\n",
            "    maddpg_config=maddpg_config\n",
            ")\n\n",
            "# 100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è¨“ç·´ã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\n",
            "maddpg_results = maddpg_trainer.train(num_episodes=100)\n\n",
            "# çµæœã®å¯è¦–åŒ–\n",
            "plt.figure(figsize=(10, 4))\n\n",
            "# Episode rewards\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.plot(maddpg_results['episode_rewards'])\n",
            "plt.title('MADDPG Episode Rewards')\n",
            "plt.xlabel('Episode')\n",
            "plt.ylabel('Total Reward')\n",
            "plt.grid(True)\n\n",
            "# Team cooperation\n",
            "plt.subplot(1, 2, 2)\n",
            "team_0_scores = [score[0] for score in maddpg_results['scores_history']]\n",
            "team_1_scores = [score[1] for score in maddpg_results['scores_history']]\n",
            "cooperation_metric = [abs(s0 - s1) for s0, s1 in zip(team_0_scores, team_1_scores)]\n",
            "plt.plot(cooperation_metric)\n",
            "plt.title('Team Balance (Lower is Better)')\n",
            "plt.xlabel('Episode')\n",
            "plt.ylabel('Score Difference')\n",
            "plt.grid(True)\n\n",
            "plt.tight_layout()\n",
            "plt.show()\n\n",
            "print(\"\\nâœ… MADDPG training completed!\")"
        ]
    })
    
    # Summary
    notebook['cells'].append({
        "cell_type": "markdown",
        "metadata": {"id": "summary"},
        "source": [
            "## ğŸ“ ã¾ã¨ã‚\n\n",
            "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n\n",
            "### âœ… å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½\n",
            "1. **å®Œå…¨ãªç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³**: è¡çªæ¤œå‡ºã€ãƒœãƒ¼ãƒ«ç‰©ç†ã€ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ç§»å‹•\n",
            "2. **PettingZooäº’æ›ç’°å¢ƒ**: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹\n",
            "3. **è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…**:\n",
            "   - Random Agent (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³)\n",
            "   - DQN Agent (ç‹¬ç«‹å­¦ç¿’)\n",
            "   - MADDPG Agent (å”èª¿å­¦ç¿’)\n",
            "4. **è±Šå¯Œãªè¦³æ¸¬ç©ºé–“**: 28æ¬¡å…ƒã®è¦³æ¸¬ï¼ˆä½ç½®ã€é€Ÿåº¦ã€ã‚²ãƒ¼ãƒ çŠ¶æ…‹ï¼‰\n",
            "5. **å¤šç›®çš„å ±é…¬ã‚·ã‚¹ãƒ†ãƒ **: ã‚´ãƒ¼ãƒ«ã€ãƒœãƒ¼ãƒ«åˆ¶å¾¡ã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯\n",
            "6. **è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: ç‹¬ç«‹å­¦ç¿’ã¨MADDPG\n\n",
            "### ğŸš€ ä»Šå¾Œã®æ‹¡å¼µ\n",
            "1. PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…\n",
            "2. QMIXã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¿½åŠ \n",
            "3. ã‚»ãƒ«ãƒ•ãƒ—ãƒ¬ã‚¤è¨“ç·´ã®å®Ÿè£…\n",
            "4. ã‚ˆã‚Šè©³ç´°ãªå‰µç™ºçš„è¡Œå‹•ã®åˆ†æ\n",
            "5. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³\n\n",
            "### ğŸ“š å‚è€ƒæ–‡çŒ®\n",
            "- [PettingZoo Documentation](https://pettingzoo.farama.org/)\n",
            "- [MADDPG Paper](https://arxiv.org/abs/1706.02275)\n",
            "- [DQN Paper](https://arxiv.org/abs/1312.5602)\n\n",
            "### ğŸ¯ å®Ÿé¨“ã®ãƒ’ãƒ³ãƒˆ\n",
            "- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§å­¦ç¿’æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã™\n",
            "- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã§ç•°ãªã‚‹æˆ¦ç•¥ãŒç”Ÿã¾ã‚Œã¾ã™\n",
            "- å ±é…¬é‡ã¿ã®èª¿æ•´ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ãŒå¤‰åŒ–ã—ã¾ã™\n\n",
            "**Happy Training! ğŸ®**"
        ]
    })
    
    return notebook

# Create and save the notebook
if __name__ == "__main__":
    notebook = create_integrated_notebook()
    
    # Save the notebook
    with open("/home/user/webapp/multiagents_soccer_colab.ipynb", "w", encoding="utf-8") as f:
        json.dump(notebook, f, indent=2, ensure_ascii=False)
    
    print("âœ… Integrated notebook created successfully!")
    print("ğŸ“ Saved as: multiagents_soccer_colab.ipynb")