{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# ğŸ® Multi-Agent Soccer Game with Deep Reinforcement Learning\n",
        "## Google Colab å®Œå…¨çµ±åˆç‰ˆ (æœ€çµ‚ä¿®æ­£ç‰ˆ)\n\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯æ·±å±¤å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ãŸãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚µãƒƒã‚«ãƒ¼ã‚²ãƒ¼ãƒ ã®å®Œå…¨çµ±åˆç‰ˆã§ã™ã€‚\n\n",
        "### âš ï¸ é‡è¦: å®Ÿè¡Œæ‰‹é †\n",
        "1. **Runtime â†’ Change runtime type â†’ GPU ã‚’é¸æŠ**\n",
        "2. **Runtime â†’ Run all ã¾ãŸã¯ä¸Šã‹ã‚‰é †ç•ªã«å®Ÿè¡Œ**\n",
        "3. **ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨**\n\n",
        "### ğŸ“‹ ä¿®æ­£å†…å®¹\n",
        "- âœ… ModuleNotFoundError ä¿®æ­£æ¸ˆã¿\n",
        "- âœ… TypeError (agent_selector) ä¿®æ­£æ¸ˆã¿\n",
        "- âœ… ã™ã¹ã¦ã®å†…éƒ¨importå‰Šé™¤æ¸ˆã¿\n\n",
        "### ğŸ¯ å®Ÿè£…å†…å®¹\n",
        "- 2v2ã‚µãƒƒã‚«ãƒ¼ã‚²ãƒ¼ãƒ ï¼ˆPettingZooäº’æ›ï¼‰\n",
        "- ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ãƒ»å ±é…¬ã‚·ã‚¹ãƒ†ãƒ å®Œå‚™\n",
        "- Random, DQN, MADDPG ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "## ğŸ“¦ Step 1: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install -q gymnasium\n",
        "!pip install -q pettingzoo\n",
        "!pip install -q pygame\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q numpy\n\n",
        "print(\"âœ… All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_header"
      },
      "source": [
        "## ğŸ”§ Step 2: åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "import json\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from abc import ABC, abstractmethod\n",
        "import time\n",
        "import os\n\n",
        "# Gymnasium and PettingZoo (ä¿®æ­£æ¸ˆã¿)\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils import AgentSelector, wrappers  # Fixed: AgentSelector instead of agent_selector\n\n",
        "# Pygame (optional)\n",
        "try:\n",
        "    import pygame\n",
        "    PYGAME_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYGAME_AVAILABLE = False\n",
        "    print(\"âš ï¸ Pygame not available. Rendering disabled.\")\n\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n\n",
        "# Matplotlib settings\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
        "print(f\"âœ… Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## âš™ï¸ Step 3: è¨­å®šã‚¯ãƒ©ã‚¹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nConfiguration file for Multi-Agent Soccer Game\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Tuple\n\n@dataclass\nclass SoccerEnvironmentConfig:\n    \"\"\"Environment configuration for soccer game\"\"\"\n    FIELD_SIZE: Tuple[int, int] = (800, 600)\n    GOAL_SIZE: Tuple[int, int] = (20, 200)\n    BALL_RADIUS: int = 10\n    PLAYER_RADIUS: int = 20\n    MAX_STEPS: int = 1000\n\n    NUM_PLAYERS_PER_TEAM: int = 2\n    TEAM_COLORS: Tuple[str, str] = ('blue', 'red')\n    PLAYER_SPEED: float = 5.0\n    BALL_SPEED_MULTIPLIER: float = 1.5\n\n    FRICTION: float = 0.95\n    BALL_DECAY: float = 0.98\n    COLLISION_THRESHOLD: float = 30.0\n\n@dataclass\nclass MADDPGConfig:\n    \"\"\"MADDPG algorithm configuration\"\"\"\n    obs_dim: int = 28\n    action_dim: int = 5\n    global_obs_dim: int = 112  # 28 * 4 agents\n    global_action_dim: int = 20  # 5 * 4 agents\n    hidden_dims: Tuple[int, ...] = (256, 128)\n\n    actor_lr: float = 1e-4\n    critic_lr: float = 1e-3\n    gamma: float = 0.95\n    tau: float = 0.01\n    batch_size: int = 256\n    buffer_size: int = int(1e6)\n    noise_scale: float = 0.1\n    noise_decay: float = 0.9999\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Training configuration\"\"\"\n    max_episodes: int = 10000\n    max_steps_per_episode: int = 1000\n    save_freq: int = 1000\n    eval_freq: int = 500\n    log_freq: int = 100\n\n    # Reproducibility\n    random_seed: int = 42\n\n@dataclass\nclass ExperimentConfig:\n    \"\"\"Experiment configuration\"\"\"\n    experiment_name: str = \"soccer_multiagent\"\n    log_dir: str = \"logs\"\n    save_dir: str = \"saved_models\"\n    video_dir: str = \"videos\"\n\n    # Algorithms to run\n    algorithms: Tuple[str, ...] = (\"random\", \"dqn\", \"ppo\", \"maddpg\")\n\nprint(\"âœ… Section completed: âš™ï¸ Step 3: è¨­å®šã‚¯ãƒ©ã‚¹\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "physics_header"
      },
      "source": [
        "## ğŸ¯ Step 4: ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "physics_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nPhysics engine for soccer game\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\nclass Ball:\n    def __init__(self, x: float, y: float, radius: float = 10):\n        self.pos = np.array([x, y], dtype=float)\n        self.vel = np.array([0.0, 0.0], dtype=float)\n        self.radius = radius\n\n    def update(self, config: SoccerEnvironmentConfig):\n        \"\"\"Update ball position with physics\"\"\"\n        # Apply velocity\n        self.pos += self.vel\n\n        # Apply ball decay (friction)\n        self.vel *= config.BALL_DECAY\n\n        # Boundary collision detection\n        field_width, field_height = config.FIELD_SIZE\n\n        # Horizontal boundaries (top/bottom)\n        if self.pos[1] <= self.radius or self.pos[1] >= field_height - self.radius:\n            self.vel[1] *= -0.8  # Energy loss on collision\n            self.pos[1] = max(self.radius, min(field_height - self.radius, self.pos[1]))\n\n        # Vertical boundaries (left/right - goals)\n        goal_top = (field_height - config.GOAL_SIZE[1]) // 2\n        goal_bottom = goal_top + config.GOAL_SIZE[1]\n\n        # Left side\n        if self.pos[0] <= self.radius:\n            if goal_top <= self.pos[1] <= goal_bottom:\n                # Goal scored\n                return \"goal_left\"\n            else:\n                self.vel[0] *= -0.8\n                self.pos[0] = self.radius\n\n        # Right side\n        elif self.pos[0] >= field_width - self.radius:\n            if goal_top <= self.pos[1] <= goal_bottom:\n                # Goal scored\n                return \"goal_right\"\n            else:\n                self.vel[0] *= -0.8\n                self.pos[0] = field_width - self.radius\n\n        return None\n\nclass Player:\n    def __init__(self, x: float, y: float, team: int, player_id: int, radius: float = 20):\n        self.pos = np.array([x, y], dtype=float)\n        self.vel = np.array([0.0, 0.0], dtype=float)\n        self.team = team\n        self.player_id = player_id\n        self.radius = radius\n        self.has_ball = False\n\n    def update(self, action: np.ndarray, config: SoccerEnvironmentConfig):\n        \"\"\"Update player position based on action\"\"\"\n        # Extract movement and kick actions\n        move_x, move_y = action[0], action[1]\n        kick_power = action[2] if len(action) > 2 else 0.0\n        kick_dir_x = action[3] if len(action) > 3 else 0.0\n        kick_dir_y = action[4] if len(action) > 4 else 0.0\n\n        # Apply movement\n        movement = np.array([move_x, move_y]) * config.PLAYER_SPEED\n        self.vel = movement\n        self.pos += self.vel\n\n        # Apply friction\n        self.vel *= config.FRICTION\n\n        # Boundary constraints\n        field_width, field_height = config.FIELD_SIZE\n        self.pos[0] = max(self.radius, min(field_width - self.radius, self.pos[0]))\n        self.pos[1] = max(self.radius, min(field_height - self.radius, self.pos[1]))\n\n        return kick_power, np.array([kick_dir_x, kick_dir_y])\n\nclass PhysicsEngine:\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n        self.ball = Ball(\n            config.FIELD_SIZE[0] // 2,\n            config.FIELD_SIZE[1] // 2,\n            config.BALL_RADIUS\n        )\n\n        # Initialize players\n        self.players = []\n        self._init_players()\n\n    def _init_players(self):\n        \"\"\"Initialize player positions\"\"\"\n        field_width, field_height = self.config.FIELD_SIZE\n\n        # Team 0 (left side - blue)\n        self.players.append(Player(field_width * 0.2, field_height * 0.3, 0, 0))\n        self.players.append(Player(field_width * 0.2, field_height * 0.7, 0, 1))\n\n        # Team 1 (right side - red)\n        self.players.append(Player(field_width * 0.8, field_height * 0.3, 1, 0))\n        self.players.append(Player(field_width * 0.8, field_height * 0.7, 1, 1))\n\n    def reset(self):\n        \"\"\"Reset physics state\"\"\"\n        self.ball.pos = np.array([\n            self.config.FIELD_SIZE[0] // 2,\n            self.config.FIELD_SIZE[1] // 2\n        ], dtype=float)\n        self.ball.vel = np.array([0.0, 0.0], dtype=float)\n\n        # Reset player positions\n        field_width, field_height = self.config.FIELD_SIZE\n        positions = [\n            (field_width * 0.2, field_height * 0.3),  # Team 0, Player 0\n            (field_width * 0.2, field_height * 0.7),  # Team 0, Player 1\n            (field_width * 0.8, field_height * 0.3),  # Team 1, Player 0\n            (field_width * 0.8, field_height * 0.7),  # Team 1, Player 1\n        ]\n\n        for i, (x, y) in enumerate(positions):\n            self.players[i].pos = np.array([x, y], dtype=float)\n            self.players[i].vel = np.array([0.0, 0.0], dtype=float)\n            self.players[i].has_ball = False\n\n    def step(self, actions: Dict[str, np.ndarray]) -> Optional[str]:\n        \"\"\"Step physics simulation\"\"\"\n        # Update players\n        kicks = {}\n        for i, player in enumerate(self.players):\n            agent_key = f\"player_{i}\"\n            if agent_key in actions:\n                kick_power, kick_dir = player.update(actions[agent_key], self.config)\n                if kick_power > 0:\n                    kicks[i] = (kick_power, kick_dir)\n\n        # Check player collisions with ball and apply kicks\n        ball_touched_by = None\n        for i, player in enumerate(self.players):\n            dist = np.linalg.norm(player.pos - self.ball.pos)\n            if dist <= player.radius + self.ball.radius:\n                ball_touched_by = i\n                player.has_ball = True\n\n                # Apply kick if player is kicking\n                if i in kicks:\n                    kick_power, kick_dir = kicks[i]\n                    kick_dir = kick_dir / (np.linalg.norm(kick_dir) + 1e-8)  # Normalize\n                    self.ball.vel += kick_dir * kick_power * self.config.BALL_SPEED_MULTIPLIER\n            else:\n                player.has_ball = False\n\n        # Update ball\n        goal_result = self.ball.update(self.config)\n\n        # Handle player-player collisions\n        self._handle_player_collisions()\n\n        return goal_result, ball_touched_by\n\n    def _handle_player_collisions(self):\n        \"\"\"Handle collisions between players\"\"\"\n        for i in range(len(self.players)):\n            for j in range(i + 1, len(self.players)):\n                p1, p2 = self.players[i], self.players[j]\n                dist = np.linalg.norm(p1.pos - p2.pos)\n\n                if dist < p1.radius + p2.radius:\n                    # Separate players\n                    direction = p1.pos - p2.pos\n                    direction = direction / (np.linalg.norm(direction) + 1e-8)\n                    overlap = (p1.radius + p2.radius) - dist\n\n                    p1.pos += direction * overlap * 0.5\n                    p2.pos -= direction * overlap * 0.5\n\n    def get_state(self) -> Dict:\n        \"\"\"Get current state of all entities\"\"\"\n        return {\n            'ball': {\n                'pos': self.ball.pos.copy(),\n                'vel': self.ball.vel.copy()\n            },\n            'players': [\n                {\n                    'pos': player.pos.copy(),\n                    'vel': player.vel.copy(),\n                    'team': player.team,\n                    'player_id': player.player_id,\n                    'has_ball': player.has_ball\n                }\n                for player in self.players\n            ]\n        }\n\nprint(\"âœ… Section completed: ğŸ¯ Step 4: ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spaces_header"
      },
      "source": [
        "## ğŸ® Step 5: è¦³æ¸¬ãƒ»è¡Œå‹•ç©ºé–“"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spaces_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nObservation and action space definitions for soccer environment\n\"\"\"\n\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom typing import Dict, List, Tuple, Union\n\nclass ObservationSpace:\n    \"\"\"\n    Observation space for each agent (28 dimensions total)\n    \"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n\n        # Observation space bounds\n        field_width, field_height = config.FIELD_SIZE\n        max_velocity = config.PLAYER_SPEED * 2  # Max possible velocity\n        max_distance = np.sqrt(field_width**2 + field_height**2)  # Diagonal distance\n\n        # Define observation bounds\n        obs_low = np.array([\n            # Self state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Ball state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Teammate state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Opponent 1 state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Opponent 2 state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Goal information (4 dims)\n            0,              # own goal distance\n            0,              # enemy goal distance\n            -np.pi,         # own goal angle\n            -np.pi,         # enemy goal angle\n\n            # Context information (4 dims)\n            -1,             # ball possession (-1: none, 0-3: player id)\n            0,              # time remaining (normalized)\n            -10,            # score difference\n            -1,             # last touch player id\n        ], dtype=np.float32)\n\n        obs_high = np.array([\n            # Self state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Ball state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Teammate state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Opponent 1 state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Opponent 2 state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Goal information (4 dims)\n            max_distance,   # own goal distance\n            max_distance,   # enemy goal distance\n            np.pi,          # own goal angle\n            np.pi,          # enemy goal angle\n\n            # Context information (4 dims)\n            3,              # ball possession (player 0-3)\n            1,              # time remaining (normalized)\n            10,             # score difference\n            3,              # last touch player id\n        ], dtype=np.float32)\n\n        self.gym_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n\n    def create_observation(self, agent_id: int, state: Dict,\n                         scores: Tuple[int, int], step: int,\n                         max_steps: int, ball_possession: int = -1,\n                         last_touch: int = -1) -> np.ndarray:\n        \"\"\"Create observation for specific agent\"\"\"\n\n        field_width, field_height = self.config.FIELD_SIZE\n        players = state['players']\n        ball = state['ball']\n\n        # Get agent info\n        agent = players[agent_id]\n        agent_team = agent['team']\n        agent_pos = agent['pos'] / np.array([field_width, field_height])  # Normalize\n        agent_vel = agent['vel'] / self.config.PLAYER_SPEED  # Normalize\n\n        # Get ball info\n        ball_pos = ball['pos'] / np.array([field_width, field_height])  # Normalize\n        ball_vel = ball['vel'] / self.config.PLAYER_SPEED  # Normalize\n\n        # Get teammate and opponents\n        teammates = [p for i, p in enumerate(players)\n                    if p['team'] == agent_team and i != agent_id]\n        opponents = [p for p in players if p['team'] != agent_team]\n\n        teammate = teammates[0] if teammates else agent  # fallback\n        teammate_pos = teammate['pos'] / np.array([field_width, field_height])\n        teammate_vel = teammate['vel'] / self.config.PLAYER_SPEED\n\n        # Opponents\n        opp1 = opponents[0] if len(opponents) > 0 else agent\n        opp2 = opponents[1] if len(opponents) > 1 else agent\n\n        opp1_pos = opp1['pos'] / np.array([field_width, field_height])\n        opp1_vel = opp1['vel'] / self.config.PLAYER_SPEED\n\n        opp2_pos = opp2['pos'] / np.array([field_width, field_height])\n        opp2_vel = opp2['vel'] / self.config.PLAYER_SPEED\n\n        # Goal information\n        if agent_team == 0:  # Blue team (left side)\n            own_goal_pos = np.array([0, 0.5])\n            enemy_goal_pos = np.array([1, 0.5])\n        else:  # Red team (right side)\n            own_goal_pos = np.array([1, 0.5])\n            enemy_goal_pos = np.array([0, 0.5])\n\n        own_goal_dist = np.linalg.norm(agent_pos - own_goal_pos)\n        enemy_goal_dist = np.linalg.norm(agent_pos - enemy_goal_pos)\n\n        # Goal angles\n        own_goal_vec = own_goal_pos - agent_pos\n        enemy_goal_vec = enemy_goal_pos - agent_pos\n\n        own_goal_angle = np.arctan2(own_goal_vec[1], own_goal_vec[0])\n        enemy_goal_angle = np.arctan2(enemy_goal_vec[1], enemy_goal_vec[0])\n\n        # Context information\n        time_remaining = (max_steps - step) / max_steps\n        score_diff = scores[agent_team] - scores[1 - agent_team]\n\n        # Construct observation\n        observation = np.concatenate([\n            # Self state\n            agent_pos, agent_vel,\n\n            # Ball state\n            ball_pos, ball_vel,\n\n            # Teammate state\n            teammate_pos, teammate_vel,\n\n            # Opponent states\n            opp1_pos, opp1_vel,\n            opp2_pos, opp2_vel,\n\n            # Goal information\n            [own_goal_dist, enemy_goal_dist, own_goal_angle, enemy_goal_angle],\n\n            # Context information\n            [ball_possession, time_remaining, score_diff, last_touch]\n        ]).astype(np.float32)\n\n        return observation\n\nclass ActionSpace:\n    \"\"\"\n    Action space for each agent\n    \"\"\"\n    def __init__(self, action_type: str = \"continuous\"):\n        self.action_type = action_type\n\n        if action_type == \"continuous\":\n            # 5-dimensional continuous action space\n            # [move_x, move_y, kick_power, kick_dir_x, kick_dir_y]\n            self.gym_space = spaces.Box(\n                low=np.array([-1, -1, 0, -1, -1], dtype=np.float32),\n                high=np.array([1, 1, 1, 1, 1], dtype=np.float32),\n                dtype=np.float32\n            )\n        else:\n            # 9-dimensional discrete action space\n            # [NOOP, UP, DOWN, LEFT, RIGHT, KICK_UP, KICK_DOWN, KICK_LEFT, KICK_RIGHT]\n            self.gym_space = spaces.Discrete(9)\n\n        self.action_meanings = {\n            0: \"NOOP\",\n            1: \"UP\",\n            2: \"DOWN\",\n            3: \"LEFT\",\n            4: \"RIGHT\",\n            5: \"KICK_UP\",\n            6: \"KICK_DOWN\",\n            7: \"KICK_LEFT\",\n            8: \"KICK_RIGHT\"\n        }\n\n    def sample(self) -> Union[np.ndarray, int]:\n        \"\"\"Sample a random action\"\"\"\n        return self.gym_space.sample()\n\n    def convert_discrete_to_continuous(self, action: int) -> np.ndarray:\n        \"\"\"Convert discrete action to continuous action format\"\"\"\n        action_map = {\n            0: np.array([0, 0, 0, 0, 0]),        # NOOP\n            1: np.array([0, -1, 0, 0, 0]),       # UP\n            2: np.array([0, 1, 0, 0, 0]),        # DOWN\n            3: np.array([-1, 0, 0, 0, 0]),       # LEFT\n            4: np.array([1, 0, 0, 0, 0]),        # RIGHT\n            5: np.array([0, 0, 0.5, 0, -1]),     # KICK_UP\n            6: np.array([0, 0, 0.5, 0, 1]),      # KICK_DOWN\n            7: np.array([0, 0, 0.5, -1, 0]),     # KICK_LEFT\n            8: np.array([0, 0, 0.5, 1, 0]),      # KICK_RIGHT\n        }\n\n        return action_map.get(action, action_map[0]).astype(np.float32)\n\ndef create_spaces(config: SoccerEnvironmentConfig, action_type: str = \"continuous\"):\n    \"\"\"Create observation and action spaces\"\"\"\n    obs_space = ObservationSpace(config)\n    action_space = ActionSpace(action_type)\n\n    return obs_space, action_space\n\nprint(\"âœ… Section completed: ğŸ® Step 5: è¦³æ¸¬ãƒ»è¡Œå‹•ç©ºé–“\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rewards_header"
      },
      "source": [
        "## ğŸ† Step 6: å ±é…¬ã‚·ã‚¹ãƒ†ãƒ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rewards_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nReward system for soccer environment with multi-objective reward function\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\nclass RewardCalculator:\n    \"\"\"\n    Multi-objective reward function for soccer agents\n    \"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n        self.field_width, self.field_height = config.FIELD_SIZE\n\n        # Reward weights\n        self.reward_weights = {\n            'goal_scored': 100.0,           # Goal scored by team\n            'goal_conceded': -100.0,        # Goal conceded by team\n            'ball_touch': 5.0,              # Touching the ball\n            'goal_approach': 0.1,           # Moving closer to enemy goal\n            'ball_approach': 0.05,          # Moving closer to ball\n            'teamwork': 0.02,               # Team coordination\n            'out_of_bounds': -10.0,         # Going out of bounds\n            'stalemate': -0.1,              # Stalemate penalty\n            'ball_possession': 0.01,        # Keeping ball possession\n            'defensive_positioning': 0.01,   # Good defensive position\n        }\n\n        # Track previous states for delta calculations\n        self.prev_states = {}\n\n    def calculate_reward(self, agent_id: int, action: np.ndarray,\n                        prev_state: Dict, current_state: Dict,\n                        goal_scored: Optional[str] = None,\n                        ball_touched_by: Optional[int] = None,\n                        scores: Tuple[int, int] = (0, 0),\n                        out_of_bounds_agents: List[int] = None) -> float:\n        \"\"\"\n        Calculate multi-objective reward for agent\n        \"\"\"\n        reward = 0.0\n        agent_team = current_state['players'][agent_id]['team']\n        out_of_bounds_agents = out_of_bounds_agents or []\n\n        # 1. Goal rewards (most important)\n        if goal_scored:\n            if goal_scored == \"goal_left\" and agent_team == 1:  # Red team scored\n                reward += self.reward_weights['goal_scored']\n            elif goal_scored == \"goal_right\" and agent_team == 0:  # Blue team scored\n                reward += self.reward_weights['goal_scored']\n            elif goal_scored == \"goal_left\" and agent_team == 0:  # Blue conceded\n                reward += self.reward_weights['goal_conceded']\n            elif goal_scored == \"goal_right\" and agent_team == 1:  # Red conceded\n                reward += self.reward_weights['goal_conceded']\n\n        # 2. Ball contact reward\n        if ball_touched_by == agent_id:\n            reward += self.reward_weights['ball_touch']\n\n        # 3. Goal approach reward\n        goal_approach_reward = self.calculate_goal_approach_reward(\n            agent_id, prev_state, current_state\n        )\n        reward += goal_approach_reward * self.reward_weights['goal_approach']\n\n        # 4. Ball approach reward\n        ball_approach_reward = self.calculate_ball_approach_reward(\n            agent_id, prev_state, current_state\n        )\n        reward += ball_approach_reward * self.reward_weights['ball_approach']\n\n        # 5. Teamwork reward\n        teamwork_reward = self.calculate_teamwork_reward(agent_id, current_state)\n        reward += teamwork_reward * self.reward_weights['teamwork']\n\n        # 6. Penalties\n        if agent_id in out_of_bounds_agents:\n            reward += self.reward_weights['out_of_bounds']\n\n        if self.is_stalemate(current_state):\n            reward += self.reward_weights['stalemate']\n\n        # 7. Ball possession reward\n        if current_state['players'][agent_id]['has_ball']:\n            reward += self.reward_weights['ball_possession']\n\n        # 8. Defensive positioning reward\n        defensive_reward = self.calculate_defensive_positioning_reward(\n            agent_id, current_state\n        )\n        reward += defensive_reward * self.reward_weights['defensive_positioning']\n\n        return reward\n\n    def calculate_goal_approach_reward(self, agent_id: int, prev_state: Dict,\n                                     current_state: Dict) -> float:\n        \"\"\"Calculate reward for approaching enemy goal\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n        current_pos = current_state['players'][agent_id]['pos']\n        prev_pos = prev_state['players'][agent_id]['pos']\n\n        # Enemy goal position\n        if agent_team == 0:  # Blue team\n            enemy_goal_pos = np.array([self.field_width, self.field_height / 2])\n        else:  # Red team\n            enemy_goal_pos = np.array([0, self.field_height / 2])\n\n        prev_dist = np.linalg.norm(prev_pos - enemy_goal_pos)\n        current_dist = np.linalg.norm(current_pos - enemy_goal_pos)\n\n        return prev_dist - current_dist  # Positive if getting closer\n\n    def calculate_ball_approach_reward(self, agent_id: int, prev_state: Dict,\n                                     current_state: Dict) -> float:\n        \"\"\"Calculate reward for approaching ball\"\"\"\n        current_pos = current_state['players'][agent_id]['pos']\n        prev_pos = prev_state['players'][agent_id]['pos']\n        ball_pos = current_state['ball']['pos']\n\n        prev_dist = np.linalg.norm(prev_pos - ball_pos)\n        current_dist = np.linalg.norm(current_pos - ball_pos)\n\n        return prev_dist - current_dist  # Positive if getting closer\n\n    def calculate_teamwork_reward(self, agent_id: int, current_state: Dict) -> float:\n        \"\"\"Calculate teamwork reward based on team coordination\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n        players = current_state['players']\n\n        # Find teammate\n        teammates = [p for i, p in enumerate(players)\n                    if p['team'] == agent_team and i != agent_id]\n\n        if not teammates:\n            return 0.0\n\n        teammate = teammates[0]\n        agent_pos = current_state['players'][agent_id]['pos']\n        teammate_pos = teammate['pos']\n\n        # Optimal distance between teammates (100-200 pixels)\n        teammate_dist = np.linalg.norm(agent_pos - teammate_pos)\n        optimal_dist = 150\n        dist_penalty = abs(teammate_dist - optimal_dist) / optimal_dist\n\n        return 1.0 - dist_penalty  # Higher reward for optimal distance\n\n    def calculate_defensive_positioning_reward(self, agent_id: int,\n                                             current_state: Dict) -> float:\n        \"\"\"Calculate reward for good defensive positioning\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n        agent_pos = current_state['players'][agent_id]['pos']\n        ball_pos = current_state['ball']['pos']\n\n        # Own goal position\n        if agent_team == 0:  # Blue team\n            own_goal_pos = np.array([0, self.field_height / 2])\n        else:  # Red team\n            own_goal_pos = np.array([self.field_width, self.field_height / 2])\n\n        # Reward for being between ball and own goal\n        goal_to_ball = ball_pos - own_goal_pos\n        goal_to_agent = agent_pos - own_goal_pos\n\n        # Project agent position onto goal-ball line\n        if np.linalg.norm(goal_to_ball) > 0:\n            projection = np.dot(goal_to_agent, goal_to_ball) / np.linalg.norm(goal_to_ball)\n            ball_dist = np.linalg.norm(goal_to_ball)\n\n            # Reward if agent is between goal and ball\n            if 0 < projection < ball_dist:\n                return 1.0\n\n        return 0.0\n\n    def is_stalemate(self, state: Dict, threshold: float = 1.0) -> bool:\n        \"\"\"Check if the game is in a stalemate (low activity)\"\"\"\n        ball_speed = np.linalg.norm(state['ball']['vel'])\n        player_speeds = [np.linalg.norm(p['vel']) for p in state['players']]\n        avg_player_speed = np.mean(player_speeds)\n\n        return ball_speed < threshold and avg_player_speed < threshold\n\n    def get_team_reward(self, team: int, individual_rewards: Dict[int, float]) -> float:\n        \"\"\"Calculate team reward from individual rewards\"\"\"\n        team_players = [i for i in individual_rewards.keys()\n                       if i // 2 == team]  # Assuming 2 players per team\n        return sum(individual_rewards[i] for i in team_players) / len(team_players)\n\nclass RewardShaper:\n    \"\"\"\n    Advanced reward shaping techniques\n    \"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n        self.reward_calculator = RewardCalculator(config)\n\n    def shaped_reward(self, agent_id: int, action: np.ndarray,\n                     prev_state: Dict, current_state: Dict,\n                     **kwargs) -> float:\n        \"\"\"Apply reward shaping for better learning\"\"\"\n        base_reward = self.reward_calculator.calculate_reward(\n            agent_id, action, prev_state, current_state, **kwargs\n        )\n\n        # Potential-based reward shaping\n        potential_reward = self.calculate_potential_based_reward(\n            agent_id, prev_state, current_state\n        )\n\n        return base_reward + potential_reward\n\n    def calculate_potential_based_reward(self, agent_id: int,\n                                       prev_state: Dict, current_state: Dict) -> float:\n        \"\"\"Calculate potential-based shaped reward\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n\n        # Potential functions\n        prev_potential = self.calculate_potential(agent_id, prev_state)\n        current_potential = self.calculate_potential(agent_id, current_state)\n\n        # Potential-based shaping: F(s,a,s') = Î³Î¦(s') - Î¦(s)\n        gamma = 0.99  # Discount factor\n        return gamma * current_potential - prev_potential\n\n    def calculate_potential(self, agent_id: int, state: Dict) -> float:\n        \"\"\"Calculate potential function value\"\"\"\n        agent_team = state['players'][agent_id]['team']\n        agent_pos = state['players'][agent_id]['pos']\n        ball_pos = state['ball']['pos']\n\n        # Potential based on distance to ball and enemy goal\n        if agent_team == 0:  # Blue team\n            enemy_goal_pos = np.array([self.config.FIELD_SIZE[0], self.config.FIELD_SIZE[1] / 2])\n        else:  # Red team\n            enemy_goal_pos = np.array([0, self.config.FIELD_SIZE[1] / 2])\n\n        ball_dist = np.linalg.norm(agent_pos - ball_pos)\n        goal_dist = np.linalg.norm(ball_pos - enemy_goal_pos)\n\n        # Potential decreases with distance (encouraging approach)\n        potential = -0.001 * ball_dist - 0.001 * goal_dist\n\n        return potential\n\nprint(\"âœ… Section completed: ğŸ† Step 6: å ±é…¬ã‚·ã‚¹ãƒ†ãƒ \")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "renderer_header"
      },
      "source": [
        "## ğŸ¨ Step 7: ãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renderer_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nRenderer for soccer game visualization using pygame\n\"\"\"\n\nimport pygame\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\nclass SoccerRenderer:\n    def __init__(self, config: SoccerEnvironmentConfig, window_size: Tuple[int, int] = None):\n        self.config = config\n        self.window_size = window_size or config.FIELD_SIZE\n\n        pygame.init()\n        self.screen = pygame.display.set_mode(self.window_size)\n        pygame.display.set_caption(\"Multi-Agent Soccer Game\")\n\n        # Colors\n        self.colors = {\n            'field': (0, 128, 0),        # Green\n            'field_lines': (255, 255, 255),  # White\n            'ball': (255, 255, 255),     # White\n            'team_0': (0, 0, 255),       # Blue\n            'team_1': (255, 0, 0),       # Red\n            'goal': (128, 128, 128),     # Gray\n            'background': (0, 64, 0),    # Dark green\n        }\n\n        self.font = pygame.font.Font(None, 36)\n        self.clock = pygame.time.Clock()\n\n    def render(self, state: Dict, scores: Tuple[int, int] = (0, 0), step: int = 0) -> bool:\n        \"\"\"\n        Render the current state\n        Returns True if rendering should continue, False if window was closed\n        \"\"\"\n        # Handle pygame events\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                return False\n\n        # Clear screen\n        self.screen.fill(self.colors['background'])\n\n        # Draw field\n        self._draw_field()\n\n        # Draw players\n        self._draw_players(state['players'])\n\n        # Draw ball\n        self._draw_ball(state['ball'])\n\n        # Draw UI\n        self._draw_ui(scores, step)\n\n        pygame.display.flip()\n        self.clock.tick(60)  # 60 FPS\n\n        return True\n\n    def _draw_field(self):\n        \"\"\"Draw soccer field with goals and center line\"\"\"\n        field_width, field_height = self.config.FIELD_SIZE\n\n        # Field background\n        field_rect = pygame.Rect(0, 0, field_width, field_height)\n        pygame.draw.rect(self.screen, self.colors['field'], field_rect)\n\n        # Field border\n        pygame.draw.rect(self.screen, self.colors['field_lines'], field_rect, 3)\n\n        # Center line\n        center_x = field_width // 2\n        pygame.draw.line(self.screen, self.colors['field_lines'],\n                        (center_x, 0), (center_x, field_height), 3)\n\n        # Center circle\n        pygame.draw.circle(self.screen, self.colors['field_lines'],\n                          (center_x, field_height // 2), 100, 3)\n\n        # Goals\n        goal_width, goal_height = self.config.GOAL_SIZE\n        goal_top = (field_height - goal_height) // 2\n        goal_bottom = goal_top + goal_height\n\n        # Left goal\n        left_goal = pygame.Rect(-goal_width//2, goal_top, goal_width, goal_height)\n        pygame.draw.rect(self.screen, self.colors['goal'], left_goal)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], left_goal, 3)\n\n        # Right goal\n        right_goal = pygame.Rect(field_width - goal_width//2, goal_top, goal_width, goal_height)\n        pygame.draw.rect(self.screen, self.colors['goal'], right_goal)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], right_goal, 3)\n\n        # Goal areas (penalty boxes)\n        penalty_width, penalty_height = 120, 200\n        penalty_top = (field_height - penalty_height) // 2\n\n        # Left penalty box\n        left_penalty = pygame.Rect(0, penalty_top, penalty_width, penalty_height)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], left_penalty, 2)\n\n        # Right penalty box\n        right_penalty = pygame.Rect(field_width - penalty_width, penalty_top,\n                                   penalty_width, penalty_height)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], right_penalty, 2)\n\n    def _draw_players(self, players: List[Dict]):\n        \"\"\"Draw all players\"\"\"\n        for i, player in enumerate(players):\n            pos = player['pos']\n            team = player['team']\n            has_ball = player['has_ball']\n\n            color = self.colors[f'team_{team}']\n\n            # Draw player circle\n            pygame.draw.circle(self.screen, color, pos.astype(int), self.config.PLAYER_RADIUS)\n\n            # Draw player outline\n            outline_color = (255, 255, 255) if has_ball else (0, 0, 0)\n            outline_width = 4 if has_ball else 2\n            pygame.draw.circle(self.screen, outline_color, pos.astype(int),\n                             self.config.PLAYER_RADIUS, outline_width)\n\n            # Draw player number\n            player_text = self.font.render(str(player['player_id']), True, (255, 255, 255))\n            text_rect = player_text.get_rect(center=pos.astype(int))\n            self.screen.blit(player_text, text_rect)\n\n    def _draw_ball(self, ball: Dict):\n        \"\"\"Draw the ball\"\"\"\n        pos = ball['pos']\n        pygame.draw.circle(self.screen, self.colors['ball'], pos.astype(int), self.config.BALL_RADIUS)\n        pygame.draw.circle(self.screen, (0, 0, 0), pos.astype(int), self.config.BALL_RADIUS, 2)\n\n        # Draw ball velocity vector (for debugging)\n        vel = ball['vel']\n        if np.linalg.norm(vel) > 0.1:\n            end_pos = pos + vel * 10  # Scale for visibility\n            pygame.draw.line(self.screen, (255, 255, 0), pos.astype(int), end_pos.astype(int), 2)\n\n    def _draw_ui(self, scores: Tuple[int, int], step: int):\n        \"\"\"Draw game UI (scores, step counter)\"\"\"\n        # Score display\n        score_text = f\"Blue: {scores[0]}  Red: {scores[1]}\"\n        score_surface = self.font.render(score_text, True, (255, 255, 255))\n        self.screen.blit(score_surface, (10, 10))\n\n        # Step counter\n        step_text = f\"Step: {step}\"\n        step_surface = self.font.render(step_text, True, (255, 255, 255))\n        step_rect = step_surface.get_rect()\n        step_rect.topright = (self.window_size[0] - 10, 10)\n        self.screen.blit(step_surface, step_rect)\n\n    def close(self):\n        \"\"\"Close the renderer\"\"\"\n        pygame.quit()\n\n    def save_frame(self, filename: str):\n        \"\"\"Save current frame as image\"\"\"\n        pygame.image.save(self.screen, filename)\n\nclass VideoRecorder:\n    \"\"\"Record gameplay videos\"\"\"\n    def __init__(self, filename: str, fps: int = 30):\n        self.filename = filename\n        self.fps = fps\n        self.frames = []\n\n    def add_frame(self, surface):\n        \"\"\"Add a frame to the video\"\"\"\n        frame_array = pygame.surfarray.array3d(surface)\n        frame_array = np.transpose(frame_array, (1, 0, 2))  # Correct orientation\n        self.frames.append(frame_array)\n\n    def save_video(self):\n        \"\"\"Save recorded frames as video (requires opencv)\"\"\"\n        if not self.frames:\n            return\n\n        try:\n            import cv2\n            height, width, _ = self.frames[0].shape\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            out = cv2.VideoWriter(self.filename, fourcc, self.fps, (width, height))\n\n            for frame in self.frames:\n                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n                out.write(frame_bgr)\n\n            out.release()\n            print(f\"Video saved as {self.filename}\")\n        except ImportError:\n            print(\"OpenCV not available. Cannot save video.\")\n\n    def clear(self):\n        \"\"\"Clear recorded frames\"\"\"\n        self.frames = []\n\nprint(\"âœ… Section completed: ğŸ¨ Step 7: ãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_header"
      },
      "source": [
        "## ğŸŒ Step 8: ãƒ¡ã‚¤ãƒ³ç’°å¢ƒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nMain Soccer Environment - PettingZoo compatible multi-agent environment\n\"\"\"\n\nimport numpy as np\nimport gymnasium as gym\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom pettingzoo import AECEnv\nfrom pettingzoo.utils import AgentSelector, wrappers\n\n\nclass SoccerEnvironment(AECEnv):\n    \"\"\"\n    PettingZoo-compatible soccer environment for multi-agent reinforcement learning\n    \"\"\"\n\n    metadata = {\n        \"render_modes\": [\"human\", \"rgb_array\"],\n        \"name\": \"soccer_v1\"\n    }\n\n    def __init__(self, config: SoccerEnvironmentConfig = None,\n                 training_config: TrainingConfig = None,\n                 render_mode: str = None,\n                 action_type: str = \"continuous\"):\n        \"\"\"\n        Initialize soccer environment\n\n        Args:\n            config: Environment configuration\n            training_config: Training configuration\n            render_mode: Rendering mode (\"human\", \"rgb_array\", or None)\n            action_type: Action space type (\"continuous\" or \"discrete\")\n        \"\"\"\n        super().__init__()\n\n        self.config = config or SoccerEnvironmentConfig()\n        self.training_config = training_config or TrainingConfig()\n        self.render_mode = render_mode\n        self.action_type = action_type\n\n        # Initialize components\n        self.physics = PhysicsEngine(self.config)\n        self.reward_calculator = RewardCalculator(self.config)\n        self.reward_shaper = RewardShaper(self.config)\n\n        # Initialize spaces\n        self.observation_space_handler = ObservationSpace(self.config)\n        self.action_space_handler = ActionSpace(action_type)\n\n        # Agent setup\n        self.possible_agents = [f\"player_{i}\" for i in range(4)]\n        self.agents = self.possible_agents[:]\n\n        # Observation and action spaces\n        self.observation_spaces = {\n            agent: self.observation_space_handler.gym_space\n            for agent in self.possible_agents\n        }\n        self.action_spaces = {\n            agent: self.action_space_handler.gym_space\n            for agent in self.possible_agents\n        }\n\n        # Agent selector for turn-based execution\n        self._agent_selector = AgentSelector(self.agents)\n\n        # Initialize renderer if needed\n        self.renderer = None\n        if self.render_mode == \"human\":\n            self.renderer = SoccerRenderer(self.config)\n\n        # Game state\n        self.reset()\n\n    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None):\n        \"\"\"Reset the environment\"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Reset physics\n        self.physics.reset()\n\n        # Reset agents\n        self.agents = self.possible_agents[:]\n        self._agent_selector = AgentSelector(self.agents)\n        self.agent_selection = self._agent_selector.next()\n\n        # Reset game state\n        self.step_count = 0\n        self.scores = [0, 0]  # [team_0, team_1]\n        self.episode_terminated = False\n        self.episode_truncated = False\n\n        # Reset rewards and info\n        self.rewards = {agent: 0.0 for agent in self.agents}\n        self.terminations = {agent: False for agent in self.agents}\n        self.truncations = {agent: False for agent in self.agents}\n        self.infos = {agent: {} for agent in self.agents}\n\n        # Store previous state for reward calculation\n        self.prev_state = None\n        self.ball_possession = -1  # -1: no possession, 0-3: player id\n        self.last_touch = -1\n\n        return self._get_observations()\n\n    def step(self, action: Any):\n        \"\"\"Execute one step in the environment\"\"\"\n        if self.episode_terminated or self.episode_truncated:\n            return self._was_dead_step(action)\n\n        agent_id = int(self.agent_selection.split('_')[1])\n\n        # Store previous state\n        if self.prev_state is None:\n            self.prev_state = self.physics.get_state()\n\n        # Convert discrete action to continuous if needed\n        if self.action_type == \"discrete\" and isinstance(action, (int, np.integer)):\n            action = self.action_space_handler.convert_discrete_to_continuous(action)\n\n        # Execute action in physics\n        actions = {self.agent_selection: action}\n        goal_result, ball_touched_by = self.physics.step(actions)\n\n        # Update ball possession and last touch\n        if ball_touched_by is not None:\n            self.ball_possession = ball_touched_by\n            self.last_touch = ball_touched_by\n\n        # Handle goal scoring\n        if goal_result:\n            if goal_result == \"goal_left\":\n                self.scores[1] += 1  # Red team scored\n            elif goal_result == \"goal_right\":\n                self.scores[0] += 1  # Blue team scored\n\n        # Get current state\n        current_state = self.physics.get_state()\n\n        # Calculate rewards\n        self._calculate_rewards(agent_id, action, self.prev_state, current_state,\n                              goal_result, ball_touched_by)\n\n        # Update step count\n        self.step_count += 1\n\n        # Check termination conditions\n        self._check_termination()\n\n        # Move to next agent\n        self.agent_selection = self._agent_selector.next()\n\n        # Update previous state\n        self.prev_state = current_state\n\n    def _calculate_rewards(self, agent_id: int, action: np.ndarray,\n                          prev_state: Dict, current_state: Dict,\n                          goal_result: Optional[str], ball_touched_by: Optional[int]):\n        \"\"\"Calculate rewards for all agents\"\"\"\n        # Reset rewards for this step\n        self.rewards = {agent: 0.0 for agent in self.agents}\n\n        # Calculate rewards for each agent\n        for i, agent in enumerate(self.agents):\n            reward = self.reward_shaper.shaped_reward(\n                i, action if i == agent_id else np.zeros(5),\n                prev_state, current_state,\n                goal_scored=goal_result,\n                ball_touched_by=ball_touched_by,\n                scores=tuple(self.scores)\n            )\n            self.rewards[agent] = reward\n\n    def _check_termination(self):\n        \"\"\"Check if episode should terminate\"\"\"\n        # Game ends if max steps reached\n        if self.step_count >= self.config.MAX_STEPS:\n            self.episode_truncated = True\n\n        # Game ends if goal difference is too large (optional)\n        goal_diff = abs(self.scores[0] - self.scores[1])\n        if goal_diff >= 5:  # End early if one team is dominating\n            self.episode_terminated = True\n\n        # Update termination/truncation for all agents\n        if self.episode_terminated or self.episode_truncated:\n            for agent in self.agents:\n                self.terminations[agent] = self.episode_terminated\n                self.truncations[agent] = self.episode_truncated\n\n    def _get_observations(self) -> Dict[str, np.ndarray]:\n        \"\"\"Get observations for all agents\"\"\"\n        current_state = self.physics.get_state()\n        observations = {}\n\n        for i, agent in enumerate(self.agents):\n            obs = self.observation_space_handler.create_observation(\n                i, current_state, tuple(self.scores), self.step_count,\n                self.config.MAX_STEPS, self.ball_possession, self.last_touch\n            )\n            observations[agent] = obs\n\n        return observations\n\n    def observe(self, agent: str) -> np.ndarray:\n        \"\"\"Get observation for specific agent\"\"\"\n        agent_id = int(agent.split('_')[1])\n        current_state = self.physics.get_state()\n\n        return self.observation_space_handler.create_observation(\n            agent_id, current_state, tuple(self.scores), self.step_count,\n            self.config.MAX_STEPS, self.ball_possession, self.last_touch\n        )\n\n    def render(self):\n        \"\"\"Render the environment\"\"\"\n        if self.render_mode == \"human\" and self.renderer:\n            current_state = self.physics.get_state()\n            return self.renderer.render(current_state, tuple(self.scores), self.step_count)\n        elif self.render_mode == \"rgb_array\":\n            # Return RGB array for recording\n            if not self.renderer:\n                self.renderer = SoccerRenderer(self.config)\n            current_state = self.physics.get_state()\n            self.renderer.render(current_state, tuple(self.scores), self.step_count)\n            # Convert pygame surface to numpy array\n            import pygame\n            rgb_array = pygame.surfarray.array3d(self.renderer.screen)\n            return np.transpose(rgb_array, (1, 0, 2))\n\n    def close(self):\n        \"\"\"Clean up resources\"\"\"\n        if self.renderer:\n            self.renderer.close()\n\n    def state(self) -> np.ndarray:\n        \"\"\"Get global state (concatenated observations)\"\"\"\n        observations = self._get_observations()\n        return np.concatenate([observations[agent] for agent in self.agents])\n\n    def _was_dead_step(self, action):\n        \"\"\"Handle action taken when episode is over\"\"\"\n        # This method is required by PettingZoo but not used in our implementation\n        pass\n\n# Wrapper functions for easier usage\n\ndef make_soccer_env(config: SoccerEnvironmentConfig = None,\n                   render_mode: str = None,\n                   action_type: str = \"continuous\") -> SoccerEnvironment:\n    \"\"\"Create soccer environment with default settings\"\"\"\n    return SoccerEnvironment(config, render_mode=render_mode, action_type=action_type)\n\ndef make_parallel_soccer_env(config: SoccerEnvironmentConfig = None,\n                            render_mode: str = None,\n                            action_type: str = \"continuous\"):\n    \"\"\"Create parallel version of soccer environment\"\"\"\n    from pettingzoo.utils import parallel_to_aec\n    env = make_soccer_env(config, render_mode, action_type)\n    return parallel_to_aec(env)\n\n# Compatibility with stable-baselines3\nclass SB3SoccerEnv:\n    \"\"\"Stable-Baselines3 compatible wrapper\"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig = None,\n                 action_type: str = \"continuous\"):\n        self.env = make_soccer_env(config, action_type=action_type)\n        self.agents = self.env.agents\n        self.num_agents = len(self.agents)\n\n        # For SB3 compatibility\n        self.observation_space = self.env.observation_spaces[self.agents[0]]\n        self.action_space = self.env.action_spaces[self.agents[0]]\n\n    def reset(self):\n        observations = self.env.reset()\n        return np.array([observations[agent] for agent in self.agents])\n\n    def step(self, actions):\n        # Execute actions for all agents simultaneously\n        rewards = []\n        done = False\n        infos = []\n\n        for i, agent in enumerate(self.agents):\n            if not done:\n                self.env.step(actions[i])\n                rewards.append(self.env.rewards[agent])\n                done = self.env.terminations[agent] or self.env.truncations[agent]\n                infos.append(self.env.infos[agent])\n\n        obs = [self.env.observe(agent) for agent in self.agents]\n        return np.array(obs), np.array(rewards), done, infos\n\nprint(\"âœ… Section completed: ğŸŒ Step 8: ãƒ¡ã‚¤ãƒ³ç’°å¢ƒ\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agents_header"
      },
      "source": [
        "## ğŸ¤– Step 9: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agents_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nAgent implementations for soccer environment\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom collections import deque\nimport random\n\n\nclass BaseAgent(ABC):\n    \"\"\"Base class for all agents\"\"\"\n\n    def __init__(self, agent_id: int, action_space_size: int):\n        self.agent_id = agent_id\n        self.action_space_size = action_space_size\n\n    @abstractmethod\n    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Select action based on observation\"\"\"\n        pass\n\n    @abstractmethod\n    def learn(self, experiences: List) -> Dict[str, float]:\n        \"\"\"Learn from experiences\"\"\"\n        pass\n\n    def save(self, filepath: str):\n        \"\"\"Save agent model\"\"\"\n        pass\n\n    def load(self, filepath: str):\n        \"\"\"Load agent model\"\"\"\n        pass\n\nclass RandomAgent(BaseAgent):\n    \"\"\"Random agent for baseline and testing\"\"\"\n\n    def __init__(self, agent_id: int, action_space_size: int = 5,\n                 action_type: str = \"continuous\"):\n        super().__init__(agent_id, action_space_size)\n        self.action_type = action_type\n\n    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Select random action\"\"\"\n        if self.action_type == \"continuous\":\n            # Continuous action: [move_x, move_y, kick_power, kick_dir_x, kick_dir_y]\n            action = np.array([\n                np.random.uniform(-1, 1),  # move_x\n                np.random.uniform(-1, 1),  # move_y\n                np.random.uniform(0, 1),   # kick_power\n                np.random.uniform(-1, 1),  # kick_dir_x\n                np.random.uniform(-1, 1),  # kick_dir_y\n            ], dtype=np.float32)\n        else:\n            # Discrete action\n            action = np.random.randint(0, 9)  # 9 possible actions\n\n        return action\n\n    def learn(self, experiences: List) -> Dict[str, float]:\n        \"\"\"Random agent doesn't learn\"\"\"\n        return {\"loss\": 0.0}\n\nclass MLPNetwork(nn.Module):\n    \"\"\"Multi-layer perceptron network\"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n        super().__init__()\n\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            prev_dim = hidden_dim\n\n        layers.append(nn.Linear(prev_dim, output_dim))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.network(x)\n\nclass DQNAgent(BaseAgent):\n    \"\"\"Deep Q-Network agent - Fixed version\"\"\"\n\n    def __init__(self, agent_id: int, obs_dim: int, action_dim: int = 9,\n                 hidden_dims: Tuple[int, ...] = (256, 128),\n                 lr: float = 1e-3, gamma: float = 0.99,\n                 epsilon: float = 1.0, epsilon_decay: float = 0.995,\n                 epsilon_min: float = 0.01, buffer_size: int = 10000,\n                 batch_size: int = 64):\n        super().__init__(agent_id, action_dim)\n\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.batch_size = batch_size\n\n        # Neural networks\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.q_network = MLPNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n        self.target_network = MLPNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n        # Experience replay buffer\n        self.replay_buffer = deque(maxlen=buffer_size)\n\n        # Copy weights to target network\n        self.update_target_network()\n\n    def select_action(self, observation: np.ndarray, training: bool = True) -> int:\n        \"\"\"Select action using epsilon-greedy policy\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.action_dim)\n\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n            q_values = self.q_network(obs_tensor)\n            action = q_values.argmax(dim=1).item()\n\n        return action\n\n    def store_experience(self, state: np.ndarray, action: int, reward: float,\n                        next_state: np.ndarray, done: bool):\n        \"\"\"Store experience in replay buffer\"\"\"\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\n    def learn(self, experiences: List = None, training: bool = True) -> Dict[str, float]:\n        \"\"\"Learn from experiences in replay buffer - FIXED\"\"\"\n        if len(self.replay_buffer) < self.batch_size:\n            return {\"loss\": 0.0}\n\n        # Sample batch from replay buffer\n        batch = random.sample(self.replay_buffer, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.BoolTensor(dones).to(self.device)\n\n        # Compute current Q values\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n\n        # Compute target Q values\n        with torch.no_grad():\n            next_q_values = self.target_network(next_states).max(1)[0]\n            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        # Compute loss and update\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n\n        # Update epsilon (only if training)\n        if training:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n        return {\"loss\": loss.item(), \"epsilon\": self.epsilon}\n\n    def update_target_network(self):\n        \"\"\"Copy weights from main network to target network\"\"\"\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def save(self, filepath: str):\n        \"\"\"Save model\"\"\"\n        torch.save({\n            'q_network': self.q_network.state_dict(),\n            'target_network': self.target_network.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'epsilon': self.epsilon\n        }, filepath)\n\n    def load(self, filepath: str):\n        \"\"\"Load model\"\"\"\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.q_network.load_state_dict(checkpoint['q_network'])\n        self.target_network.load_state_dict(checkpoint['target_network'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.epsilon = checkpoint['epsilon']\n\n# Replace the DQNAgent class with the fixed version\nprint(\"âœ… DQNAgent class fixed - 'training' parameter added to learn method\")\n\nclass MADDPGAgent(BaseAgent):\n    \"\"\"Multi-Agent Deep Deterministic Policy Gradient agent\"\"\"\n\n    def __init__(self, agent_id: int, config: MADDPGConfig):\n        super().__init__(agent_id, config.action_dim)\n\n        self.config = config\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Networks\n        self.actor = Actor(config.obs_dim, config.action_dim, config.hidden_dims).to(self.device)\n        self.critic = Critic(config.global_obs_dim, config.global_action_dim,\n                           config.hidden_dims).to(self.device)\n        self.target_actor = Actor(config.obs_dim, config.action_dim, config.hidden_dims).to(self.device)\n        self.target_critic = Critic(config.global_obs_dim, config.global_action_dim,\n                                  config.hidden_dims).to(self.device)\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)\n\n        # Copy weights to target networks\n        self.target_actor.load_state_dict(self.actor.state_dict())\n        self.target_critic.load_state_dict(self.critic.state_dict())\n\n        # Noise for exploration\n        self.noise = OUNoise(config.action_dim, sigma=config.noise_scale)\n\n    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Select action using actor network\"\"\"\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n            action = self.actor(obs_tensor).cpu().data.numpy().flatten()\n\n        if training:\n            action += self.noise.sample()\n            action = np.clip(action, -1, 1)\n\n        return action\n\n    def learn(self, experiences: Dict) -> Dict[str, float]:\n        \"\"\"Learn from experiences (implemented in trainer)\"\"\"\n        # This will be implemented in the MADDPG trainer\n        return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n\n    def soft_update(self, local_model: nn.Module, target_model: nn.Module):\n        \"\"\"Soft update of target network\"\"\"\n        for target_param, local_param in zip(target_model.parameters(),\n                                           local_model.parameters()):\n            target_param.data.copy_(\n                self.config.tau * local_param.data + (1.0 - self.config.tau) * target_param.data\n            )\n\n    def save(self, filepath: str):\n        \"\"\"Save model\"\"\"\n        torch.save({\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'target_actor': self.target_actor.state_dict(),\n            'target_critic': self.target_critic.state_dict(),\n            'actor_optimizer': self.actor_optimizer.state_dict(),\n            'critic_optimizer': self.critic_optimizer.state_dict(),\n        }, filepath)\n\n    def load(self, filepath: str):\n        \"\"\"Load model\"\"\"\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.actor.load_state_dict(checkpoint['actor'])\n        self.critic.load_state_dict(checkpoint['critic'])\n        self.target_actor.load_state_dict(checkpoint['target_actor'])\n        self.target_critic.load_state_dict(checkpoint['target_critic'])\n        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n\ndef create_agent(agent_type: str, agent_id: int, config: Dict) -> BaseAgent:\n    \"\"\"Factory function to create agents\"\"\"\n    if agent_type == \"random\":\n        return RandomAgent(agent_id)\n    elif agent_type == \"dqn\":\n        return DQNAgent(agent_id, **config)\n    elif agent_type == \"maddpg\":\n        return MADDPGAgent(agent_id, **config)\n    else:\n        raise ValueError(f\"Unknown agent type: {agent_type}\")\n\nprint(\"âœ… Section completed: ğŸ¤– Step 9: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainers_header"
      },
      "source": [
        "## ğŸ“š Step 10: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trainers_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nTraining frameworks for multi-agent soccer environment\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom collections import deque, defaultdict\nimport random\nimport time\nfrom abc import ABC, abstractmethod\n\n\nclass ReplayBuffer:\n    \"\"\"Experience replay buffer for multi-agent learning\"\"\"\n\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, experience: Tuple):\n        \"\"\"Add experience to buffer\"\"\"\n        self.buffer.append(experience)\n\n    def sample(self, batch_size: int) -> List[Tuple]:\n        \"\"\"Sample batch from buffer\"\"\"\n        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n\n    def __len__(self):\n        return len(self.buffer)\n\nclass BaseTrainer(ABC):\n    \"\"\"Base class for all training frameworks\"\"\"\n\n    def __init__(self, env_config: SoccerEnvironmentConfig,\n                 training_config: TrainingConfig):\n        self.env_config = env_config\n        self.training_config = training_config\n        self.env = make_soccer_env(env_config, render_mode=None)\n\n        # Training statistics\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.scores_history = []\n        self.training_metrics = defaultdict(list)\n\n    @abstractmethod\n    def train(self, num_episodes: int) -> Dict[str, Any]:\n        \"\"\"Train agents for specified number of episodes\"\"\"\n        pass\n\n    def evaluate(self, num_episodes: int = 10) -> Dict[str, float]:\n        \"\"\"Evaluate current agent performance\"\"\"\n        total_rewards = []\n        total_lengths = []\n        team_scores = [[], []]\n\n        for episode in range(num_episodes):\n            observations = self.env.reset()\n            episode_reward = 0\n            steps = 0\n\n            while not all(self.env.terminations.values()) and not all(self.env.truncations.values()):\n                actions = {}\n                for agent in self.env.agents:\n                    if not self.env.terminations.get(agent, False) and not self.env.truncations.get(agent, False):\n                        obs = self.env.observe(agent)\n                        actions[agent] = self._get_agent_action(agent, obs, training=False)\n\n                for agent, action in actions.items():\n                    self.env.step(action)\n                    episode_reward += self.env.rewards.get(agent, 0)\n                    steps += 1\n\n                    if self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False):\n                        break\n\n            total_rewards.append(episode_reward)\n            total_lengths.append(steps)\n            team_scores[0].append(self.env.scores[0])\n            team_scores[1].append(self.env.scores[1])\n\n        return {\n            'avg_reward': np.mean(total_rewards),\n            'avg_length': np.mean(total_lengths),\n            'team_0_avg_score': np.mean(team_scores[0]),\n            'team_1_avg_score': np.mean(team_scores[1]),\n            'win_rate_team_0': sum(1 for i in range(num_episodes) if team_scores[0][i] > team_scores[1][i]) / num_episodes,\n            'win_rate_team_1': sum(1 for i in range(num_episodes) if team_scores[1][i] > team_scores[0][i]) / num_episodes,\n        }\n\n    @abstractmethod\n    def _get_agent_action(self, agent: str, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Get action from agent\"\"\"\n        pass\n\nclass IndependentLearningTrainer(BaseTrainer):\n    \"\"\"Independent learning trainer where each agent learns separately\"\"\"\n\n    def __init__(self, env_config: SoccerEnvironmentConfig,\n                 training_config: TrainingConfig,\n                 agent_type: str = \"dqn\",\n                 agent_configs: Dict = None):\n        super().__init__(env_config, training_config)\n\n        self.agent_type = agent_type\n        self.agent_configs = agent_configs or {}\n\n        # Create agents\n        self.agents = {}\n        for i, agent_name in enumerate(self.env.agents):\n            if agent_type == \"dqn\":\n                self.agents[agent_name] = DQNAgent(\n                    agent_id=i,\n                    obs_dim=28,  # From observation space\n                    **self.agent_configs\n                )\n            elif agent_type == \"random\":\n                self.agents[agent_name] = RandomAgent(i)\n            else:\n                raise ValueError(f\"Unknown agent type: {agent_type}\")\n\n    def train(self, num_episodes: int) -> Dict[str, Any]:\n        \"\"\"Train agents independently\"\"\"\n        print(f\"Starting independent learning training with {self.agent_type} agents\")\n\n        for episode in range(num_episodes):\n            observations = self.env.reset()\n            episode_rewards = {agent: 0 for agent in self.env.agents}\n            episode_length = 0\n\n            # Store previous observations for experience replay\n            prev_observations = {}\n\n            while not all(self.env.terminations.values()) and not all(self.env.truncations.values()):\n                actions = {}\n\n                # Get actions from all agents\n                for agent in self.env.agents:\n                    if not self.env.terminations.get(agent, False) and not self.env.truncations.get(agent, False):\n                        obs = self.env.observe(agent)\n                        action = self.agents[agent].select_action(obs, training=True)\n                        actions[agent] = action\n                        prev_observations[agent] = obs\n\n                # Execute actions\n                for agent, action in actions.items():\n                    self.env.step(action)\n                    reward = self.env.rewards.get(agent, 0)\n                    episode_rewards[agent] += reward\n                    episode_length += 1\n\n                    # Store experience for DQN agents\n                    if self.agent_type == \"dqn\" and agent in prev_observations:\n                        next_obs = self.env.observe(agent)\n                        done = self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False)\n\n                        if isinstance(action, np.ndarray):\n                            action = int(action[0]) if len(action) > 0 else 0\n\n                        self.agents[agent].store_experience(\n                            prev_observations[agent], action, reward, next_obs, done\n                        )\n\n                        # Learn from experience\n                        metrics = self.agents[agent].learn()\n                        if metrics and metrics['loss'] > 0:\n                            self.training_metrics[f'{agent}_loss'].append(metrics['loss'])\n\n                    if self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False):\n                        break\n\n            # Update target networks for DQN agents\n            if self.agent_type == \"dqn\" and episode % 100 == 0:\n                for agent_name, agent in self.agents.items():\n                    agent.update_target_network()\n\n            # Record episode statistics\n            total_reward = sum(episode_rewards.values())\n            self.episode_rewards.append(total_reward)\n            self.episode_lengths.append(episode_length)\n            self.scores_history.append(self.env.scores.copy())\n\n            # Print progress\n            if episode % 100 == 0:\n                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n                print(f\"Episode {episode}: Avg Reward (last 100): {avg_reward:.2f}, Scores: {self.env.scores}\")\n\n                # Evaluate current performance\n                if episode % 500 == 0 and episode > 0:\n                    eval_metrics = self.evaluate(num_episodes=10)\n                    print(f\"Evaluation: {eval_metrics}\")\n\n        return {\n            'episode_rewards': self.episode_rewards,\n            'episode_lengths': self.episode_lengths,\n            'scores_history': self.scores_history,\n            'training_metrics': dict(self.training_metrics)\n        }\n\n    def _get_agent_action(self, agent: str, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Get action from specific agent\"\"\"\n        if self.agent_type == \"dqn\":\n            discrete_action = self.agents[agent].select_action(observation, training)\n            # Convert discrete action to continuous for environment\n            action_space = ActionSpace(\"discrete\")\n            return action_space.convert_discrete_to_continuous(discrete_action)\n        else:\n            return self.agents[agent].select_action(observation, training)\n\nclass MADDPGTrainer(BaseTrainer):\n    \"\"\"MADDPG trainer with centralized critic\"\"\"\n\n    def __init__(self, env_config: SoccerEnvironmentConfig,\n                 training_config: TrainingConfig,\n                 maddpg_config: MADDPGConfig):\n        super().__init__(env_config, training_config)\n        self.maddpg_config = maddpg_config\n\n        # Create MADDPG agents\n        self.agents = {}\n        for i, agent_name in enumerate(self.env.agents):\n            self.agents[agent_name] = MADDPGAgent(i, maddpg_config)\n\n        # Shared replay buffer\n        self.replay_buffer = ReplayBuffer(maddpg_config.buffer_size)\n\n    def train(self, num_episodes: int) -> Dict[str, Any]:\n        \"\"\"Train MADDPG agents\"\"\"\n        print(\"Starting MADDPG training\")\n\n        for episode in range(num_episodes):\n            observations = self.env.reset()\n            episode_rewards = {agent: 0 for agent in self.env.agents}\n            episode_length = 0\n\n            # Episode experience\n            episode_experiences = []\n\n            while not all(self.env.terminations.values()) and not all(self.env.truncations.values()):\n                # Get global observation and actions\n                global_obs = []\n                actions = {}\n\n                for agent in self.env.agents:\n                    if not self.env.terminations.get(agent, False) and not self.env.truncations.get(agent, False):\n                        obs = self.env.observe(agent)\n                        action = self.agents[agent].select_action(obs, training=True)\n                        actions[agent] = action\n                        global_obs.append(obs)\n\n                # Execute actions and collect rewards\n                global_actions = list(actions.values())\n                step_experience = {\n                    'global_obs': np.concatenate(global_obs),\n                    'actions': actions.copy(),\n                    'global_actions': np.concatenate(global_actions),\n                    'rewards': {},\n                    'next_global_obs': None,\n                    'dones': {}\n                }\n\n                for agent, action in actions.items():\n                    self.env.step(action)\n                    reward = self.env.rewards.get(agent, 0)\n                    episode_rewards[agent] += reward\n                    step_experience['rewards'][agent] = reward\n                    step_experience['dones'][agent] = self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False)\n                    episode_length += 1\n\n                    if step_experience['dones'][agent]:\n                        break\n\n                # Get next global observation\n                next_global_obs = []\n                for agent in self.env.agents:\n                    next_obs = self.env.observe(agent)\n                    next_global_obs.append(next_obs)\n                step_experience['next_global_obs'] = np.concatenate(next_global_obs)\n\n                episode_experiences.append(step_experience)\n\n            # Store experiences in replay buffer\n            for exp in episode_experiences:\n                self.replay_buffer.push(exp)\n\n            # Train agents if enough experiences\n            if len(self.replay_buffer) > self.maddpg_config.batch_size:\n                self._train_maddpg_step()\n\n            # Record statistics\n            total_reward = sum(episode_rewards.values())\n            self.episode_rewards.append(total_reward)\n            self.episode_lengths.append(episode_length)\n            self.scores_history.append(self.env.scores.copy())\n\n            # Print progress\n            if episode % 100 == 0:\n                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n                print(f\"Episode {episode}: Avg Reward: {avg_reward:.2f}, Scores: {self.env.scores}\")\n\n        return {\n            'episode_rewards': self.episode_rewards,\n            'episode_lengths': self.episode_lengths,\n            'scores_history': self.scores_history,\n            'training_metrics': dict(self.training_metrics)\n        }\n\n    def _train_maddpg_step(self):\n        \"\"\"Perform one MADDPG training step\"\"\"\n        batch = self.replay_buffer.sample(self.maddpg_config.batch_size)\n\n        for i, (agent_name, agent) in enumerate(self.agents.items()):\n            # Extract data for this agent\n            states = torch.FloatTensor([exp['global_obs'] for exp in batch]).to(agent.device)\n            actions = torch.FloatTensor([exp['global_actions'] for exp in batch]).to(agent.device)\n            rewards = torch.FloatTensor([exp['rewards'][agent_name] for exp in batch]).to(agent.device)\n            next_states = torch.FloatTensor([exp['next_global_obs'] for exp in batch]).to(agent.device)\n            dones = torch.BoolTensor([exp['dones'][agent_name] for exp in batch]).to(agent.device)\n\n            # Get agent-specific observations\n            agent_obs = states[:, i*28:(i+1)*28]  # 28D observation per agent\n            next_agent_obs = next_states[:, i*28:(i+1)*28]\n\n            # Update critic\n            with torch.no_grad():\n                next_actions = torch.cat([\n                    self.agents[list(self.agents.keys())[j]].target_actor(next_states[:, j*28:(j+1)*28])\n                    for j in range(len(self.agents))\n                ], dim=1)\n                target_q = agent.target_critic(next_states, next_actions)\n                target_q = rewards + (self.maddpg_config.gamma * target_q * ~dones)\n\n            current_q = agent.critic(states, actions)\n            critic_loss = F.mse_loss(current_q.squeeze(), target_q.squeeze())\n\n            agent.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), max_norm=0.5)\n            agent.critic_optimizer.step()\n\n            # Update actor\n            agent_actions = agent.actor(agent_obs)\n            full_actions = actions.clone()\n            full_actions[:, i*5:(i+1)*5] = agent_actions  # 5D action per agent\n\n            actor_loss = -agent.critic(states, full_actions).mean()\n\n            agent.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            torch.nn.utils.clip_grad_norm_(agent.actor.parameters(), max_norm=0.5)\n            agent.actor_optimizer.step()\n\n            # Soft update target networks\n            agent.soft_update(agent.actor, agent.target_actor)\n            agent.soft_update(agent.critic, agent.target_critic)\n\n            # Record metrics\n            self.training_metrics[f'{agent_name}_critic_loss'].append(critic_loss.item())\n            self.training_metrics[f'{agent_name}_actor_loss'].append(actor_loss.item())\n\n    def _get_agent_action(self, agent: str, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Get action from MADDPG agent\"\"\"\n        return self.agents[agent].select_action(observation, training)\n\ndef create_trainer(trainer_type: str, env_config: SoccerEnvironmentConfig,\n                  training_config: TrainingConfig, **kwargs) -> BaseTrainer:\n    \"\"\"Factory function to create trainers\"\"\"\n    if trainer_type == \"independent\":\n        return IndependentLearningTrainer(env_config, training_config, **kwargs)\n    elif trainer_type == \"maddpg\":\n        maddpg_config = kwargs.get('maddpg_config', MADDPGConfig())\n        return MADDPGTrainer(env_config, training_config, maddpg_config)\n    else:\n        raise ValueError(f\"Unknown trainer type: {trainer_type}\")\n\nprint(\"âœ… Section completed: ğŸ“š Step 10: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_header"
      },
      "source": [
        "## ğŸ§ª Step 11: ãƒ†ã‚¹ãƒˆé–¢æ•°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nTest script for soccer environment with random agents\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict\nimport time\n\n\ndef test_basic_environment():\n    \"\"\"Test basic environment functionality\"\"\"\n    print(\"Testing basic environment functionality...\")\n\n    # Create environment\n    config = SoccerEnvironmentConfig()\n    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n    print(f\"Environment created with {len(env.agents)} agents\")\n    print(f\"Agents: {env.agents}\")\n    print(f\"Observation space: {env.observation_spaces[env.agents[0]]}\")\n    print(f\"Action space: {env.action_spaces[env.agents[0]]}\")\n\n    # Test reset\n    observations = env.reset()\n    print(f\"Reset successful, observations shape: {[obs.shape for obs in observations.values()]}\")\n\n    # Test step\n    for agent in env.agents:\n        action = env.action_spaces[agent].sample()\n        print(f\"Agent {agent} taking action: {action}\")\n        env.step(action)\n\n    print(\"Basic environment test completed successfully!\")\n    return True\n\ndef test_random_agents_episode():\n    \"\"\"Test full episode with random agents\"\"\"\n    print(\"Testing full episode with random agents...\")\n\n    # Create environment and agents\n    config = SoccerEnvironmentConfig()\n    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n    # Create random agents\n    agents = {}\n    for i, agent_name in enumerate(env.agents):\n        agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n\n    # Run episode\n    observations = env.reset()\n    episode_rewards = {agent: 0 for agent in env.agents}\n    episode_length = 0\n\n    print(\"Running episode...\")\n    start_time = time.time()\n\n    while not all(env.terminations.values()) and not all(env.truncations.values()):\n        for agent in env.agents:\n            if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n                # Get action from agent\n                obs = env.observe(agent)\n                action = agents[agent].select_action(obs, training=False)\n\n                # Take step\n                env.step(action)\n\n                # Accumulate reward\n                episode_rewards[agent] += env.rewards.get(agent, 0)\n\n                episode_length += 1\n\n                # Break if episode is done\n                if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n                    break\n\n    elapsed_time = time.time() - start_time\n\n    print(f\"Episode completed in {elapsed_time:.2f} seconds\")\n    print(f\"Episode length: {episode_length} steps\")\n    print(f\"Final scores: {env.scores}\")\n    print(f\"Episode rewards: {episode_rewards}\")\n\n    env.close()\n    return True\n\ndef test_multiple_episodes(num_episodes: int = 5):\n    \"\"\"Test multiple episodes and collect statistics\"\"\"\n    print(f\"Testing {num_episodes} episodes for performance analysis...\")\n\n    config = SoccerEnvironmentConfig()\n    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n    # Create random agents\n    agents = {}\n    for i, agent_name in enumerate(env.agents):\n        agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n\n    # Statistics\n    episode_lengths = []\n    episode_rewards = []\n    final_scores = []\n\n    for episode in range(num_episodes):\n        print(f\"Episode {episode + 1}/{num_episodes}\")\n\n        observations = env.reset()\n        episode_reward = {agent: 0 for agent in env.agents}\n        steps = 0\n\n        while not all(env.terminations.values()) and not all(env.truncations.values()):\n            for agent in env.agents:\n                if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n                    obs = env.observe(agent)\n                    action = agents[agent].select_action(obs, training=False)\n                    env.step(action)\n                    episode_reward[agent] += env.rewards.get(agent, 0)\n                    steps += 1\n\n                    if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n                        break\n\n        episode_lengths.append(steps)\n        episode_rewards.append(episode_reward)\n        final_scores.append(env.scores.copy())\n\n        print(f\"  Steps: {steps}, Scores: {env.scores}, Avg Reward: {np.mean(list(episode_reward.values())):.2f}\")\n\n    # Print statistics\n    print(f\"\\n=== Statistics over {num_episodes} episodes ===\")\n    print(f\"Average episode length: {np.mean(episode_lengths):.2f} Â± {np.std(episode_lengths):.2f}\")\n\n    # Team scores\n    team_0_scores = [score[0] for score in final_scores]\n    team_1_scores = [score[1] for score in final_scores]\n\n    print(f\"Team 0 (Blue) average score: {np.mean(team_0_scores):.2f} Â± {np.std(team_0_scores):.2f}\")\n    print(f\"Team 1 (Red) average score: {np.mean(team_1_scores):.2f} Â± {np.std(team_1_scores):.2f}\")\n\n    # Average rewards per agent\n    for agent in env.agents:\n        agent_rewards = [ep_reward[agent] for ep_reward in episode_rewards]\n        print(f\"{agent} average reward: {np.mean(agent_rewards):.2f} Â± {np.std(agent_rewards):.2f}\")\n\n    env.close()\n    return True\n\ndef run_all_tests():\n    \"\"\"Run all tests\"\"\"\n    print(\"=\" * 60)\n    print(\"Multi-Agent Soccer Environment Test Suite\")\n    print(\"=\" * 60)\n\n    tests = [\n        (\"Basic Environment\", test_basic_environment),\n        (\"Random Agents Episode\", test_random_agents_episode),\n        (\"Multiple Episodes\", lambda: test_multiple_episodes(3))\n    ]\n\n    results = []\n    for test_name, test_func in tests:\n        print(f\"\\n[TEST] {test_name}\")\n        print(\"-\" * 40)\n        try:\n            result = test_func()\n            results.append(result)\n            print(f\"âœ“ {test_name} PASSED\")\n        except Exception as e:\n            print(f\"âœ— {test_name} FAILED: {e}\")\n            results.append(False)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TEST SUMMARY\")\n    print(\"=\" * 60)\n    for i, (test_name, _) in enumerate(tests):\n        status = \"PASSED\" if results[i] else \"FAILED\"\n        print(f\"{test_name}: {status}\")\n\n    success_rate = sum(results) / len(results)\n    print(f\"\\nSuccess Rate: {success_rate:.1%} ({sum(results)}/{len(results)})\")\n\n    return success_rate == 1.0\n\nif __name__ == \"__main__\":\n    run_all_tests()\n\nprint(\"âœ… Section completed: ğŸ§ª Step 11: ãƒ†ã‚¹ãƒˆé–¢æ•°\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exec_header"
      },
      "source": [
        "## ğŸš€ Step 12: å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³\n\n",
        "**æ³¨æ„**: ä¸Šè¨˜ã®ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_env"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ç’°å¢ƒã®ãƒ†ã‚¹ãƒˆ\n",
        "print(\"ğŸ§ª Testing the environment...\")\n",
        "print(\"=\" * 60)\n\n",
        "try:\n",
        "    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ\n",
        "    test_config = SoccerEnvironmentConfig()\n",
        "    test_env = make_soccer_env(test_config, render_mode=None, action_type=\"continuous\")\n",
        "    print(f\"âœ… Environment created successfully!\")\n",
        "    print(f\"   Agents: {test_env.agents}\")\n",
        "    print(f\"   Observation space: {test_env.observation_spaces[test_env.agents[0]].shape}\")\n",
        "    print(f\"   Action space: {test_env.action_spaces[test_env.agents[0]].shape}\")\n",
        "    \n",
        "    # Reset test\n",
        "    test_env.reset()\n",
        "    print(f\"âœ… Environment reset successful!\")\n",
        "    \n",
        "    # Step test\n",
        "    for agent in test_env.agents:\n",
        "        action = test_env.action_spaces[agent].sample()\n",
        "        test_env.step(action)\n",
        "        break  # Just test one step\n",
        "    print(f\"âœ… Environment step successful!\")\n",
        "    \n",
        "    test_env.close()\n",
        "    print(f\"\\nâœ… All basic tests passed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error during testing: {e}\")\n",
        "    print(\"Please make sure all previous cells have been executed.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_baseline"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰\n",
        "print(\"ğŸ® Running baseline with random agents...\")\n",
        "print(\"=\" * 60)\n\n",
        "# ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ\n",
        "config = SoccerEnvironmentConfig()\n",
        "env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n",
        "# ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ\n",
        "random_agents = {}\n",
        "for i, agent_name in enumerate(env.agents):\n",
        "    random_agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n\n",
        "# 5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ\n",
        "episode_results = []\n",
        "for episode in range(5):\n",
        "    env.reset()\n",
        "    episode_reward = 0\n",
        "    steps = 0\n",
        "    \n",
        "    while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
        "        for agent in env.agents:\n",
        "            if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n",
        "                obs = env.observe(agent)\n",
        "                action = random_agents[agent].select_action(obs, training=False)\n",
        "                env.step(action)\n",
        "                episode_reward += env.rewards.get(agent, 0)\n",
        "                steps += 1\n",
        "                \n",
        "                if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n",
        "                    break\n",
        "    \n",
        "    episode_results.append({\n",
        "        'steps': steps,\n",
        "        'reward': episode_reward,\n",
        "        'scores': env.scores.copy()\n",
        "    })\n",
        "    \n",
        "    print(f\"Episode {episode + 1}: Steps={steps}, Scores={env.scores}, Reward={episode_reward:.2f}\")\n\n",
        "# çµ±è¨ˆ\n",
        "avg_steps = np.mean([r['steps'] for r in episode_results])\n",
        "avg_reward = np.mean([r['reward'] for r in episode_results])\n",
        "print(f\"\\nğŸ“Š Statistics:\")\n",
        "print(f\"   Average steps: {avg_steps:.1f}\")\n",
        "print(f\"   Average reward: {avg_reward:.2f}\")\n\n",
        "env.close()\n",
        "print(\"\\nâœ… Baseline completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## ğŸ“ ã¾ã¨ã‚\n\n",
        "### âœ… å®Ÿè£…å®Œäº†\n",
        "- å®Œå…¨ãªç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ã¨PettingZooäº’æ›ç’°å¢ƒ\n",
        "- Random, DQN, MADDPG ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\n",
        "- è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯\n\n",
        "### ğŸ”§ ä¿®æ­£æ¸ˆã¿ã®å•é¡Œ\n",
        "- ModuleNotFoundError: å†…éƒ¨importå‰Šé™¤\n",
        "- TypeError (agent_selector): AgentSelectorã«ä¿®æ­£\n",
        "- å®Ÿè¡Œé †åºã®ä¾å­˜é–¢ä¿‚\n\n",
        "### ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
        "1. ã‚ˆã‚Šé•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§è¨“ç·´\n",
        "2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´\n",
        "3. å­¦ç¿’æ›²ç·šã®åˆ†æ\n\n",
        "**Happy Training! ğŸ®**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqn_fix_header"
      },
      "source": [
        "### ğŸ”§ DQNAgent Fix (Run if you encounter 'training' error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqn_fix_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "class DQNAgent(BaseAgent):\n    \"\"\"Deep Q-Network agent - Fixed version\"\"\"\n\n    def __init__(self, agent_id: int, obs_dim: int, action_dim: int = 9,\n                 hidden_dims: Tuple[int, ...] = (256, 128),\n                 lr: float = 1e-3, gamma: float = 0.99,\n                 epsilon: float = 1.0, epsilon_decay: float = 0.995,\n                 epsilon_min: float = 0.01, buffer_size: int = 10000,\n                 batch_size: int = 64):\n        super().__init__(agent_id, action_dim)\n\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.batch_size = batch_size\n\n        # Neural networks\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.q_network = MLPNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n        self.target_network = MLPNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n        # Experience replay buffer\n        self.replay_buffer = deque(maxlen=buffer_size)\n\n        # Copy weights to target network\n        self.update_target_network()\n\n    def select_action(self, observation: np.ndarray, training: bool = True) -> int:\n        \"\"\"Select action using epsilon-greedy policy\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.action_dim)\n\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n            q_values = self.q_network(obs_tensor)\n            action = q_values.argmax(dim=1).item()\n\n        return action\n\n    def store_experience(self, state: np.ndarray, action: int, reward: float,\n                        next_state: np.ndarray, done: bool):\n        \"\"\"Store experience in replay buffer\"\"\"\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\n    def learn(self, experiences: List = None, training: bool = True) -> Dict[str, float]:\n        \"\"\"Learn from experiences in replay buffer - FIXED\"\"\"\n        if len(self.replay_buffer) < self.batch_size:\n            return {\"loss\": 0.0}\n\n        # Sample batch from replay buffer\n        batch = random.sample(self.replay_buffer, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.BoolTensor(dones).to(self.device)\n\n        # Compute current Q values\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n\n        # Compute target Q values\n        with torch.no_grad():\n            next_q_values = self.target_network(next_states).max(1)[0]\n            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        # Compute loss and update\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n\n        # Update epsilon (only if training)\n        if training:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n        return {\"loss\": loss.item(), \"epsilon\": self.epsilon}\n\n    def update_target_network(self):\n        \"\"\"Copy weights from main network to target network\"\"\"\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def save(self, filepath: str):\n        \"\"\"Save model\"\"\"\n        torch.save({\n            'q_network': self.q_network.state_dict(),\n            'target_network': self.target_network.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'epsilon': self.epsilon\n        }, filepath)\n\n    def load(self, filepath: str):\n        \"\"\"Load model\"\"\"\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.q_network.load_state_dict(checkpoint['q_network'])\n        self.target_network.load_state_dict(checkpoint['target_network'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.epsilon = checkpoint['epsilon']\n\n# Replace the DQNAgent class with the fixed version\nprint(\"âœ… DQNAgent class fixed - 'training' parameter added to learn method\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extended_training_header"
      },
      "source": [
        "# ğŸš€ Extended Training and Visualization\n",
        "## ã‚ˆã‚Šé•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã®è¨“ç·´ã¨å‹•ç”»å¯è¦–åŒ–\n\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚ˆã‚Šé•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã—ã€çµæœã‚’å‹•ç”»ã¨ã—ã¦å¯è¦–åŒ–ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_deps_header"
      },
      "source": [
        "### ğŸ“¦ å‹•ç”»ä½œæˆç”¨ã®è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_video_deps"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# å‹•ç”»ä½œæˆç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq xvfb\n",
        "!pip install -q imageio imageio-ffmpeg\n",
        "!pip install -q pyvirtualdisplay\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "import base64\n",
        "\n",
        "# Virtual display for rendering\n",
        "from pyvirtualdisplay import Display\n",
        "display_virtual = Display(visible=0, size=(1400, 900))\n",
        "display_virtual.start()\n",
        "\n",
        "print(\"âœ… Video dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extended_trainer_header"
      },
      "source": [
        "### ğŸ“š æ‹¡å¼µè¨“ç·´ã‚¯ãƒ©ã‚¹ã®å®Ÿè£…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extended_trainer_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ExtendedTrainer:\n",
        "    \"\"\"Extended trainer with video recording capabilities\"\"\"\n",
        "    \n",
        "    def __init__(self, env_config, agent_type=\"dqn\"):\n",
        "        self.env_config = env_config\n",
        "        self.agent_type = agent_type\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "        self.scores_history = []\n",
        "        self.video_frames = []\n",
        "        \n",
        "    def create_agents(self, env):\n",
        "        \"\"\"Create agents based on type\"\"\"\n",
        "        agents = {}\n",
        "        \n",
        "        if self.agent_type == \"random\":\n",
        "            for i, agent_name in enumerate(env.agents):\n",
        "                agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n",
        "        \n",
        "        elif self.agent_type == \"dqn\":\n",
        "            for i, agent_name in enumerate(env.agents):\n",
        "                agents[agent_name] = DQNAgent(\n",
        "                    agent_id=i,\n",
        "                    obs_dim=28,\n",
        "                    action_dim=9,\n",
        "                    hidden_dims=(256, 128),\n",
        "                    lr=1e-3,\n",
        "                    gamma=0.99,\n",
        "                    epsilon=1.0,\n",
        "                    epsilon_decay=0.995,\n",
        "                    epsilon_min=0.01,\n",
        "                    buffer_size=10000,\n",
        "                    batch_size=64\n",
        "                )\n",
        "        \n",
        "        elif self.agent_type == \"maddpg\":\n",
        "            maddpg_config = MADDPGConfig()\n",
        "            for i, agent_name in enumerate(env.agents):\n",
        "                agents[agent_name] = MADDPGAgent(i, maddpg_config)\n",
        "        \n",
        "        return agents\n",
        "    \n",
        "    def train(self, num_episodes=100, record_video_every=20, max_video_episodes=5):\n",
        "        \"\"\"Train agents and record videos\"\"\"\n",
        "        print(f\"ğŸ® Starting extended training with {self.agent_type} agents\")\n",
        "        print(f\"   Episodes: {num_episodes}\")\n",
        "        print(f\"   Recording video every {record_video_every} episodes\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Create environments\n",
        "        env = make_soccer_env(self.env_config, render_mode=None, action_type=\"continuous\")\n",
        "        render_env = make_soccer_env(self.env_config, render_mode=\"rgb_array\", action_type=\"continuous\")\n",
        "        \n",
        "        # Create agents\n",
        "        agents = self.create_agents(env)\n",
        "        \n",
        "        videos = []  # Store video data\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            # Determine if we should record this episode\n",
        "            record_this_episode = (episode % record_video_every == 0) and (len(videos) < max_video_episodes)\n",
        "            \n",
        "            # Use render environment if recording\n",
        "            current_env = render_env if record_this_episode else env\n",
        "            current_env.reset()\n",
        "            \n",
        "            episode_reward = {agent: 0 for agent in current_env.agents}\n",
        "            episode_frames = []\n",
        "            steps = 0\n",
        "            \n",
        "            # Store experiences for learning (DQN)\n",
        "            episode_experiences = {agent: [] for agent in current_env.agents}\n",
        "            \n",
        "            while not all(current_env.terminations.values()) and not all(current_env.truncations.values()):\n",
        "                # Record frame if needed\n",
        "                if record_this_episode:\n",
        "                    frame = current_env.render()\n",
        "                    if frame is not None:\n",
        "                        episode_frames.append(frame)\n",
        "                \n",
        "                for agent_name in current_env.agents:\n",
        "                    if not current_env.terminations.get(agent_name, False) and not current_env.truncations.get(agent_name, False):\n",
        "                        # Get observation and action\n",
        "                        obs = current_env.observe(agent_name)\n",
        "                        \n",
        "                        if self.agent_type == \"dqn\":\n",
        "                            action = agents[agent_name].select_action(obs, training=True)\n",
        "                            # Convert discrete to continuous\n",
        "                            action_space = ActionSpace(\"discrete\")\n",
        "                            action_continuous = action_space.convert_discrete_to_continuous(action)\n",
        "                            current_env.step(action_continuous)\n",
        "                            \n",
        "                            # Store experience\n",
        "                            next_obs = current_env.observe(agent_name)\n",
        "                            reward = current_env.rewards.get(agent_name, 0)\n",
        "                            done = current_env.terminations.get(agent_name, False) or current_env.truncations.get(agent_name, False)\n",
        "                            \n",
        "                            agents[agent_name].store_experience(obs, action, reward, next_obs, done)\n",
        "                            \n",
        "                            # Learn from experience\n",
        "                            if len(agents[agent_name].replay_buffer) > agents[agent_name].batch_size:\n",
        "                                agents[agent_name].learn()\n",
        "                        else:\n",
        "                            action = agents[agent_name].select_action(obs, training=True)\n",
        "                            current_env.step(action)\n",
        "                            reward = current_env.rewards.get(agent_name, 0)\n",
        "                        \n",
        "                        episode_reward[agent_name] += reward\n",
        "                        steps += 1\n",
        "                        \n",
        "                        if current_env.terminations.get(agent_name, False) or current_env.truncations.get(agent_name, False):\n",
        "                            break\n",
        "            \n",
        "            # Save video if recorded\n",
        "            if record_this_episode and episode_frames:\n",
        "                videos.append({\n",
        "                    'episode': episode,\n",
        "                    'frames': episode_frames,\n",
        "                    'scores': current_env.scores.copy(),\n",
        "                    'reward': sum(episode_reward.values())\n",
        "                })\n",
        "                print(f\"ğŸ“¹ Recorded video for episode {episode}\")\n",
        "            \n",
        "            # Update target networks for DQN\n",
        "            if self.agent_type == \"dqn\" and episode % 10 == 0:\n",
        "                for agent_name in agents:\n",
        "                    agents[agent_name].update_target_network()\n",
        "            \n",
        "            # Store metrics\n",
        "            self.episode_rewards.append(sum(episode_reward.values()))\n",
        "            self.episode_lengths.append(steps)\n",
        "            self.scores_history.append(current_env.scores.copy())\n",
        "            \n",
        "            # Print progress\n",
        "            if episode % 10 == 0:\n",
        "                avg_reward = np.mean(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else np.mean(self.episode_rewards)\n",
        "                print(f\"Episode {episode}: Avg Reward (last 10): {avg_reward:.2f}, Scores: {current_env.scores}\")\n",
        "        \n",
        "        env.close()\n",
        "        render_env.close()\n",
        "        \n",
        "        return videos\n",
        "\n",
        "print(\"âœ… Extended trainer class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_extended_header"
      },
      "source": [
        "### ğŸ® æ‹¡å¼µè¨“ç·´ã®å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_extended_training"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# æ‹¡å¼µè¨“ç·´ã®å®Ÿè¡Œ\n",
        "print(\"ğŸš€ Starting extended training...\")\n",
        "print(\"This will take a few minutes. Please be patient.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuration\n",
        "config = SoccerEnvironmentConfig()\n",
        "config.MAX_STEPS = 500  # Shorter episodes for faster training\n",
        "\n",
        "# Create trainer\n",
        "trainer = ExtendedTrainer(config, agent_type=\"random\")  # Start with random for quick results\n",
        "\n",
        "# Train and record videos\n",
        "videos = trainer.train(\n",
        "    num_episodes=50,      # Total episodes\n",
        "    record_video_every=10, # Record every 10 episodes\n",
        "    max_video_episodes=5   # Maximum 5 videos\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Training completed!\")\n",
        "print(f\"   Total episodes: {len(trainer.episode_rewards)}\")\n",
        "print(f\"   Videos recorded: {len(videos)}\")\n",
        "print(f\"   Average reward: {np.mean(trainer.episode_rewards):.2f}\")\n",
        "print(f\"   Average episode length: {np.mean(trainer.episode_lengths):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viz_results_header"
      },
      "source": [
        "### ğŸ“Š è¨“ç·´çµæœã®å¯è¦–åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visualize_results"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# è¨“ç·´çµæœã®å¯è¦–åŒ–\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Episode rewards\n",
        "ax = axes[0, 0]\n",
        "ax.plot(trainer.episode_rewards, alpha=0.3, label='Raw')\n",
        "if len(trainer.episode_rewards) > 10:\n",
        "    smoothed = np.convolve(trainer.episode_rewards, np.ones(10)/10, mode='valid')\n",
        "    ax.plot(range(9, len(trainer.episode_rewards)), smoothed, linewidth=2, label='Smoothed (10-ep)')\n",
        "ax.set_title('Episode Rewards Over Time')\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Total Reward')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "\n",
        "# Episode lengths\n",
        "ax = axes[0, 1]\n",
        "ax.plot(trainer.episode_lengths, alpha=0.5, color='orange')\n",
        "ax.set_title('Episode Lengths')\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Steps')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Team scores over time\n",
        "ax = axes[1, 0]\n",
        "team_0_scores = [score[0] for score in trainer.scores_history]\n",
        "team_1_scores = [score[1] for score in trainer.scores_history]\n",
        "ax.plot(team_0_scores, label='Team 0 (Blue)', alpha=0.7, color='blue')\n",
        "ax.plot(team_1_scores, label='Team 1 (Red)', alpha=0.7, color='red')\n",
        "ax.set_title('Team Scores Over Episodes')\n",
        "ax.set_xlabel('Episode')\n",
        "ax.set_ylabel('Goals Scored')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Win rate analysis\n",
        "ax = axes[1, 1]\n",
        "wins_0 = sum(1 for s in trainer.scores_history if s[0] > s[1])\n",
        "wins_1 = sum(1 for s in trainer.scores_history if s[1] > s[0])\n",
        "draws = len(trainer.scores_history) - wins_0 - wins_1\n",
        "\n",
        "labels = ['Team 0 Wins', 'Team 1 Wins', 'Draws']\n",
        "sizes = [wins_0, wins_1, draws]\n",
        "colors = ['#3498db', '#e74c3c', '#95a5a6']\n",
        "ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax.set_title('Win Rate Distribution')\n",
        "\n",
        "plt.suptitle(f'Training Results - {trainer.agent_type.upper()} Agents', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ“ˆ Statistics Summary:\")\n",
        "print(f\"   Team 0 wins: {wins_0} ({wins_0/len(trainer.scores_history)*100:.1f}%)\")\n",
        "print(f\"   Team 1 wins: {wins_1} ({wins_1/len(trainer.scores_history)*100:.1f}%)\")\n",
        "print(f\"   Draws: {draws} ({draws/len(trainer.scores_history)*100:.1f}%)\")\n",
        "print(f\"   Max reward: {max(trainer.episode_rewards):.2f}\")\n",
        "print(f\"   Min reward: {min(trainer.episode_rewards):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_videos_header"
      },
      "source": [
        "### ğŸ¬ å‹•ç”»ã®ä½œæˆã¨è¡¨ç¤º"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_videos"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def create_video_from_frames(frames, output_path, fps=30):\n",
        "    \"\"\"Create video from frames\"\"\"\n",
        "    if not frames:\n",
        "        print(\"No frames to create video\")\n",
        "        return None\n",
        "    \n",
        "    # Convert frames to proper format\n",
        "    processed_frames = []\n",
        "    for frame in frames:\n",
        "        if frame.dtype != np.uint8:\n",
        "            frame = (frame * 255).astype(np.uint8) if frame.max() <= 1 else frame.astype(np.uint8)\n",
        "        processed_frames.append(frame)\n",
        "    \n",
        "    # Create video\n",
        "    imageio.mimsave(output_path, processed_frames, fps=fps)\n",
        "    return output_path\n",
        "\n",
        "def display_video(video_path):\n",
        "    \"\"\"Display video in Colab\"\"\"\n",
        "    video = open(video_path, 'rb').read()\n",
        "    encoded = base64.b64encode(video).decode('ascii')\n",
        "    html_code = f'''\n",
        "    <video width=\"800\" height=\"600\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    '''\n",
        "    return HTML(html_code)\n",
        "\n",
        "# Create and display videos\n",
        "print(\"ğŸ¬ Creating videos from recorded episodes...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "video_paths = []\n",
        "for i, video_data in enumerate(videos):\n",
        "    output_path = f'/tmp/soccer_episode_{video_data[\"episode\"]}.mp4'\n",
        "    \n",
        "    # Create video\n",
        "    create_video_from_frames(video_data['frames'], output_path, fps=30)\n",
        "    video_paths.append(output_path)\n",
        "    \n",
        "    print(f\"âœ… Created video {i+1}: Episode {video_data['episode']}\")\n",
        "    print(f\"   Scores: {video_data['scores']}\")\n",
        "    print(f\"   Total Reward: {video_data['reward']:.2f}\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\nğŸ¥ Videos saved to: /tmp/\")\n",
        "print(\"Use the next cell to display videos\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "display_videos"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# å‹•ç”»ã®è¡¨ç¤º\n",
        "if videos and len(video_paths) > 0:\n",
        "    print(f\"ğŸ“º Displaying video from Episode {videos[0]['episode']}\")\n",
        "    print(f\"   Scores: Blue {videos[0]['scores'][0]} - Red {videos[0]['scores'][1]}\")\n",
        "    display(display_video(video_paths[0]))\n",
        "else:\n",
        "    print(\"No videos available to display.\")\n",
        "    print(\"Please run the training cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_training_header"
      },
      "source": [
        "### ğŸ§  DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®é«˜åº¦ãªè¨“ç·´"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advanced_dqn_training"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®ã‚ˆã‚Šé•·ã„è¨“ç·´\n",
        "print(\"ğŸ§  Starting advanced DQN training...\")\n",
        "print(\"This will take longer but show learning progress.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuration for longer training\n",
        "config_dqn = SoccerEnvironmentConfig()\n",
        "config_dqn.MAX_STEPS = 600  # Balanced episode length\n",
        "\n",
        "# Create DQN trainer\n",
        "dqn_trainer = ExtendedTrainer(config_dqn, agent_type=\"dqn\")\n",
        "\n",
        "# Train with DQN agents\n",
        "dqn_videos = dqn_trainer.train(\n",
        "    num_episodes=200,      # More episodes for learning\n",
        "    record_video_every=40, # Record every 40 episodes to see progress\n",
        "    max_video_episodes=5   # Record 5 videos total\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… DQN Training completed!\")\n",
        "print(f\"   Total episodes: {len(dqn_trainer.episode_rewards)}\")\n",
        "print(f\"   Videos recorded: {len(dqn_videos)}\")\n",
        "print(f\"   Final avg reward (last 20): {np.mean(dqn_trainer.episode_rewards[-20:]):.2f}\")\n",
        "print(f\"   Initial avg reward (first 20): {np.mean(dqn_trainer.episode_rewards[:20]):.2f}\")\n",
        "print(f\"   Improvement: {np.mean(dqn_trainer.episode_rewards[-20:]) - np.mean(dqn_trainer.episode_rewards[:20]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compare_results"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# è¨“ç·´çµæœã®æ¯”è¼ƒ\n",
        "if 'dqn_trainer' in globals():\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Rewards comparison\n",
        "    ax = axes[0]\n",
        "    \n",
        "    # Random agent rewards\n",
        "    random_rewards_smooth = np.convolve(trainer.episode_rewards, np.ones(10)/10, mode='valid')\n",
        "    ax.plot(range(9, len(trainer.episode_rewards)), random_rewards_smooth, \n",
        "            label='Random Agents', alpha=0.7, color='gray')\n",
        "    \n",
        "    # DQN agent rewards\n",
        "    dqn_rewards_smooth = np.convolve(dqn_trainer.episode_rewards, np.ones(10)/10, mode='valid')\n",
        "    ax.plot(range(9, len(dqn_trainer.episode_rewards)), dqn_rewards_smooth, \n",
        "            label='DQN Agents', linewidth=2, color='green')\n",
        "    \n",
        "    ax.set_title('Learning Progress Comparison')\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Average Reward (10-ep smoothed)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Score distribution\n",
        "    ax = axes[1]\n",
        "    \n",
        "    # Calculate average scores for last 20 episodes\n",
        "    random_scores = trainer.scores_history[-20:] if len(trainer.scores_history) >= 20 else trainer.scores_history\n",
        "    dqn_scores = dqn_trainer.scores_history[-20:] if len(dqn_trainer.scores_history) >= 20 else dqn_trainer.scores_history\n",
        "    \n",
        "    random_avg = [np.mean([s[0] for s in random_scores]), np.mean([s[1] for s in random_scores])]\n",
        "    dqn_avg = [np.mean([s[0] for s in dqn_scores]), np.mean([s[1] for s in dqn_scores])]\n",
        "    \n",
        "    x = np.arange(2)\n",
        "    width = 0.35\n",
        "    \n",
        "    ax.bar(x - width/2, random_avg, width, label='Random', color='gray', alpha=0.7)\n",
        "    ax.bar(x + width/2, dqn_avg, width, label='DQN', color='green', alpha=0.7)\n",
        "    \n",
        "    ax.set_title('Average Goals Scored (Last 20 Episodes)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(['Team 0 (Blue)', 'Team 1 (Red)'])\n",
        "    ax.set_ylabel('Average Goals')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.suptitle('Random vs DQN Agent Performance', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nğŸ“Š Performance Comparison:\")\n",
        "    print(f\"Random Agents - Avg Reward: {np.mean(trainer.episode_rewards):.2f} Â± {np.std(trainer.episode_rewards):.2f}\")\n",
        "    print(f\"DQN Agents - Avg Reward: {np.mean(dqn_trainer.episode_rewards):.2f} Â± {np.std(dqn_trainer.episode_rewards):.2f}\")\n",
        "else:\n",
        "    print(\"Please run DQN training first to see comparison.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "display_dqn_videos"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# DQNè¨“ç·´ã®å‹•ç”»è¡¨ç¤º\n",
        "if 'dqn_videos' in globals() and dqn_videos:\n",
        "    print(\"ğŸ¬ Creating DQN training videos...\")\n",
        "    \n",
        "    dqn_video_paths = []\n",
        "    for i, video_data in enumerate(dqn_videos):\n",
        "        output_path = f'/tmp/dqn_episode_{video_data[\"episode\"]}.mp4'\n",
        "        create_video_from_frames(video_data['frames'], output_path, fps=30)\n",
        "        dqn_video_paths.append(output_path)\n",
        "        print(f\"âœ… Created DQN video {i+1}: Episode {video_data['episode']}\")\n",
        "    \n",
        "    # Display comparison: early vs late training\n",
        "    print(\"\\nğŸ“º Early Training (Episode {}):\".format(dqn_videos[0]['episode']))\n",
        "    display(display_video(dqn_video_paths[0]))\n",
        "    \n",
        "    if len(dqn_video_paths) > 1:\n",
        "        print(\"\\nğŸ“º Later Training (Episode {}):\".format(dqn_videos[-1]['episode']))\n",
        "        display(display_video(dqn_video_paths[-1]))\n",
        "else:\n",
        "    print(\"No DQN videos available. Please run DQN training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expert_learning_header"
      },
      "source": [
        "# ğŸ† Expert Learning and Advanced Training\n",
        "## ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸå­¦ç¿’ã¨æ”¹å–„ã•ã‚ŒãŸè¨“ç·´\n\n",
        "ç‚¹ãŒå…¥ã‚‰ãªã„å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã™ï¼š\n",
        "1. ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæˆ¦ç•¥ã®å®Ÿè£…ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰\n",
        "2. æ¨¡å€£å­¦ç¿’ï¼ˆImitation Learningï¼‰\n",
        "3. å ±é…¬ã‚·ã‚§ãƒ¼ãƒ”ãƒ³ã‚°ã®æ”¹å–„\n",
        "4. ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expert_agent_header"
      },
      "source": [
        "### ğŸ¯ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expert_agent_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ExpertAgent(BaseAgent):\n",
        "    \"\"\"Expert agent with rule-based strategy for scoring goals\"\"\"\n",
        "    \n",
        "    def __init__(self, agent_id: int, team: int, config: SoccerEnvironmentConfig):\n",
        "        super().__init__(agent_id, 5)  # 5D continuous action\n",
        "        self.team = team\n",
        "        self.config = config\n",
        "        self.field_width, self.field_height = config.FIELD_SIZE\n",
        "        \n",
        "    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        \"\"\"Select action based on expert strategy\"\"\"\n",
        "        # Parse observation (28 dimensions)\n",
        "        # [0-1]: self position (normalized)\n",
        "        # [2-3]: self velocity\n",
        "        # [4-5]: ball position (normalized)\n",
        "        # [6-7]: ball velocity\n",
        "        # [8-9]: teammate position\n",
        "        # [10-11]: teammate velocity\n",
        "        # [12-15]: opponents positions\n",
        "        # [16-19]: opponents velocities\n",
        "        # [20-23]: goal information\n",
        "        # [24-27]: context\n",
        "        \n",
        "        self_pos = observation[0:2]\n",
        "        ball_pos = observation[4:6]\n",
        "        teammate_pos = observation[8:10]\n",
        "        \n",
        "        # Denormalize positions for strategy\n",
        "        self_x, self_y = self_pos[0] * self.field_width, self_pos[1] * self.field_height\n",
        "        ball_x, ball_y = ball_pos[0] * self.field_width, ball_pos[1] * self.field_height\n",
        "        \n",
        "        # Calculate distances\n",
        "        dist_to_ball = np.sqrt((self_x - ball_x)**2 + (self_y - ball_y)**2)\n",
        "        \n",
        "        # Determine target goal position\n",
        "        if self.team == 0:  # Blue team attacks right\n",
        "            goal_x = self.field_width\n",
        "            goal_y = self.field_height / 2\n",
        "        else:  # Red team attacks left\n",
        "            goal_x = 0\n",
        "            goal_y = self.field_height / 2\n",
        "        \n",
        "        # Strategy 1: Go to ball if far\n",
        "        if dist_to_ball > 50:\n",
        "            # Move towards ball\n",
        "            move_x = np.clip((ball_x - self_x) / 100, -1, 1)\n",
        "            move_y = np.clip((ball_y - self_y) / 100, -1, 1)\n",
        "            kick_power = 0.0\n",
        "            kick_dir_x = 0.0\n",
        "            kick_dir_y = 0.0\n",
        "        \n",
        "        # Strategy 2: Kick towards goal if close to ball\n",
        "        else:\n",
        "            # Move towards ball for better position\n",
        "            move_x = np.clip((ball_x - self_x) / 50, -1, 1)\n",
        "            move_y = np.clip((ball_y - self_y) / 50, -1, 1)\n",
        "            \n",
        "            # Calculate kick direction towards goal\n",
        "            kick_dir_x = np.clip((goal_x - ball_x) / self.field_width, -1, 1)\n",
        "            kick_dir_y = np.clip((goal_y - ball_y) / self.field_height, -1, 1)\n",
        "            \n",
        "            # Strong kick when aligned with goal\n",
        "            alignment = abs(kick_dir_y) < 0.3  # Close to horizontal alignment\n",
        "            kick_power = 0.8 if alignment else 0.5\n",
        "        \n",
        "        # Add some randomness for diversity\n",
        "        if training and np.random.random() < 0.1:\n",
        "            move_x += np.random.uniform(-0.2, 0.2)\n",
        "            move_y += np.random.uniform(-0.2, 0.2)\n",
        "        \n",
        "        action = np.array([move_x, move_y, kick_power, kick_dir_x, kick_dir_y], dtype=np.float32)\n",
        "        return np.clip(action, [-1, -1, 0, -1, -1], [1, 1, 1, 1, 1])\n",
        "    \n",
        "    def learn(self, experiences: List) -> Dict[str, float]:\n",
        "        \"\"\"Expert doesn't learn\"\"\"\n",
        "        return {\"loss\": 0.0}\n",
        "\n",
        "print(\"âœ… Expert agent with goal-scoring strategy implemented!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "collect_demos_header"
      },
      "source": [
        "### ğŸ“Š ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®åé›†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collect_demonstrations"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def collect_expert_demonstrations(num_episodes=50):\n",
        "    \"\"\"Collect expert demonstrations for imitation learning\"\"\"\n",
        "    print(f\"ğŸ“Š Collecting {num_episodes} episodes of expert demonstrations...\")\n",
        "    \n",
        "    config = SoccerEnvironmentConfig()\n",
        "    config.MAX_STEPS = 600  # Shorter episodes\n",
        "    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n",
        "    \n",
        "    # Create expert agents\n",
        "    expert_agents = {}\n",
        "    for i, agent_name in enumerate(env.agents):\n",
        "        team = i // 2  # 0 or 1\n",
        "        expert_agents[agent_name] = ExpertAgent(i, team, config)\n",
        "    \n",
        "    demonstrations = []\n",
        "    total_goals = {'team_0': 0, 'team_1': 0}\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        env.reset()\n",
        "        episode_data = []\n",
        "        \n",
        "        while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
        "            for agent_name in env.agents:\n",
        "                if not env.terminations.get(agent_name, False) and not env.truncations.get(agent_name, False):\n",
        "                    # Get observation\n",
        "                    obs = env.observe(agent_name)\n",
        "                    \n",
        "                    # Get expert action\n",
        "                    action = expert_agents[agent_name].select_action(obs, training=False)\n",
        "                    \n",
        "                    # Store state-action pair\n",
        "                    episode_data.append({\n",
        "                        'agent': agent_name,\n",
        "                        'observation': obs.copy(),\n",
        "                        'action': action.copy()\n",
        "                    })\n",
        "                    \n",
        "                    # Execute action\n",
        "                    env.step(action)\n",
        "                    \n",
        "                    if env.terminations.get(agent_name, False) or env.truncations.get(agent_name, False):\n",
        "                        break\n",
        "        \n",
        "        demonstrations.append(episode_data)\n",
        "        total_goals['team_0'] += env.scores[0]\n",
        "        total_goals['team_1'] += env.scores[1]\n",
        "        \n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}: Scores = {env.scores}\")\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    print(f\"\\nâœ… Collected {num_episodes} expert demonstrations\")\n",
        "    print(f\"   Total goals: Team 0 = {total_goals['team_0']}, Team 1 = {total_goals['team_1']}\")\n",
        "    print(f\"   Average goals per episode: {(total_goals['team_0'] + total_goals['team_1']) / num_episodes:.2f}\")\n",
        "    \n",
        "    return demonstrations\n",
        "\n",
        "# Collect demonstrations\n",
        "expert_demonstrations = collect_expert_demonstrations(50)\n",
        "print(f\"\\nğŸ“¦ Demonstration data size: {len(expert_demonstrations)} episodes\")\n",
        "print(f\"   First episode length: {len(expert_demonstrations[0])} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc_header"
      },
      "source": [
        "### ğŸ§  è¡Œå‹•ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆBehavioral Cloningï¼‰ã®å®Ÿè£…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "behavioral_cloning"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class BehavioralCloningAgent(BaseAgent):\n",
        "    \"\"\"Agent trained with behavioral cloning from expert demonstrations\"\"\"\n",
        "    \n",
        "    def __init__(self, agent_id: int, obs_dim: int = 28, action_dim: int = 5,\n",
        "                 hidden_dims: Tuple[int, ...] = (256, 128), lr: float = 1e-3):\n",
        "        super().__init__(agent_id, action_dim)\n",
        "        \n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        # Policy network (observation -> action)\n",
        "        self.policy_network = self._build_network(obs_dim, action_dim, hidden_dims)\n",
        "        self.policy_network.to(self.device)\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def _build_network(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n",
        "        \"\"\"Build neural network\"\"\"\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.1))  # Add dropout for regularization\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        layers.append(nn.Tanh())  # Output in [-1, 1]\n",
        "        \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        \"\"\"Select action using learned policy\"\"\"\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
        "            action = self.policy_network(obs_tensor).cpu().numpy().flatten()\n",
        "        \n",
        "        # Add exploration noise during training\n",
        "        if training:\n",
        "            noise = np.random.normal(0, 0.1, size=action.shape)\n",
        "            action = action + noise\n",
        "        \n",
        "        # Ensure kick power is positive\n",
        "        action[2] = np.clip(action[2], 0, 1)\n",
        "        \n",
        "        return np.clip(action, [-1, -1, 0, -1, -1], [1, 1, 1, 1, 1])\n",
        "    \n",
        "    def train_on_demonstrations(self, demonstrations: List, epochs: int = 100, batch_size: int = 64):\n",
        "        \"\"\"Train the agent on expert demonstrations\"\"\"\n",
        "        print(f\"\\nğŸ§  Training behavioral cloning agent...\")\n",
        "        print(f\"   Epochs: {epochs}, Batch size: {batch_size}\")\n",
        "        \n",
        "        # Prepare training data\n",
        "        all_observations = []\n",
        "        all_actions = []\n",
        "        \n",
        "        for episode in demonstrations:\n",
        "            for step_data in episode:\n",
        "                if step_data['agent'] == f'player_{self.agent_id}':\n",
        "                    all_observations.append(step_data['observation'])\n",
        "                    all_actions.append(step_data['action'])\n",
        "        \n",
        "        # Convert to tensors\n",
        "        observations = torch.FloatTensor(all_observations).to(self.device)\n",
        "        actions = torch.FloatTensor(all_actions).to(self.device)\n",
        "        \n",
        "        dataset_size = observations.shape[0]\n",
        "        print(f\"   Training on {dataset_size} samples\")\n",
        "        \n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = torch.randperm(dataset_size)\n",
        "            \n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "            \n",
        "            for i in range(0, dataset_size, batch_size):\n",
        "                batch_indices = indices[i:i+batch_size]\n",
        "                batch_obs = observations[batch_indices]\n",
        "                batch_actions = actions[batch_indices]\n",
        "                \n",
        "                # Forward pass\n",
        "                predicted_actions = self.policy_network(batch_obs)\n",
        "                \n",
        "                # Compute loss (MSE)\n",
        "                loss = nn.MSELoss()(predicted_actions, batch_actions)\n",
        "                \n",
        "                # Backward pass\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "            \n",
        "            avg_loss = total_loss / num_batches\n",
        "            self.loss_history.append(avg_loss)\n",
        "            \n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                print(f\"   Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        print(f\"\\nâœ… Behavioral cloning training completed!\")\n",
        "        print(f\"   Final loss: {self.loss_history[-1]:.4f}\")\n",
        "        \n",
        "    def learn(self, experiences: List) -> Dict[str, float]:\n",
        "        \"\"\"Can continue learning during deployment\"\"\"\n",
        "        return {\"loss\": self.loss_history[-1] if self.loss_history else 0.0}\n",
        "\n",
        "print(\"âœ… Behavioral cloning agent implemented!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_bc_header"
      },
      "source": [
        "### ğŸ“ BCã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨“ç·´"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train_bc_agents"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create and train BC agents\n",
        "print(\"ğŸ“ Creating and training Behavioral Cloning agents...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "bc_agents = {}\n",
        "\n",
        "for i in range(4):  # 4 agents total\n",
        "    agent_name = f\"player_{i}\"\n",
        "    print(f\"\\nTraining {agent_name}...\")\n",
        "    \n",
        "    # Create BC agent\n",
        "    bc_agent = BehavioralCloningAgent(\n",
        "        agent_id=i,\n",
        "        obs_dim=28,\n",
        "        action_dim=5,\n",
        "        hidden_dims=(256, 128),\n",
        "        lr=5e-4\n",
        "    )\n",
        "    \n",
        "    # Train on expert demonstrations\n",
        "    bc_agent.train_on_demonstrations(\n",
        "        expert_demonstrations,\n",
        "        epochs=50,\n",
        "        batch_size=32\n",
        "    )\n",
        "    \n",
        "    bc_agents[agent_name] = bc_agent\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… All BC agents trained successfully!\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i, (name, agent) in enumerate(bc_agents.items()):\n",
        "    plt.plot(agent.loss_history, label=name, alpha=0.7)\n",
        "plt.title('Behavioral Cloning Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_bc_header"
      },
      "source": [
        "### ğŸ® BCã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆã¨è©•ä¾¡"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_bc_agents"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate_agents(agents_dict, num_episodes=20, record_video=False):\n",
        "    \"\"\"Evaluate agent performance\"\"\"\n",
        "    config = SoccerEnvironmentConfig()\n",
        "    config.MAX_STEPS = 600\n",
        "    \n",
        "    render_mode = \"rgb_array\" if record_video else None\n",
        "    env = make_soccer_env(config, render_mode=render_mode, action_type=\"continuous\")\n",
        "    \n",
        "    stats = {\n",
        "        'scores': [],\n",
        "        'rewards': [],\n",
        "        'steps': [],\n",
        "        'goals_team_0': 0,\n",
        "        'goals_team_1': 0\n",
        "    }\n",
        "    \n",
        "    video_frames = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        env.reset()\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "        episode_frames = [] if record_video and episode == 0 else None\n",
        "        \n",
        "        while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
        "            if episode_frames is not None:\n",
        "                frame = env.render()\n",
        "                if frame is not None:\n",
        "                    episode_frames.append(frame)\n",
        "            \n",
        "            for agent_name in env.agents:\n",
        "                if not env.terminations.get(agent_name, False) and not env.truncations.get(agent_name, False):\n",
        "                    obs = env.observe(agent_name)\n",
        "                    action = agents_dict[agent_name].select_action(obs, training=False)\n",
        "                    env.step(action)\n",
        "                    episode_reward += env.rewards.get(agent_name, 0)\n",
        "                    steps += 1\n",
        "                    \n",
        "                    if env.terminations.get(agent_name, False) or env.truncations.get(agent_name, False):\n",
        "                        break\n",
        "        \n",
        "        stats['scores'].append(env.scores.copy())\n",
        "        stats['rewards'].append(episode_reward)\n",
        "        stats['steps'].append(steps)\n",
        "        stats['goals_team_0'] += env.scores[0]\n",
        "        stats['goals_team_1'] += env.scores[1]\n",
        "        \n",
        "        if episode_frames:\n",
        "            video_frames = episode_frames\n",
        "        \n",
        "        if (episode + 1) % 5 == 0:\n",
        "            print(f\"Episode {episode + 1}: Scores = {env.scores}, Reward = {episode_reward:.2f}\")\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    return stats, video_frames\n",
        "\n",
        "# Test BC agents\n",
        "print(\"ğŸ® Testing Behavioral Cloning agents...\")\n",
        "print(\"=\" * 60)\n",
        "bc_stats, bc_video = evaluate_agents(bc_agents, num_episodes=20, record_video=True)\n",
        "\n",
        "print(\"\\nğŸ“Š BC Agents Performance:\")\n",
        "print(f\"   Total goals scored: Team 0 = {bc_stats['goals_team_0']}, Team 1 = {bc_stats['goals_team_1']}\")\n",
        "print(f\"   Average goals per episode: {(bc_stats['goals_team_0'] + bc_stats['goals_team_1']) / 20:.2f}\")\n",
        "print(f\"   Average reward: {np.mean(bc_stats['rewards']):.2f}\")\n",
        "print(f\"   Average steps: {np.mean(bc_stats['steps']):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compare_header"
      },
      "source": [
        "### ğŸ“ˆ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ€§èƒ½ã®æ¯”è¼ƒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compare_agents"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Compare different agent types\n",
        "print(\"ğŸ“ˆ Comparing agent performances...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test random agents for comparison\n",
        "config = SoccerEnvironmentConfig()\n",
        "env_temp = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n",
        "random_agents_comp = {}\n",
        "for i, agent_name in enumerate(env_temp.agents):\n",
        "    random_agents_comp[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n",
        "env_temp.close()\n",
        "\n",
        "print(\"\\nTesting Random agents...\")\n",
        "random_stats, _ = evaluate_agents(random_agents_comp, num_episodes=20, record_video=False)\n",
        "\n",
        "# Test expert agents\n",
        "expert_agents_comp = {}\n",
        "for i in range(4):\n",
        "    agent_name = f\"player_{i}\"\n",
        "    team = i // 2\n",
        "    expert_agents_comp[agent_name] = ExpertAgent(i, team, config)\n",
        "\n",
        "print(\"\\nTesting Expert agents...\")\n",
        "expert_stats, expert_video = evaluate_agents(expert_agents_comp, num_episodes=20, record_video=True)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Goals comparison\n",
        "ax = axes[0]\n",
        "agent_types = ['Random', 'BC (Learned)', 'Expert']\n",
        "goals_team0 = [random_stats['goals_team_0'], bc_stats['goals_team_0'], expert_stats['goals_team_0']]\n",
        "goals_team1 = [random_stats['goals_team_1'], bc_stats['goals_team_1'], expert_stats['goals_team_1']]\n",
        "\n",
        "x = np.arange(len(agent_types))\n",
        "width = 0.35\n",
        "ax.bar(x - width/2, goals_team0, width, label='Team 0 (Blue)', color='blue', alpha=0.7)\n",
        "ax.bar(x + width/2, goals_team1, width, label='Team 1 (Red)', color='red', alpha=0.7)\n",
        "ax.set_xlabel('Agent Type')\n",
        "ax.set_ylabel('Total Goals (20 episodes)')\n",
        "ax.set_title('Goals Scored Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(agent_types)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Average rewards\n",
        "ax = axes[1]\n",
        "avg_rewards = [\n",
        "    np.mean(random_stats['rewards']),\n",
        "    np.mean(bc_stats['rewards']),\n",
        "    np.mean(expert_stats['rewards'])\n",
        "]\n",
        "bars = ax.bar(agent_types, avg_rewards, color=['gray', 'green', 'gold'], alpha=0.7)\n",
        "ax.set_ylabel('Average Reward per Episode')\n",
        "ax.set_title('Reward Comparison')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, avg_rewards):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.1f}', ha='center', va='bottom')\n",
        "\n",
        "# Goals per episode distribution\n",
        "ax = axes[2]\n",
        "total_goals = [\n",
        "    (random_stats['goals_team_0'] + random_stats['goals_team_1']) / 20,\n",
        "    (bc_stats['goals_team_0'] + bc_stats['goals_team_1']) / 20,\n",
        "    (expert_stats['goals_team_0'] + expert_stats['goals_team_1']) / 20\n",
        "]\n",
        "bars = ax.bar(agent_types, total_goals, color=['gray', 'green', 'gold'], alpha=0.7)\n",
        "ax.set_ylabel('Average Goals per Episode')\n",
        "ax.set_title('Scoring Frequency')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, total_goals):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{val:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.suptitle('Agent Performance Comparison', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“Š Summary:\")\n",
        "print(f\"Random - Goals/episode: {total_goals[0]:.2f}, Avg reward: {avg_rewards[0]:.1f}\")\n",
        "print(f\"BC     - Goals/episode: {total_goals[1]:.2f}, Avg reward: {avg_rewards[1]:.1f}\")\n",
        "print(f\"Expert - Goals/episode: {total_goals[2]:.2f}, Avg reward: {avg_rewards[2]:.1f}\")\n",
        "\n",
        "improvement = (total_goals[1] - total_goals[0]) / (total_goals[0] + 0.01) * 100\n",
        "print(f\"\\nğŸ¯ BC improvement over Random: {improvement:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "videos_header"
      },
      "source": [
        "### ğŸ¬ å‹•ç”»ã§ã®æ¯”è¼ƒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_comparison_videos"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create and display comparison videos\n",
        "print(\"ğŸ¬ Creating comparison videos...\")\n",
        "\n",
        "if bc_video:\n",
        "    # Create BC agent video\n",
        "    bc_video_path = '/tmp/bc_agents_gameplay.mp4'\n",
        "    create_video_from_frames(bc_video, bc_video_path, fps=30)\n",
        "    print(\"âœ… BC agents video created\")\n",
        "    \n",
        "    print(\"\\nğŸ“º Behavioral Cloning Agents Gameplay:\")\n",
        "    display(display_video(bc_video_path))\n",
        "\n",
        "if expert_video:\n",
        "    # Create expert agent video\n",
        "    expert_video_path = '/tmp/expert_agents_gameplay.mp4'\n",
        "    create_video_from_frames(expert_video, expert_video_path, fps=30)\n",
        "    print(\"\\nâœ… Expert agents video created\")\n",
        "    \n",
        "    print(\"\\nğŸ“º Expert Agents Gameplay:\")\n",
        "    display(display_video(expert_video_path))\n",
        "\n",
        "print(\"\\nğŸ’¡ è¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ:\")\n",
        "print(\"  - Expert: ãƒœãƒ¼ãƒ«ã«å‘ã‹ã£ã¦ç©æ¥µçš„ã«ç§»å‹•ã—ã€ã‚´ãƒ¼ãƒ«ã‚’ç‹™ã†\")\n",
        "print(\"  - BC: Expertã®æˆ¦ç•¥ã‚’æ¨¡å€£ã—ã€ã‚ˆã‚Šå¤šãã®ã‚´ãƒ¼ãƒ«ã‚’æ±ºã‚ã‚‹\")\n",
        "print(\"  - Random: ãƒ©ãƒ³ãƒ€ãƒ ãªå‹•ãã§ã€ã‚´ãƒ¼ãƒ«ã¯ã»ã¨ã‚“ã©å…¥ã‚‰ãªã„\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "longer_episode_header"
      },
      "source": [
        "# â±ï¸ Extended Episode Duration (20 seconds) with Improved Physics\n",
        "## ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’20ç§’ã«æ‹¡å¼µ + ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³æ”¹å–„ç‰ˆ\n\n",
        "ã‚ˆã‚Šç¾å®Ÿçš„ãªã‚µãƒƒã‚«ãƒ¼ã‚²ãƒ¼ãƒ ã®ãŸã‚ã«ã€1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’20ç§’ï¼ˆç´„600ã‚¹ãƒ†ãƒƒãƒ— @ 30FPSï¼‰ã«è¨­å®šã€‚\n",
        "ã•ã‚‰ã«ã€ãƒœãƒ¼ãƒ«ãŒæŒŸã¾ã‚‰ãªã„ã‚ˆã†ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¾ã—ãŸã€‚\n\n",
        "### ğŸ”§ æ”¹å–„ç‚¹:\n",
        "- âš™ï¸ ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–\n",
        "- ğŸš¨ ã‚¹ã‚¿ãƒƒã‚¯æ¤œå‡ºãƒ»è„±å‡ºã‚·ã‚¹ãƒ†ãƒ \n",
        "- ğŸ² å¯¾ç§°æ€§ç ´å£Šãƒ¡ã‚«ãƒ‹ã‚ºãƒ \n",
        "- ğŸ‘ï¸ ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ”¹å–„\n",
        "- ğŸ¤– ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæˆ¦ç•¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "updated_config_header"
      },
      "source": [
        "### âš™ï¸ æ‹¡å¼µã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extended_config"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Extended episode configuration with improved physics\n",
        "from collections import deque\n",
        "import math\n",
        "\n",
        "@dataclass\n",
        "class ExtendedSoccerConfig(SoccerEnvironmentConfig):\n",
        "    \"\"\"Extended configuration for 20-second episodes with improved physics\"\"\"\n",
        "    # Override MAX_STEPS for 20 seconds at ~30 FPS\n",
        "    MAX_STEPS: int = 600  # 20 seconds * 30 steps/second\n",
        "    \n",
        "    # Player movement\n",
        "    PLAYER_SPEED: float = 4.0  # Strategic play speed\n",
        "    \n",
        "    # âš™ï¸ IMPROVED PHYSICS PARAMETERS\n",
        "    BALL_SPEED_MULTIPLIER: float = 1.8  # Faster ball (was 1.3)\n",
        "    FRICTION: float = 0.96  # Less friction (was 0.93)\n",
        "    BALL_DECAY: float = 0.97  # Ball moves longer\n",
        "    \n",
        "    # Enhanced collision physics\n",
        "    BALL_RESTITUTION: float = 0.85  # Higher bounce (was 0.7)\n",
        "    COLLISION_ELASTICITY: float = 0.9  # Elastic collisions (was 0.6)\n",
        "    \n",
        "    # ğŸš¨ Anti-stuck mechanics\n",
        "    MIN_BALL_SPEED: float = 0.5  # Minimum speed threshold\n",
        "    STUCK_DETECTION_FRAMES: int = 15  # Frames to detect stuck\n",
        "    STUCK_VELOCITY_THRESHOLD: float = 0.8  # Velocity threshold\n",
        "    ESCAPE_FORCE: float = 8.0  # Escape force strength\n",
        "    PLAYER_SEPARATION_FORCE: float = 3.0  # Player separation\n",
        "    \n",
        "    # Goal celebration\n",
        "    GOAL_PAUSE_STEPS: int = 30  # 1 second pause\n",
        "\n",
        "print(\"âœ… Improved physics configuration:\")\n",
        "print(f\"   Ball friction: 0.96 (improved from 0.93)\")\n",
        "print(f\"   Ball restitution: 0.85 (improved from 0.7)\")\n",
        "print(f\"   Collision elasticity: 0.9 (improved from 0.6)\")\n",
        "print(f\"   Anti-stuck system: Enabled\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anti_stuck_physics"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ğŸš¨ Anti-stuck detection and escape system\n",
        "class AntiStuckSystem:\n",
        "    \"\"\"System to detect and resolve ball stuck situations\"\"\"\n",
        "    \n",
        "    def __init__(self, config: ExtendedSoccerConfig):\n",
        "        self.config = config\n",
        "        self.stuck_frames = 0\n",
        "        self.ball_velocity_history = deque(maxlen=config.STUCK_DETECTION_FRAMES)\n",
        "        self.last_ball_pos = None\n",
        "    \n",
        "    def update(self, ball_pos, ball_vel, players):\n",
        "        \"\"\"Update stuck detection and apply corrections\"\"\"\n",
        "        ball_speed = np.linalg.norm(ball_vel)\n",
        "        self.ball_velocity_history.append(ball_speed)\n",
        "        \n",
        "        # Check if stuck\n",
        "        if self._is_stuck(ball_pos, players):\n",
        "            self.stuck_frames += 1\n",
        "            return self._apply_escape(ball_vel, ball_pos, players)\n",
        "        else:\n",
        "            self.stuck_frames = 0\n",
        "            # Add small perturbation to prevent symmetry\n",
        "            if ball_speed < self.config.MIN_BALL_SPEED:\n",
        "                ball_vel += np.random.randn(2) * 0.1\n",
        "            return ball_vel\n",
        "    \n",
        "    def _is_stuck(self, ball_pos, players):\n",
        "        \"\"\"Check if ball is stuck\"\"\"\n",
        "        if len(self.ball_velocity_history) < self.config.STUCK_DETECTION_FRAMES:\n",
        "            return False\n",
        "        \n",
        "        avg_velocity = np.mean(list(self.ball_velocity_history))\n",
        "        if avg_velocity > self.config.STUCK_VELOCITY_THRESHOLD:\n",
        "            return False\n",
        "        \n",
        "        # Count nearby players\n",
        "        nearby = 0\n",
        "        for p in players:\n",
        "            dist = np.linalg.norm(ball_pos - p.position)\n",
        "            if dist < self.config.PLAYER_RADIUS + self.config.BALL_RADIUS + 5:\n",
        "                nearby += 1\n",
        "        \n",
        "        return nearby >= 2\n",
        "    \n",
        "    def _apply_escape(self, ball_vel, ball_pos, players):\n",
        "        \"\"\"Apply escape force\"\"\"\n",
        "        # Find closest players\n",
        "        dists = [(p, np.linalg.norm(ball_pos - p.position)) for p in players]\n",
        "        dists.sort(key=lambda x: x[1])\n",
        "        \n",
        "        if len(dists) >= 2:\n",
        "            p1, p2 = dists[0][0], dists[1][0]\n",
        "            \n",
        "            # Escape perpendicular to player line\n",
        "            line = p2.position - p1.position\n",
        "            if np.linalg.norm(line) > 0:\n",
        "                line = line / np.linalg.norm(line)\n",
        "                escape_dir = np.array([-line[1], line[0]])\n",
        "                if random.random() > 0.5:\n",
        "                    escape_dir = -escape_dir\n",
        "            else:\n",
        "                angle = random.uniform(0, 2 * math.pi)\n",
        "                escape_dir = np.array([math.cos(angle), math.sin(angle)])\n",
        "            \n",
        "            # Apply force\n",
        "            ball_vel += escape_dir * self.config.ESCAPE_FORCE\n",
        "            \n",
        "            # Separate players slightly\n",
        "            sep = p2.position - p1.position\n",
        "            if np.linalg.norm(sep) > 0:\n",
        "                sep = sep / np.linalg.norm(sep) * self.config.PLAYER_SEPARATION_FORCE\n",
        "                p1.position -= sep * 0.5\n",
        "                p2.position += sep * 0.5\n",
        "        \n",
        "        return ball_vel\n",
        "\n",
        "print('âœ… Anti-stuck system initialized')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhanced_expert_header"
      },
      "source": [
        "### ğŸ¯ æ”¹è‰¯ç‰ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆ20ç§’å¯¾å¿œï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enhanced_expert_agent"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class EnhancedExpertAgent(BaseAgent):",
        "    \"\"\"Enhanced expert agent for 20-second episodes with stamina management\"\"\"",
        "    ",
        "    def __init__(self, agent_id: int, team: int, config: ExtendedSoccerConfig):",
        "        super().__init__(agent_id, 5)",
        "        self.team = team",
        "        self.config = config",
        "        self.field_width, self.field_height = config.FIELD_SIZE",
        "        self.stamina = 1.0  # Stamina system for longer games",
        "        self.role = 'attacker' if agent_id % 2 == 0 else 'defender'",
        "        self.last_ball_pos = None",
        "        self.stuck_counter = 0",
        "        ",
        "    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:",
        "        \"\"\"Enhanced strategy for longer episodes\"\"\"\n",
        "        # Initialize action first\n",
        "        action = np.zeros(5)\n",
        "        \n",
        "        \"\"\"Enhanced strategy for longer episodes\"\"\"",
        "        # Initialize action\n",
        "        \n",
        "        # Parse observation",
        "        self_pos = observation[0:2]",
        "        ball_pos = observation[4:6]",
        "        ",
        "        # ğŸš¨ Stuck detection",
        "        if self.last_ball_pos is not None:",
        "            ball_movement = np.linalg.norm(ball_pos - self.last_ball_pos)",
        "            if ball_movement < 2.0:",
        "                self.stuck_counter += 1",
        "            else:",
        "                self.stuck_counter = 0",
        "        self.last_ball_pos = ball_pos.copy()",
        "        ",
        "        # Apply escape strategy if stuck",
        "        if self.stuck_counter > 5:",
        "            # Escape strategy when stuck\n",
        "            angle = random.uniform(0, 2 * math.pi)\n",
        "            action[0:2] = np.array([math.cos(angle), math.sin(angle)])\n",
        "            \n",
        "            # Check if close enough to kick\n",
        "            dist_to_ball = np.linalg.norm(ball_pos - self_pos)\n",
        "            if dist_to_ball < 50:\n",
        "                action[2] = 1.0  # Strong kick\n",
        "            action[0:2] += np.random.randn(2) * 0.2  # Add noise\n",
        "            return action\n",
        "            angle = random.uniform(0, 2 * math.pi)",
        "            action[0:2] = np.array([math.cos(angle), math.sin(angle)])",
        "            if dist_to_ball < 50:",
        "                action[2] = 1.0  # Strong kick",
        "            action[0:2] += np.random.randn(2) * 0.2  # Add noise",
        "            return action",
        "        teammate_pos = observation[8:10]",
        "        opp1_pos = observation[12:14]",
        "        opp2_pos = observation[16:18]",
        "        ",
        "        # Denormalize positions",
        "        self_x, self_y = self_pos[0] * self.field_width, self_pos[1] * self.field_height",
        "        ball_x, ball_y = ball_pos[0] * self.field_width, ball_pos[1] * self.field_height",
        "        teammate_x, teammate_y = teammate_pos[0] * self.field_width, teammate_pos[1] * self.field_height",
        "        ",
        "        # Calculate distances",
        "        dist_to_ball = np.sqrt((self_x - ball_x)**2 + (self_y - ball_y)**2)",
        "        dist_to_teammate = np.sqrt((self_x - teammate_x)**2 + (self_y - teammate_y)**2)",
        "        ",
        "        # Goal positions",
        "        if self.team == 0:  # Blue team",
        "            own_goal_x, enemy_goal_x = 0, self.field_width",
        "        else:  # Red team",
        "            own_goal_x, enemy_goal_x = self.field_width, 0",
        "        goal_y = self.field_height / 2",
        "        ",
        "        # Role-based strategy",
        "        if self.role == 'attacker':",
        "            # Attackers focus on scoring",
        "            if dist_to_ball > 60:",
        "                # Rush to ball",
        "                move_x = np.clip((ball_x - self_x) / 80, -1, 1)",
        "                move_y = np.clip((ball_y - self_y) / 80, -1, 1)",
        "                kick_power = 0.0",
        "                kick_dir_x, kick_dir_y = 0.0, 0.0",
        "            else:",
        "                # Dribble towards goal or shoot",
        "                move_x = np.clip((ball_x - self_x) / 40, -1, 1)",
        "                move_y = np.clip((ball_y - self_y) / 40, -1, 1)",
        "                ",
        "                # Calculate shot angle",
        "                goal_dist = np.sqrt((enemy_goal_x - ball_x)**2 + (goal_y - ball_y)**2)",
        "                ",
        "                if goal_dist < 200:  # Close enough to shoot",
        "                    kick_power = 0.9",
        "                    kick_dir_x = np.clip((enemy_goal_x - ball_x) / self.field_width, -1, 1)",
        "                    kick_dir_y = np.clip((goal_y - ball_y) / self.field_height, -1, 1)",
        "                else:",
        "                    # Dribble forward",
        "                    kick_power = 0.3",
        "                    kick_dir_x = np.clip((enemy_goal_x - ball_x) / self.field_width * 0.5, -1, 1)",
        "                    kick_dir_y = 0.0",
        "        ",
        "        else:  # Defender",
        "            # Defenders focus on ball interception and clearing",
        "            ball_to_own_goal = np.sqrt((own_goal_x - ball_x)**2 + (goal_y - ball_y)**2)",
        "            ",
        "            if ball_to_own_goal < 200:  # Ball near own goal - defend!",
        "                # Rush to ball",
        "                move_x = np.clip((ball_x - self_x) / 50, -1, 1)",
        "                move_y = np.clip((ball_y - self_y) / 50, -1, 1)",
        "                ",
        "                if dist_to_ball < 40:",
        "                    # Clear ball away from goal",
        "                    kick_power = 0.8",
        "                    kick_dir_x = np.sign(enemy_goal_x - own_goal_x)",
        "                    kick_dir_y = np.random.uniform(-0.3, 0.3)  # Random clear direction",
        "                else:",
        "                    kick_power = 0.0",
        "                    kick_dir_x, kick_dir_y = 0.0, 0.0",
        "            ",
        "            elif dist_to_ball < 100:",
        "                # Support play",
        "                move_x = np.clip((ball_x - self_x) / 80, -1, 1)",
        "                move_y = np.clip((ball_y - self_y) / 80, -1, 1)",
        "                ",
        "                if dist_to_ball < 40:",
        "                    # Pass to teammate",
        "                    kick_power = 0.5",
        "                    kick_dir_x = np.clip((teammate_x - ball_x) / self.field_width, -1, 1)",
        "                    kick_dir_y = np.clip((teammate_y - ball_y) / self.field_height, -1, 1)",
        "                else:",
        "                    kick_power = 0.0",
        "                    kick_dir_x, kick_dir_y = 0.0, 0.0",
        "            ",
        "            else:",
        "                # Position between ball and own goal",
        "                defensive_x = (ball_x + own_goal_x) / 2",
        "                defensive_y = (ball_y + goal_y) / 2",
        "                move_x = np.clip((defensive_x - self_x) / 100, -1, 1)",
        "                move_y = np.clip((defensive_y - self_y) / 100, -1, 1)",
        "                kick_power = 0.0",
        "                kick_dir_x, kick_dir_y = 0.0, 0.0",
        "        ",
        "        # Stamina management for 20-second games",
        "        self.stamina = max(0.3, self.stamina - 0.001)  # Gradual stamina decrease",
        "        speed_modifier = 0.7 + 0.3 * self.stamina  # Speed affected by stamina",
        "        ",
        "        move_x *= speed_modifier",
        "        move_y *= speed_modifier",
        "        ",
        "        action = np.array([move_x, move_y, kick_power, kick_dir_x, kick_dir_y], dtype=np.float32)",
        "        return np.clip(action, [-1, -1, 0, -1, -1], [1, 1, 1, 1, 1])",
        "    ",
        "    def reset_stamina(self):",
        "        \"\"\"Reset stamina for new episode\"\"\"",
        "        self.stamina = 1.0",
        "    ",
        "    def learn(self, experiences: List) -> Dict[str, float]:",
        "        return {\"loss\": 0.0}",
        "",
        "print(\"âœ… Enhanced expert agent implemented with:\")",
        "print(\"   - Role-based strategies (attacker/defender)\")",
        "print(\"   - Stamina management for 20-second games\")",
        "print(\"   - Advanced positioning and passing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extended_training_header"
      },
      "source": [
        "### ğŸ® 20ç§’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã®è¨“ç·´"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extended_episode_training"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class LongEpisodeTrainer:\n",
        "    \"\"\"Trainer for 20-second episodes\"\"\"\n",
        "    \n",
        "    def __init__(self, config: ExtendedSoccerConfig):\n",
        "        self.config = config\n",
        "        self.episode_stats = []\n",
        "        \n",
        "    def run_episode(self, agents_dict, record_video=False, verbose=True):\n",
        "        \"\"\"Run a single 20-second episode\"\"\"\n",
        "        render_mode = \"rgb_array\" if record_video else None\n",
        "        env = make_soccer_env(self.config, render_mode=render_mode, action_type=\"continuous\")\n",
        "        \n",
        "        env.reset()\n",
        "        \n",
        "        # Episode statistics\n",
        "        stats = {\n",
        "            'scores': [0, 0],\n",
        "            'rewards': {agent: 0 for agent in env.agents},\n",
        "            'steps': 0,\n",
        "            'goals_timeline': [],  # When goals were scored\n",
        "            'possession_time': {0: 0, 1: 0, -1: 0},  # Ball possession time\n",
        "            'shots': {0: 0, 1: 0},  # Shot attempts\n",
        "        }\n",
        "        \n",
        "        video_frames = [] if record_video else None\n",
        "        last_ball_possession = -1\n",
        "        \n",
        "        # Run 20-second episode\n",
        "        for step in range(self.config.MAX_STEPS):\n",
        "            # Record frame\n",
        "            if record_video and step % 2 == 0:  # Record every 2nd frame to reduce size\n",
        "                frame = env.render()\n",
        "                if frame is not None:\n",
        "                    video_frames.append(frame)\n",
        "            \n",
        "            # Get actions from all agents\n",
        "            for agent_name in env.agents:\n",
        "                if not env.terminations.get(agent_name, False) and not env.truncations.get(agent_name, False):\n",
        "                    obs = env.observe(agent_name)\n",
        "                    action = agents_dict[agent_name].select_action(obs, training=False)\n",
        "                    \n",
        "                    # Detect shots (high kick power)\n",
        "                    if action[2] > 0.7:  # kick_power > 0.7\n",
        "                        team = int(agent_name.split('_')[1]) // 2\n",
        "                        stats['shots'][team] += 1\n",
        "                    \n",
        "                    env.step(action)\n",
        "                    stats['rewards'][agent_name] += env.rewards.get(agent_name, 0)\n",
        "                    \n",
        "                    if env.terminations.get(agent_name, False) or env.truncations.get(agent_name, False):\n",
        "                        break\n",
        "            \n",
        "            # Track goals\n",
        "            if env.scores[0] > stats['scores'][0]:\n",
        "                stats['goals_timeline'].append({'team': 0, 'time': step / 30})  # Convert to seconds\n",
        "                stats['scores'][0] = env.scores[0]\n",
        "                if verbose:\n",
        "                    print(f\"âš½ GOAL! Team 0 scores at {step/30:.1f}s\")\n",
        "            \n",
        "            if env.scores[1] > stats['scores'][1]:\n",
        "                stats['goals_timeline'].append({'team': 1, 'time': step / 30})\n",
        "                stats['scores'][1] = env.scores[1]\n",
        "                if verbose:\n",
        "                    print(f\"âš½ GOAL! Team 1 scores at {step/30:.1f}s\")\n",
        "            \n",
        "            # Track possession (simplified)\n",
        "            current_possession = getattr(env, 'ball_possession', -1)\n",
        "            if current_possession != -1:\n",
        "                team = current_possession // 2\n",
        "                stats['possession_time'][team] += 1\n",
        "            else:\n",
        "                stats['possession_time'][-1] += 1\n",
        "            \n",
        "            stats['steps'] += 1\n",
        "        \n",
        "        env.close()\n",
        "        \n",
        "        # Calculate possession percentage\n",
        "        total_possession = sum(stats['possession_time'].values())\n",
        "        if total_possession > 0:\n",
        "            stats['possession_pct'] = {\n",
        "                0: stats['possession_time'][0] / total_possession * 100,\n",
        "                1: stats['possession_time'][1] / total_possession * 100\n",
        "            }\n",
        "        else:\n",
        "            stats['possession_pct'] = {0: 0, 1: 0}\n",
        "        \n",
        "        return stats, video_frames\n",
        "    \n",
        "    def run_match(self, agents_dict, num_episodes=5, record_first=True):\n",
        "        \"\"\"Run multiple 20-second matches\"\"\"\n",
        "        print(f\"ğŸ† Running {num_episodes} x 20-second matches\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        all_stats = []\n",
        "        videos = []\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            print(f\"\\nğŸ“… Match {episode + 1}/{num_episodes}\")\n",
        "            record = record_first and episode == 0\n",
        "            \n",
        "            stats, frames = self.run_episode(agents_dict, record_video=record, verbose=True)\n",
        "            all_stats.append(stats)\n",
        "            \n",
        "            if frames:\n",
        "                videos.append(frames)\n",
        "            \n",
        "            # Match summary\n",
        "            print(f\"\\nğŸ“Š Match {episode + 1} Summary:\")\n",
        "            print(f\"   Final Score: {stats['scores'][0]} - {stats['scores'][1]}\")\n",
        "            print(f\"   Total Rewards: {sum(stats['rewards'].values()):.1f}\")\n",
        "            print(f\"   Shots: Team 0 = {stats['shots'][0]}, Team 1 = {stats['shots'][1]}\")\n",
        "            print(f\"   Possession: Team 0 = {stats['possession_pct'][0]:.1f}%, Team 1 = {stats['possession_pct'][1]:.1f}%\")\n",
        "            print(f\"   Goals scored at: {[f\"{g['time']:.1f}s\" for g in stats['goals_timeline']]}\")\n",
        "        \n",
        "        return all_stats, videos\n",
        "\n",
        "print(\"âœ… Long episode trainer ready!\")\n",
        "print(\"   - 20-second episodes (600 steps)\")\n",
        "print(\"   - Detailed statistics tracking\")\n",
        "print(\"   - Goal timeline and possession stats\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_matches_header"
      },
      "source": [
        "### ğŸ† 20ç§’ãƒãƒƒãƒã®å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_20sec_matches"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create enhanced expert agents for 20-second games\n",
        "print(\"ğŸ¯ Creating enhanced expert agents for 20-second matches...\")\n",
        "\n",
        "extended_config = ExtendedSoccerConfig()\n",
        "enhanced_experts = {}\n",
        "\n",
        "for i in range(4):\n",
        "    agent_name = f\"player_{i}\"\n",
        "    team = i // 2\n",
        "    enhanced_experts[agent_name] = EnhancedExpertAgent(i, team, extended_config)\n",
        "\n",
        "print(\"âœ… Enhanced expert agents created\")\n",
        "print(\"   Team 0: player_0 (attacker), player_1 (defender)\")\n",
        "print(\"   Team 1: player_2 (attacker), player_3 (defender)\")\n",
        "\n",
        "# Run matches\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "trainer = LongEpisodeTrainer(extended_config)\n",
        "match_stats, match_videos = trainer.run_match(\n",
        "    enhanced_experts,\n",
        "    num_episodes=3,  # Run 3 matches\n",
        "    record_first=True  # Record first match\n",
        ")\n",
        "\n",
        "# Overall statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ… Overall Statistics (3 matches):\")\n",
        "total_goals_0 = sum(s['scores'][0] for s in match_stats)\n",
        "total_goals_1 = sum(s['scores'][1] for s in match_stats)\n",
        "avg_rewards = np.mean([sum(s['rewards'].values()) for s in match_stats])\n",
        "avg_shots_0 = np.mean([s['shots'][0] for s in match_stats])\n",
        "avg_shots_1 = np.mean([s['shots'][1] for s in match_stats])\n",
        "\n",
        "print(f\"   Total Goals: Team 0 = {total_goals_0}, Team 1 = {total_goals_1}\")\n",
        "print(f\"   Average Rewards per Match: {avg_rewards:.1f}\")\n",
        "print(f\"   Average Shots per Match: Team 0 = {avg_shots_0:.1f}, Team 1 = {avg_shots_1:.1f}\")\n",
        "print(f\"   Goals per 20 seconds: {(total_goals_0 + total_goals_1) / 3:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_match_header"
      },
      "source": [
        "### ğŸ“Š 20ç§’ãƒãƒƒãƒã®å¯è¦–åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visualize_20sec_match"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Visualize match statistics\n",
        "if match_stats:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Goals over matches\n",
        "    ax = axes[0, 0]\n",
        "    matches = range(1, len(match_stats) + 1)\n",
        "    team0_scores = [s['scores'][0] for s in match_stats]\n",
        "    team1_scores = [s['scores'][1] for s in match_stats]\n",
        "    \n",
        "    width = 0.35\n",
        "    x = np.arange(len(matches))\n",
        "    ax.bar(x - width/2, team0_scores, width, label='Team 0 (Blue)', color='blue', alpha=0.7)\n",
        "    ax.bar(x + width/2, team1_scores, width, label='Team 1 (Red)', color='red', alpha=0.7)\n",
        "    ax.set_xlabel('Match Number')\n",
        "    ax.set_ylabel('Goals Scored')\n",
        "    ax.set_title('Goals per 20-second Match')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(matches)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Shots comparison\n",
        "    ax = axes[0, 1]\n",
        "    team0_shots = [s['shots'][0] for s in match_stats]\n",
        "    team1_shots = [s['shots'][1] for s in match_stats]\n",
        "    \n",
        "    ax.plot(matches, team0_shots, 'o-', label='Team 0', color='blue', linewidth=2, markersize=8)\n",
        "    ax.plot(matches, team1_shots, 's-', label='Team 1', color='red', linewidth=2, markersize=8)\n",
        "    ax.set_xlabel('Match Number')\n",
        "    ax.set_ylabel('Shot Attempts')\n",
        "    ax.set_title('Shots on Goal per Match')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Possession percentage\n",
        "    ax = axes[1, 0]\n",
        "    team0_possession = [s['possession_pct'][0] for s in match_stats]\n",
        "    team1_possession = [s['possession_pct'][1] for s in match_stats]\n",
        "    \n",
        "    ax.bar(x - width/2, team0_possession, width, label='Team 0', color='blue', alpha=0.7)\n",
        "    ax.bar(x + width/2, team1_possession, width, label='Team 1', color='red', alpha=0.7)\n",
        "    ax.set_xlabel('Match Number')\n",
        "    ax.set_ylabel('Ball Possession (%)')\n",
        "    ax.set_title('Ball Possession Statistics')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(matches)\n",
        "    ax.legend()\n",
        "    ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Goal timeline (for first match)\n",
        "    ax = axes[1, 1]\n",
        "    if match_stats[0]['goals_timeline']:\n",
        "        goals = match_stats[0]['goals_timeline']\n",
        "        times = [g['time'] for g in goals]\n",
        "        teams = [g['team'] for g in goals]\n",
        "        colors = ['blue' if t == 0 else 'red' for t in teams]\n",
        "        \n",
        "        ax.scatter(times, teams, c=colors, s=200, alpha=0.7)\n",
        "        ax.set_xlabel('Time (seconds)')\n",
        "        ax.set_ylabel('Team')\n",
        "        ax.set_title('Goal Timeline (First Match)')\n",
        "        ax.set_yticks([0, 1])\n",
        "        ax.set_yticklabels(['Team 0', 'Team 1'])\n",
        "        ax.set_xlim(0, 20)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        for i, (time, team) in enumerate(zip(times, teams)):\n",
        "            ax.annotate(f'{time:.1f}s', (time, team), \n",
        "                       xytext=(0, 10), textcoords='offset points',\n",
        "                       ha='center', fontsize=9)\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No goals scored', \n",
        "               ha='center', va='center', transform=ax.transAxes,\n",
        "               fontsize=14, color='gray')\n",
        "        ax.set_title('Goal Timeline (First Match)')\n",
        "    \n",
        "    plt.suptitle('20-Second Match Statistics', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nğŸ“ˆ Key Observations:\")\n",
        "print(\"  - 20ç§’ã®è©¦åˆã§æˆ¦ç•¥çš„ãªå±•é–‹ãŒå¯èƒ½\")\n",
        "print(\"  - æ”»æ’ƒã¨é˜²å¾¡ã®å½¹å‰²åˆ†æ‹…ãŒæ˜ç¢º\")\n",
        "print(\"  - ãƒœãƒ¼ãƒ«æ”¯é…ç‡ã¨å¾—ç‚¹ã®ç›¸é–¢\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video_20sec_header"
      },
      "source": [
        "### ğŸ¬ 20ç§’ãƒãƒƒãƒå‹•ç”»"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_20sec_video"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create and display 20-second match video\n",
        "if match_videos and match_videos[0]:\n",
        "    print(\"ğŸ¬ Creating 20-second match video...\")\n",
        "    print(f\"   Frames: {len(match_videos[0])}\")\n",
        "    print(f\"   Duration: 20 seconds\")\n",
        "    print(f\"   FPS: 15 (downsampled from 30)\")\n",
        "    \n",
        "    video_path = '/tmp/soccer_20sec_match.mp4'\n",
        "    create_video_from_frames(match_videos[0], video_path, fps=15)  # 15 FPS for smaller file\n",
        "    \n",
        "    print(\"\\nâœ… Video created successfully!\")\n",
        "    print(\"\\nğŸ“º 20-Second Soccer Match:\")\n",
        "    display(display_video(video_path))\n",
        "    \n",
        "    print(\"\\nğŸ¯ è¦–è´ãƒã‚¤ãƒ³ãƒˆ:\")\n",
        "    print(\"  - 20ç§’é–“ã®æˆ¦ç•¥çš„ãªæ”»é˜²\")\n",
        "    print(\"  - ã‚¢ã‚¿ãƒƒã‚«ãƒ¼ã¨ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ãƒ€ãƒ¼ã®é€£æº\")\n",
        "    print(\"  - ã‚¹ã‚¿ãƒŸãƒŠã«ã‚ˆã‚‹å¾ŒåŠã®å‹•ãã®å¤‰åŒ–\")\n",
        "    print(\"  - ã‚´ãƒ¼ãƒ«ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¨ãƒãƒ£ãƒ³ã‚¹å‰µå‡º\")\n",
        "else:\n",
        "    print(\"No video available. Please run the match first.\")"
      ]
    }
  ]
}