{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# 🎮 Multi-Agent Soccer Game with Deep Reinforcement Learning\n",
        "## Google Colab 完全統合版 (修正版)\n\n",
        "このノートブックは深層強化学習を用いたマルチエージェントサッカーゲームの完全統合版です。\n",
        "すべての必要なコードが単一のノートブック内に含まれており、インポートエラーが修正されています。\n\n",
        "### 📋 実装内容\n",
        "- **環境**: 2v2サッカーゲーム（PettingZoo互換）\n",
        "- **物理エンジン**: リアルタイムな衝突検出とボール物理\n",
        "- **エージェント**: Random, DQN, MADDPG\n",
        "- **観測空間**: 28次元（プレイヤー位置、ボール状態、ゲーム情報）\n",
        "- **行動空間**: 5次元連続行動（移動+キック）\n",
        "- **報酬システム**: 多目的報酬関数\n\n",
        "### ⚠️ 実行の注意事項\n",
        "1. **上から順番に実行してください** - 各セルが前のセルで定義されたクラスに依存しています\n",
        "2. **GPU使用推奨** - Runtime → Change runtime type → GPU\n",
        "3. **すべてのコードセルを実行** - スキップすると後続のセルでエラーが発生します\n\n",
        "### 📊 実装ドキュメント\n",
        "詳細な実装内容は「マルチエージェントサッカーゲーム実装ドキュメント.pdf」に記載されています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "## 📦 1. 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title ライブラリのインストール (クリックして実行)\n",
        "# Google Colab用のライブラリインストール\n",
        "!pip install -q gymnasium\n",
        "!pip install -q pettingzoo\n",
        "!pip install -q pygame\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q numpy\n",
        "!pip install -q opencv-python\n\n",
        "print(\"✅ All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_header"
      },
      "source": [
        "## 🔧 2. 基本インポートとセットアップ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 基本ライブラリのインポート (クリックして実行)\n",
        "# 基本ライブラリのインポート\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "import json\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from abc import ABC, abstractmethod\n",
        "import time\n",
        "import os\n\n",
        "# Gymnasium and PettingZoo\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils import agent_selector, wrappers\n\n",
        "# Pygame (optional for rendering)\n",
        "try:\n",
        "    import pygame\n",
        "    PYGAME_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYGAME_AVAILABLE = False\n",
        "    print(\"⚠️ Pygame not available. Rendering disabled.\")\n\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n\n",
        "# Set matplotlib style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n\n",
        "print(f\"✅ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"✅ Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## ⚙️ 3. 設定クラス (Configuration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nConfiguration file for Multi-Agent Soccer Game\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Tuple\n\n@dataclass\nclass SoccerEnvironmentConfig:\n    \"\"\"Environment configuration for soccer game\"\"\"\n    FIELD_SIZE: Tuple[int, int] = (800, 600)\n    GOAL_SIZE: Tuple[int, int] = (20, 200)\n    BALL_RADIUS: int = 10\n    PLAYER_RADIUS: int = 20\n    MAX_STEPS: int = 1000\n\n    NUM_PLAYERS_PER_TEAM: int = 2\n    TEAM_COLORS: Tuple[str, str] = ('blue', 'red')\n    PLAYER_SPEED: float = 5.0\n    BALL_SPEED_MULTIPLIER: float = 1.5\n\n    FRICTION: float = 0.95\n    BALL_DECAY: float = 0.98\n    COLLISION_THRESHOLD: float = 30.0\n\n@dataclass\nclass MADDPGConfig:\n    \"\"\"MADDPG algorithm configuration\"\"\"\n    obs_dim: int = 28\n    action_dim: int = 5\n    global_obs_dim: int = 112  # 28 * 4 agents\n    global_action_dim: int = 20  # 5 * 4 agents\n    hidden_dims: Tuple[int, ...] = (256, 128)\n\n    actor_lr: float = 1e-4\n    critic_lr: float = 1e-3\n    gamma: float = 0.95\n    tau: float = 0.01\n    batch_size: int = 256\n    buffer_size: int = int(1e6)\n    noise_scale: float = 0.1\n    noise_decay: float = 0.9999\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Training configuration\"\"\"\n    max_episodes: int = 10000\n    max_steps_per_episode: int = 1000\n    save_freq: int = 1000\n    eval_freq: int = 500\n    log_freq: int = 100\n\n    # Reproducibility\n    random_seed: int = 42\n\n@dataclass\nclass ExperimentConfig:\n    \"\"\"Experiment configuration\"\"\"\n    experiment_name: str = \"soccer_multiagent\"\n    log_dir: str = \"logs\"\n    save_dir: str = \"saved_models\"\n    video_dir: str = \"videos\"\n\n    # Algorithms to run\n    algorithms: Tuple[str, ...] = (\"random\", \"dqn\", \"ppo\", \"maddpg\")\n\nprint(\"✅ Configuration classes defined successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "physics_header"
      },
      "source": [
        "## 🎯 4. 物理エンジン (Physics Engine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "physics_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nPhysics engine for soccer game\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\nclass Ball:\n    def __init__(self, x: float, y: float, radius: float = 10):\n        self.pos = np.array([x, y], dtype=float)\n        self.vel = np.array([0.0, 0.0], dtype=float)\n        self.radius = radius\n\n    def update(self, config: SoccerEnvironmentConfig):\n        \"\"\"Update ball position with physics\"\"\"\n        # Apply velocity\n        self.pos += self.vel\n\n        # Apply ball decay (friction)\n        self.vel *= config.BALL_DECAY\n\n        # Boundary collision detection\n        field_width, field_height = config.FIELD_SIZE\n\n        # Horizontal boundaries (top/bottom)\n        if self.pos[1] <= self.radius or self.pos[1] >= field_height - self.radius:\n            self.vel[1] *= -0.8  # Energy loss on collision\n            self.pos[1] = max(self.radius, min(field_height - self.radius, self.pos[1]))\n\n        # Vertical boundaries (left/right - goals)\n        goal_top = (field_height - config.GOAL_SIZE[1]) // 2\n        goal_bottom = goal_top + config.GOAL_SIZE[1]\n\n        # Left side\n        if self.pos[0] <= self.radius:\n            if goal_top <= self.pos[1] <= goal_bottom:\n                # Goal scored\n                return \"goal_left\"\n            else:\n                self.vel[0] *= -0.8\n                self.pos[0] = self.radius\n\n        # Right side\n        elif self.pos[0] >= field_width - self.radius:\n            if goal_top <= self.pos[1] <= goal_bottom:\n                # Goal scored\n                return \"goal_right\"\n            else:\n                self.vel[0] *= -0.8\n                self.pos[0] = field_width - self.radius\n\n        return None\n\nclass Player:\n    def __init__(self, x: float, y: float, team: int, player_id: int, radius: float = 20):\n        self.pos = np.array([x, y], dtype=float)\n        self.vel = np.array([0.0, 0.0], dtype=float)\n        self.team = team\n        self.player_id = player_id\n        self.radius = radius\n        self.has_ball = False\n\n    def update(self, action: np.ndarray, config: SoccerEnvironmentConfig):\n        \"\"\"Update player position based on action\"\"\"\n        # Extract movement and kick actions\n        move_x, move_y = action[0], action[1]\n        kick_power = action[2] if len(action) > 2 else 0.0\n        kick_dir_x = action[3] if len(action) > 3 else 0.0\n        kick_dir_y = action[4] if len(action) > 4 else 0.0\n\n        # Apply movement\n        movement = np.array([move_x, move_y]) * config.PLAYER_SPEED\n        self.vel = movement\n        self.pos += self.vel\n\n        # Apply friction\n        self.vel *= config.FRICTION\n\n        # Boundary constraints\n        field_width, field_height = config.FIELD_SIZE\n        self.pos[0] = max(self.radius, min(field_width - self.radius, self.pos[0]))\n        self.pos[1] = max(self.radius, min(field_height - self.radius, self.pos[1]))\n\n        return kick_power, np.array([kick_dir_x, kick_dir_y])\n\nclass PhysicsEngine:\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n        self.ball = Ball(\n            config.FIELD_SIZE[0] // 2,\n            config.FIELD_SIZE[1] // 2,\n            config.BALL_RADIUS\n        )\n\n        # Initialize players\n        self.players = []\n        self._init_players()\n\n    def _init_players(self):\n        \"\"\"Initialize player positions\"\"\"\n        field_width, field_height = self.config.FIELD_SIZE\n\n        # Team 0 (left side - blue)\n        self.players.append(Player(field_width * 0.2, field_height * 0.3, 0, 0))\n        self.players.append(Player(field_width * 0.2, field_height * 0.7, 0, 1))\n\n        # Team 1 (right side - red)\n        self.players.append(Player(field_width * 0.8, field_height * 0.3, 1, 0))\n        self.players.append(Player(field_width * 0.8, field_height * 0.7, 1, 1))\n\n    def reset(self):\n        \"\"\"Reset physics state\"\"\"\n        self.ball.pos = np.array([\n            self.config.FIELD_SIZE[0] // 2,\n            self.config.FIELD_SIZE[1] // 2\n        ], dtype=float)\n        self.ball.vel = np.array([0.0, 0.0], dtype=float)\n\n        # Reset player positions\n        field_width, field_height = self.config.FIELD_SIZE\n        positions = [\n            (field_width * 0.2, field_height * 0.3),  # Team 0, Player 0\n            (field_width * 0.2, field_height * 0.7),  # Team 0, Player 1\n            (field_width * 0.8, field_height * 0.3),  # Team 1, Player 0\n            (field_width * 0.8, field_height * 0.7),  # Team 1, Player 1\n        ]\n\n        for i, (x, y) in enumerate(positions):\n            self.players[i].pos = np.array([x, y], dtype=float)\n            self.players[i].vel = np.array([0.0, 0.0], dtype=float)\n            self.players[i].has_ball = False\n\n    def step(self, actions: Dict[str, np.ndarray]) -> Optional[str]:\n        \"\"\"Step physics simulation\"\"\"\n        # Update players\n        kicks = {}\n        for i, player in enumerate(self.players):\n            agent_key = f\"player_{i}\"\n            if agent_key in actions:\n                kick_power, kick_dir = player.update(actions[agent_key], self.config)\n                if kick_power > 0:\n                    kicks[i] = (kick_power, kick_dir)\n\n        # Check player collisions with ball and apply kicks\n        ball_touched_by = None\n        for i, player in enumerate(self.players):\n            dist = np.linalg.norm(player.pos - self.ball.pos)\n            if dist <= player.radius + self.ball.radius:\n                ball_touched_by = i\n                player.has_ball = True\n\n                # Apply kick if player is kicking\n                if i in kicks:\n                    kick_power, kick_dir = kicks[i]\n                    kick_dir = kick_dir / (np.linalg.norm(kick_dir) + 1e-8)  # Normalize\n                    self.ball.vel += kick_dir * kick_power * self.config.BALL_SPEED_MULTIPLIER\n            else:\n                player.has_ball = False\n\n        # Update ball\n        goal_result = self.ball.update(self.config)\n\n        # Handle player-player collisions\n        self._handle_player_collisions()\n\n        return goal_result, ball_touched_by\n\n    def _handle_player_collisions(self):\n        \"\"\"Handle collisions between players\"\"\"\n        for i in range(len(self.players)):\n            for j in range(i + 1, len(self.players)):\n                p1, p2 = self.players[i], self.players[j]\n                dist = np.linalg.norm(p1.pos - p2.pos)\n\n                if dist < p1.radius + p2.radius:\n                    # Separate players\n                    direction = p1.pos - p2.pos\n                    direction = direction / (np.linalg.norm(direction) + 1e-8)\n                    overlap = (p1.radius + p2.radius) - dist\n\n                    p1.pos += direction * overlap * 0.5\n                    p2.pos -= direction * overlap * 0.5\n\n    def get_state(self) -> Dict:\n        \"\"\"Get current state of all entities\"\"\"\n        return {\n            'ball': {\n                'pos': self.ball.pos.copy(),\n                'vel': self.ball.vel.copy()\n            },\n            'players': [\n                {\n                    'pos': player.pos.copy(),\n                    'vel': player.vel.copy(),\n                    'team': player.team,\n                    'player_id': player.player_id,\n                    'has_ball': player.has_ball\n                }\n                for player in self.players\n            ]\n        }\n\nprint(\"✅ Physics engine implemented successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spaces_header"
      },
      "source": [
        "## 🎮 5. 観測・行動空間 (Observation and Action Spaces)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spaces_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nObservation and action space definitions for soccer environment\n\"\"\"\n\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom typing import Dict, List, Tuple, Union\n\nclass ObservationSpace:\n    \"\"\"\n    Observation space for each agent (28 dimensions total)\n    \"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n\n        # Observation space bounds\n        field_width, field_height = config.FIELD_SIZE\n        max_velocity = config.PLAYER_SPEED * 2  # Max possible velocity\n        max_distance = np.sqrt(field_width**2 + field_height**2)  # Diagonal distance\n\n        # Define observation bounds\n        obs_low = np.array([\n            # Self state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Ball state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Teammate state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Opponent 1 state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Opponent 2 state (4 dims)\n            0, 0,           # position (normalized)\n            -max_velocity, -max_velocity,  # velocity\n\n            # Goal information (4 dims)\n            0,              # own goal distance\n            0,              # enemy goal distance\n            -np.pi,         # own goal angle\n            -np.pi,         # enemy goal angle\n\n            # Context information (4 dims)\n            -1,             # ball possession (-1: none, 0-3: player id)\n            0,              # time remaining (normalized)\n            -10,            # score difference\n            -1,             # last touch player id\n        ], dtype=np.float32)\n\n        obs_high = np.array([\n            # Self state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Ball state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Teammate state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Opponent 1 state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Opponent 2 state (4 dims)\n            1, 1,           # position (normalized)\n            max_velocity, max_velocity,  # velocity\n\n            # Goal information (4 dims)\n            max_distance,   # own goal distance\n            max_distance,   # enemy goal distance\n            np.pi,          # own goal angle\n            np.pi,          # enemy goal angle\n\n            # Context information (4 dims)\n            3,              # ball possession (player 0-3)\n            1,              # time remaining (normalized)\n            10,             # score difference\n            3,              # last touch player id\n        ], dtype=np.float32)\n\n        self.gym_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n\n    def create_observation(self, agent_id: int, state: Dict,\n                         scores: Tuple[int, int], step: int,\n                         max_steps: int, ball_possession: int = -1,\n                         last_touch: int = -1) -> np.ndarray:\n        \"\"\"Create observation for specific agent\"\"\"\n\n        field_width, field_height = self.config.FIELD_SIZE\n        players = state['players']\n        ball = state['ball']\n\n        # Get agent info\n        agent = players[agent_id]\n        agent_team = agent['team']\n        agent_pos = agent['pos'] / np.array([field_width, field_height])  # Normalize\n        agent_vel = agent['vel'] / self.config.PLAYER_SPEED  # Normalize\n\n        # Get ball info\n        ball_pos = ball['pos'] / np.array([field_width, field_height])  # Normalize\n        ball_vel = ball['vel'] / self.config.PLAYER_SPEED  # Normalize\n\n        # Get teammate and opponents\n        teammates = [p for i, p in enumerate(players)\n                    if p['team'] == agent_team and i != agent_id]\n        opponents = [p for p in players if p['team'] != agent_team]\n\n        teammate = teammates[0] if teammates else agent  # fallback\n        teammate_pos = teammate['pos'] / np.array([field_width, field_height])\n        teammate_vel = teammate['vel'] / self.config.PLAYER_SPEED\n\n        # Opponents\n        opp1 = opponents[0] if len(opponents) > 0 else agent\n        opp2 = opponents[1] if len(opponents) > 1 else agent\n\n        opp1_pos = opp1['pos'] / np.array([field_width, field_height])\n        opp1_vel = opp1['vel'] / self.config.PLAYER_SPEED\n\n        opp2_pos = opp2['pos'] / np.array([field_width, field_height])\n        opp2_vel = opp2['vel'] / self.config.PLAYER_SPEED\n\n        # Goal information\n        if agent_team == 0:  # Blue team (left side)\n            own_goal_pos = np.array([0, 0.5])\n            enemy_goal_pos = np.array([1, 0.5])\n        else:  # Red team (right side)\n            own_goal_pos = np.array([1, 0.5])\n            enemy_goal_pos = np.array([0, 0.5])\n\n        own_goal_dist = np.linalg.norm(agent_pos - own_goal_pos)\n        enemy_goal_dist = np.linalg.norm(agent_pos - enemy_goal_pos)\n\n        # Goal angles\n        own_goal_vec = own_goal_pos - agent_pos\n        enemy_goal_vec = enemy_goal_pos - agent_pos\n\n        own_goal_angle = np.arctan2(own_goal_vec[1], own_goal_vec[0])\n        enemy_goal_angle = np.arctan2(enemy_goal_vec[1], enemy_goal_vec[0])\n\n        # Context information\n        time_remaining = (max_steps - step) / max_steps\n        score_diff = scores[agent_team] - scores[1 - agent_team]\n\n        # Construct observation\n        observation = np.concatenate([\n            # Self state\n            agent_pos, agent_vel,\n\n            # Ball state\n            ball_pos, ball_vel,\n\n            # Teammate state\n            teammate_pos, teammate_vel,\n\n            # Opponent states\n            opp1_pos, opp1_vel,\n            opp2_pos, opp2_vel,\n\n            # Goal information\n            [own_goal_dist, enemy_goal_dist, own_goal_angle, enemy_goal_angle],\n\n            # Context information\n            [ball_possession, time_remaining, score_diff, last_touch]\n        ]).astype(np.float32)\n\n        return observation\n\nclass ActionSpace:\n    \"\"\"\n    Action space for each agent\n    \"\"\"\n    def __init__(self, action_type: str = \"continuous\"):\n        self.action_type = action_type\n\n        if action_type == \"continuous\":\n            # 5-dimensional continuous action space\n            # [move_x, move_y, kick_power, kick_dir_x, kick_dir_y]\n            self.gym_space = spaces.Box(\n                low=np.array([-1, -1, 0, -1, -1], dtype=np.float32),\n                high=np.array([1, 1, 1, 1, 1], dtype=np.float32),\n                dtype=np.float32\n            )\n        else:\n            # 9-dimensional discrete action space\n            # [NOOP, UP, DOWN, LEFT, RIGHT, KICK_UP, KICK_DOWN, KICK_LEFT, KICK_RIGHT]\n            self.gym_space = spaces.Discrete(9)\n\n        self.action_meanings = {\n            0: \"NOOP\",\n            1: \"UP\",\n            2: \"DOWN\",\n            3: \"LEFT\",\n            4: \"RIGHT\",\n            5: \"KICK_UP\",\n            6: \"KICK_DOWN\",\n            7: \"KICK_LEFT\",\n            8: \"KICK_RIGHT\"\n        }\n\n    def sample(self) -> Union[np.ndarray, int]:\n        \"\"\"Sample a random action\"\"\"\n        return self.gym_space.sample()\n\n    def convert_discrete_to_continuous(self, action: int) -> np.ndarray:\n        \"\"\"Convert discrete action to continuous action format\"\"\"\n        action_map = {\n            0: np.array([0, 0, 0, 0, 0]),        # NOOP\n            1: np.array([0, -1, 0, 0, 0]),       # UP\n            2: np.array([0, 1, 0, 0, 0]),        # DOWN\n            3: np.array([-1, 0, 0, 0, 0]),       # LEFT\n            4: np.array([1, 0, 0, 0, 0]),        # RIGHT\n            5: np.array([0, 0, 0.5, 0, -1]),     # KICK_UP\n            6: np.array([0, 0, 0.5, 0, 1]),      # KICK_DOWN\n            7: np.array([0, 0, 0.5, -1, 0]),     # KICK_LEFT\n            8: np.array([0, 0, 0.5, 1, 0]),      # KICK_RIGHT\n        }\n\n        return action_map.get(action, action_map[0]).astype(np.float32)\n\ndef create_spaces(config: SoccerEnvironmentConfig, action_type: str = \"continuous\"):\n    \"\"\"Create observation and action spaces\"\"\"\n    obs_space = ObservationSpace(config)\n    action_space = ActionSpace(action_type)\n\n    return obs_space, action_space\n\nprint(\"✅ Observation and action spaces defined successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rewards_header"
      },
      "source": [
        "## 🏆 6. 報酬システム (Reward System)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rewards_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nReward system for soccer environment with multi-objective reward function\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\nclass RewardCalculator:\n    \"\"\"\n    Multi-objective reward function for soccer agents\n    \"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n        self.field_width, self.field_height = config.FIELD_SIZE\n\n        # Reward weights\n        self.reward_weights = {\n            'goal_scored': 100.0,           # Goal scored by team\n            'goal_conceded': -100.0,        # Goal conceded by team\n            'ball_touch': 5.0,              # Touching the ball\n            'goal_approach': 0.1,           # Moving closer to enemy goal\n            'ball_approach': 0.05,          # Moving closer to ball\n            'teamwork': 0.02,               # Team coordination\n            'out_of_bounds': -10.0,         # Going out of bounds\n            'stalemate': -0.1,              # Stalemate penalty\n            'ball_possession': 0.01,        # Keeping ball possession\n            'defensive_positioning': 0.01,   # Good defensive position\n        }\n\n        # Track previous states for delta calculations\n        self.prev_states = {}\n\n    def calculate_reward(self, agent_id: int, action: np.ndarray,\n                        prev_state: Dict, current_state: Dict,\n                        goal_scored: Optional[str] = None,\n                        ball_touched_by: Optional[int] = None,\n                        scores: Tuple[int, int] = (0, 0),\n                        out_of_bounds_agents: List[int] = None) -> float:\n        \"\"\"\n        Calculate multi-objective reward for agent\n        \"\"\"\n        reward = 0.0\n        agent_team = current_state['players'][agent_id]['team']\n        out_of_bounds_agents = out_of_bounds_agents or []\n\n        # 1. Goal rewards (most important)\n        if goal_scored:\n            if goal_scored == \"goal_left\" and agent_team == 1:  # Red team scored\n                reward += self.reward_weights['goal_scored']\n            elif goal_scored == \"goal_right\" and agent_team == 0:  # Blue team scored\n                reward += self.reward_weights['goal_scored']\n            elif goal_scored == \"goal_left\" and agent_team == 0:  # Blue conceded\n                reward += self.reward_weights['goal_conceded']\n            elif goal_scored == \"goal_right\" and agent_team == 1:  # Red conceded\n                reward += self.reward_weights['goal_conceded']\n\n        # 2. Ball contact reward\n        if ball_touched_by == agent_id:\n            reward += self.reward_weights['ball_touch']\n\n        # 3. Goal approach reward\n        goal_approach_reward = self.calculate_goal_approach_reward(\n            agent_id, prev_state, current_state\n        )\n        reward += goal_approach_reward * self.reward_weights['goal_approach']\n\n        # 4. Ball approach reward\n        ball_approach_reward = self.calculate_ball_approach_reward(\n            agent_id, prev_state, current_state\n        )\n        reward += ball_approach_reward * self.reward_weights['ball_approach']\n\n        # 5. Teamwork reward\n        teamwork_reward = self.calculate_teamwork_reward(agent_id, current_state)\n        reward += teamwork_reward * self.reward_weights['teamwork']\n\n        # 6. Penalties\n        if agent_id in out_of_bounds_agents:\n            reward += self.reward_weights['out_of_bounds']\n\n        if self.is_stalemate(current_state):\n            reward += self.reward_weights['stalemate']\n\n        # 7. Ball possession reward\n        if current_state['players'][agent_id]['has_ball']:\n            reward += self.reward_weights['ball_possession']\n\n        # 8. Defensive positioning reward\n        defensive_reward = self.calculate_defensive_positioning_reward(\n            agent_id, current_state\n        )\n        reward += defensive_reward * self.reward_weights['defensive_positioning']\n\n        return reward\n\n    def calculate_goal_approach_reward(self, agent_id: int, prev_state: Dict,\n                                     current_state: Dict) -> float:\n        \"\"\"Calculate reward for approaching enemy goal\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n        current_pos = current_state['players'][agent_id]['pos']\n        prev_pos = prev_state['players'][agent_id]['pos']\n\n        # Enemy goal position\n        if agent_team == 0:  # Blue team\n            enemy_goal_pos = np.array([self.field_width, self.field_height / 2])\n        else:  # Red team\n            enemy_goal_pos = np.array([0, self.field_height / 2])\n\n        prev_dist = np.linalg.norm(prev_pos - enemy_goal_pos)\n        current_dist = np.linalg.norm(current_pos - enemy_goal_pos)\n\n        return prev_dist - current_dist  # Positive if getting closer\n\n    def calculate_ball_approach_reward(self, agent_id: int, prev_state: Dict,\n                                     current_state: Dict) -> float:\n        \"\"\"Calculate reward for approaching ball\"\"\"\n        current_pos = current_state['players'][agent_id]['pos']\n        prev_pos = prev_state['players'][agent_id]['pos']\n        ball_pos = current_state['ball']['pos']\n\n        prev_dist = np.linalg.norm(prev_pos - ball_pos)\n        current_dist = np.linalg.norm(current_pos - ball_pos)\n\n        return prev_dist - current_dist  # Positive if getting closer\n\n    def calculate_teamwork_reward(self, agent_id: int, current_state: Dict) -> float:\n        \"\"\"Calculate teamwork reward based on team coordination\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n        players = current_state['players']\n\n        # Find teammate\n        teammates = [p for i, p in enumerate(players)\n                    if p['team'] == agent_team and i != agent_id]\n\n        if not teammates:\n            return 0.0\n\n        teammate = teammates[0]\n        agent_pos = current_state['players'][agent_id]['pos']\n        teammate_pos = teammate['pos']\n\n        # Optimal distance between teammates (100-200 pixels)\n        teammate_dist = np.linalg.norm(agent_pos - teammate_pos)\n        optimal_dist = 150\n        dist_penalty = abs(teammate_dist - optimal_dist) / optimal_dist\n\n        return 1.0 - dist_penalty  # Higher reward for optimal distance\n\n    def calculate_defensive_positioning_reward(self, agent_id: int,\n                                             current_state: Dict) -> float:\n        \"\"\"Calculate reward for good defensive positioning\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n        agent_pos = current_state['players'][agent_id]['pos']\n        ball_pos = current_state['ball']['pos']\n\n        # Own goal position\n        if agent_team == 0:  # Blue team\n            own_goal_pos = np.array([0, self.field_height / 2])\n        else:  # Red team\n            own_goal_pos = np.array([self.field_width, self.field_height / 2])\n\n        # Reward for being between ball and own goal\n        goal_to_ball = ball_pos - own_goal_pos\n        goal_to_agent = agent_pos - own_goal_pos\n\n        # Project agent position onto goal-ball line\n        if np.linalg.norm(goal_to_ball) > 0:\n            projection = np.dot(goal_to_agent, goal_to_ball) / np.linalg.norm(goal_to_ball)\n            ball_dist = np.linalg.norm(goal_to_ball)\n\n            # Reward if agent is between goal and ball\n            if 0 < projection < ball_dist:\n                return 1.0\n\n        return 0.0\n\n    def is_stalemate(self, state: Dict, threshold: float = 1.0) -> bool:\n        \"\"\"Check if the game is in a stalemate (low activity)\"\"\"\n        ball_speed = np.linalg.norm(state['ball']['vel'])\n        player_speeds = [np.linalg.norm(p['vel']) for p in state['players']]\n        avg_player_speed = np.mean(player_speeds)\n\n        return ball_speed < threshold and avg_player_speed < threshold\n\n    def get_team_reward(self, team: int, individual_rewards: Dict[int, float]) -> float:\n        \"\"\"Calculate team reward from individual rewards\"\"\"\n        team_players = [i for i in individual_rewards.keys()\n                       if i // 2 == team]  # Assuming 2 players per team\n        return sum(individual_rewards[i] for i in team_players) / len(team_players)\n\nclass RewardShaper:\n    \"\"\"\n    Advanced reward shaping techniques\n    \"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig):\n        self.config = config\n        self.reward_calculator = RewardCalculator(config)\n\n    def shaped_reward(self, agent_id: int, action: np.ndarray,\n                     prev_state: Dict, current_state: Dict,\n                     **kwargs) -> float:\n        \"\"\"Apply reward shaping for better learning\"\"\"\n        base_reward = self.reward_calculator.calculate_reward(\n            agent_id, action, prev_state, current_state, **kwargs\n        )\n\n        # Potential-based reward shaping\n        potential_reward = self.calculate_potential_based_reward(\n            agent_id, prev_state, current_state\n        )\n\n        return base_reward + potential_reward\n\n    def calculate_potential_based_reward(self, agent_id: int,\n                                       prev_state: Dict, current_state: Dict) -> float:\n        \"\"\"Calculate potential-based shaped reward\"\"\"\n        agent_team = current_state['players'][agent_id]['team']\n\n        # Potential functions\n        prev_potential = self.calculate_potential(agent_id, prev_state)\n        current_potential = self.calculate_potential(agent_id, current_state)\n\n        # Potential-based shaping: F(s,a,s') = γΦ(s') - Φ(s)\n        gamma = 0.99  # Discount factor\n        return gamma * current_potential - prev_potential\n\n    def calculate_potential(self, agent_id: int, state: Dict) -> float:\n        \"\"\"Calculate potential function value\"\"\"\n        agent_team = state['players'][agent_id]['team']\n        agent_pos = state['players'][agent_id]['pos']\n        ball_pos = state['ball']['pos']\n\n        # Potential based on distance to ball and enemy goal\n        if agent_team == 0:  # Blue team\n            enemy_goal_pos = np.array([self.config.FIELD_SIZE[0], self.config.FIELD_SIZE[1] / 2])\n        else:  # Red team\n            enemy_goal_pos = np.array([0, self.config.FIELD_SIZE[1] / 2])\n\n        ball_dist = np.linalg.norm(agent_pos - ball_pos)\n        goal_dist = np.linalg.norm(ball_pos - enemy_goal_pos)\n\n        # Potential decreases with distance (encouraging approach)\n        potential = -0.001 * ball_dist - 0.001 * goal_dist\n\n        return potential\n\nprint(\"✅ Reward system implemented successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "renderer_header"
      },
      "source": [
        "## 🎨 7. レンダラー (Renderer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renderer_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nRenderer for soccer game visualization using pygame\n\"\"\"\n\nimport pygame\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\nclass SoccerRenderer:\n    def __init__(self, config: SoccerEnvironmentConfig, window_size: Tuple[int, int] = None):\n        self.config = config\n        self.window_size = window_size or config.FIELD_SIZE\n\n        pygame.init()\n        self.screen = pygame.display.set_mode(self.window_size)\n        pygame.display.set_caption(\"Multi-Agent Soccer Game\")\n\n        # Colors\n        self.colors = {\n            'field': (0, 128, 0),        # Green\n            'field_lines': (255, 255, 255),  # White\n            'ball': (255, 255, 255),     # White\n            'team_0': (0, 0, 255),       # Blue\n            'team_1': (255, 0, 0),       # Red\n            'goal': (128, 128, 128),     # Gray\n            'background': (0, 64, 0),    # Dark green\n        }\n\n        self.font = pygame.font.Font(None, 36)\n        self.clock = pygame.time.Clock()\n\n    def render(self, state: Dict, scores: Tuple[int, int] = (0, 0), step: int = 0) -> bool:\n        \"\"\"\n        Render the current state\n        Returns True if rendering should continue, False if window was closed\n        \"\"\"\n        # Handle pygame events\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                return False\n\n        # Clear screen\n        self.screen.fill(self.colors['background'])\n\n        # Draw field\n        self._draw_field()\n\n        # Draw players\n        self._draw_players(state['players'])\n\n        # Draw ball\n        self._draw_ball(state['ball'])\n\n        # Draw UI\n        self._draw_ui(scores, step)\n\n        pygame.display.flip()\n        self.clock.tick(60)  # 60 FPS\n\n        return True\n\n    def _draw_field(self):\n        \"\"\"Draw soccer field with goals and center line\"\"\"\n        field_width, field_height = self.config.FIELD_SIZE\n\n        # Field background\n        field_rect = pygame.Rect(0, 0, field_width, field_height)\n        pygame.draw.rect(self.screen, self.colors['field'], field_rect)\n\n        # Field border\n        pygame.draw.rect(self.screen, self.colors['field_lines'], field_rect, 3)\n\n        # Center line\n        center_x = field_width // 2\n        pygame.draw.line(self.screen, self.colors['field_lines'],\n                        (center_x, 0), (center_x, field_height), 3)\n\n        # Center circle\n        pygame.draw.circle(self.screen, self.colors['field_lines'],\n                          (center_x, field_height // 2), 100, 3)\n\n        # Goals\n        goal_width, goal_height = self.config.GOAL_SIZE\n        goal_top = (field_height - goal_height) // 2\n        goal_bottom = goal_top + goal_height\n\n        # Left goal\n        left_goal = pygame.Rect(-goal_width//2, goal_top, goal_width, goal_height)\n        pygame.draw.rect(self.screen, self.colors['goal'], left_goal)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], left_goal, 3)\n\n        # Right goal\n        right_goal = pygame.Rect(field_width - goal_width//2, goal_top, goal_width, goal_height)\n        pygame.draw.rect(self.screen, self.colors['goal'], right_goal)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], right_goal, 3)\n\n        # Goal areas (penalty boxes)\n        penalty_width, penalty_height = 120, 200\n        penalty_top = (field_height - penalty_height) // 2\n\n        # Left penalty box\n        left_penalty = pygame.Rect(0, penalty_top, penalty_width, penalty_height)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], left_penalty, 2)\n\n        # Right penalty box\n        right_penalty = pygame.Rect(field_width - penalty_width, penalty_top,\n                                   penalty_width, penalty_height)\n        pygame.draw.rect(self.screen, self.colors['field_lines'], right_penalty, 2)\n\n    def _draw_players(self, players: List[Dict]):\n        \"\"\"Draw all players\"\"\"\n        for i, player in enumerate(players):\n            pos = player['pos']\n            team = player['team']\n            has_ball = player['has_ball']\n\n            color = self.colors[f'team_{team}']\n\n            # Draw player circle\n            pygame.draw.circle(self.screen, color, pos.astype(int), self.config.PLAYER_RADIUS)\n\n            # Draw player outline\n            outline_color = (255, 255, 255) if has_ball else (0, 0, 0)\n            outline_width = 4 if has_ball else 2\n            pygame.draw.circle(self.screen, outline_color, pos.astype(int),\n                             self.config.PLAYER_RADIUS, outline_width)\n\n            # Draw player number\n            player_text = self.font.render(str(player['player_id']), True, (255, 255, 255))\n            text_rect = player_text.get_rect(center=pos.astype(int))\n            self.screen.blit(player_text, text_rect)\n\n    def _draw_ball(self, ball: Dict):\n        \"\"\"Draw the ball\"\"\"\n        pos = ball['pos']\n        pygame.draw.circle(self.screen, self.colors['ball'], pos.astype(int), self.config.BALL_RADIUS)\n        pygame.draw.circle(self.screen, (0, 0, 0), pos.astype(int), self.config.BALL_RADIUS, 2)\n\n        # Draw ball velocity vector (for debugging)\n        vel = ball['vel']\n        if np.linalg.norm(vel) > 0.1:\n            end_pos = pos + vel * 10  # Scale for visibility\n            pygame.draw.line(self.screen, (255, 255, 0), pos.astype(int), end_pos.astype(int), 2)\n\n    def _draw_ui(self, scores: Tuple[int, int], step: int):\n        \"\"\"Draw game UI (scores, step counter)\"\"\"\n        # Score display\n        score_text = f\"Blue: {scores[0]}  Red: {scores[1]}\"\n        score_surface = self.font.render(score_text, True, (255, 255, 255))\n        self.screen.blit(score_surface, (10, 10))\n\n        # Step counter\n        step_text = f\"Step: {step}\"\n        step_surface = self.font.render(step_text, True, (255, 255, 255))\n        step_rect = step_surface.get_rect()\n        step_rect.topright = (self.window_size[0] - 10, 10)\n        self.screen.blit(step_surface, step_rect)\n\n    def close(self):\n        \"\"\"Close the renderer\"\"\"\n        pygame.quit()\n\n    def save_frame(self, filename: str):\n        \"\"\"Save current frame as image\"\"\"\n        pygame.image.save(self.screen, filename)\n\nclass VideoRecorder:\n    \"\"\"Record gameplay videos\"\"\"\n    def __init__(self, filename: str, fps: int = 30):\n        self.filename = filename\n        self.fps = fps\n        self.frames = []\n\n    def add_frame(self, surface):\n        \"\"\"Add a frame to the video\"\"\"\n        frame_array = pygame.surfarray.array3d(surface)\n        frame_array = np.transpose(frame_array, (1, 0, 2))  # Correct orientation\n        self.frames.append(frame_array)\n\n    def save_video(self):\n        \"\"\"Save recorded frames as video (requires opencv)\"\"\"\n        if not self.frames:\n            return\n\n        try:\n            import cv2\n            height, width, _ = self.frames[0].shape\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            out = cv2.VideoWriter(self.filename, fourcc, self.fps, (width, height))\n\n            for frame in self.frames:\n                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n                out.write(frame_bgr)\n\n            out.release()\n            print(f\"Video saved as {self.filename}\")\n        except ImportError:\n            print(\"OpenCV not available. Cannot save video.\")\n\n    def clear(self):\n        \"\"\"Clear recorded frames\"\"\"\n        self.frames = []\n\nprint(\"✅ Renderer implemented successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_header"
      },
      "source": [
        "## 🌍 8. メイン環境 (Main Soccer Environment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nMain Soccer Environment - PettingZoo compatible multi-agent environment\n\"\"\"\n\nimport numpy as np\nimport gymnasium as gym\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom pettingzoo import AECEnv\nfrom pettingzoo.utils import agent_selector as AgentSelector, wrappers\n\n\nclass SoccerEnvironment(AECEnv):\n    \"\"\"\n    PettingZoo-compatible soccer environment for multi-agent reinforcement learning\n    \"\"\"\n\n    metadata = {\n        \"render_modes\": [\"human\", \"rgb_array\"],\n        \"name\": \"soccer_v1\"\n    }\n\n    def __init__(self, config: SoccerEnvironmentConfig = None,\n                 training_config: TrainingConfig = None,\n                 render_mode: str = None,\n                 action_type: str = \"continuous\"):\n        \"\"\"\n        Initialize soccer environment\n\n        Args:\n            config: Environment configuration\n            training_config: Training configuration\n            render_mode: Rendering mode (\"human\", \"rgb_array\", or None)\n            action_type: Action space type (\"continuous\" or \"discrete\")\n        \"\"\"\n        super().__init__()\n\n        self.config = config or SoccerEnvironmentConfig()\n        self.training_config = training_config or TrainingConfig()\n        self.render_mode = render_mode\n        self.action_type = action_type\n\n        # Initialize components\n        self.physics = PhysicsEngine(self.config)\n        self.reward_calculator = RewardCalculator(self.config)\n        self.reward_shaper = RewardShaper(self.config)\n\n        # Initialize spaces\n        self.observation_space_handler = ObservationSpace(self.config)\n        self.action_space_handler = ActionSpace(action_type)\n\n        # Agent setup\n        self.possible_agents = [f\"player_{i}\" for i in range(4)]\n        self.agents = self.possible_agents[:]\n\n        # Observation and action spaces\n        self.observation_spaces = {\n            agent: self.observation_space_handler.gym_space\n            for agent in self.possible_agents\n        }\n        self.action_spaces = {\n            agent: self.action_space_handler.gym_space\n            for agent in self.possible_agents\n        }\n\n        # Agent selector for turn-based execution\n        self._agent_selector = AgentSelector(self.agents)\n\n        # Initialize renderer if needed\n        self.renderer = None\n        if self.render_mode == \"human\":\n            self.renderer = SoccerRenderer(self.config)\n\n        # Game state\n        self.reset()\n\n    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None):\n        \"\"\"Reset the environment\"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n\n        # Reset physics\n        self.physics.reset()\n\n        # Reset agents\n        self.agents = self.possible_agents[:]\n        self._agent_selector = AgentSelector(self.agents)\n        self.agent_selection = self._agent_selector.next()\n\n        # Reset game state\n        self.step_count = 0\n        self.scores = [0, 0]  # [team_0, team_1]\n        self.episode_terminated = False\n        self.episode_truncated = False\n\n        # Reset rewards and info\n        self.rewards = {agent: 0.0 for agent in self.agents}\n        self.terminations = {agent: False for agent in self.agents}\n        self.truncations = {agent: False for agent in self.agents}\n        self.infos = {agent: {} for agent in self.agents}\n\n        # Store previous state for reward calculation\n        self.prev_state = None\n        self.ball_possession = -1  # -1: no possession, 0-3: player id\n        self.last_touch = -1\n\n        return self._get_observations()\n\n    def step(self, action: Any):\n        \"\"\"Execute one step in the environment\"\"\"\n        if self.episode_terminated or self.episode_truncated:\n            return self._was_dead_step(action)\n\n        agent_id = int(self.agent_selection.split('_')[1])\n\n        # Store previous state\n        if self.prev_state is None:\n            self.prev_state = self.physics.get_state()\n\n        # Convert discrete action to continuous if needed\n        if self.action_type == \"discrete\" and isinstance(action, (int, np.integer)):\n            action = self.action_space_handler.convert_discrete_to_continuous(action)\n\n        # Execute action in physics\n        actions = {self.agent_selection: action}\n        goal_result, ball_touched_by = self.physics.step(actions)\n\n        # Update ball possession and last touch\n        if ball_touched_by is not None:\n            self.ball_possession = ball_touched_by\n            self.last_touch = ball_touched_by\n\n        # Handle goal scoring\n        if goal_result:\n            if goal_result == \"goal_left\":\n                self.scores[1] += 1  # Red team scored\n            elif goal_result == \"goal_right\":\n                self.scores[0] += 1  # Blue team scored\n\n        # Get current state\n        current_state = self.physics.get_state()\n\n        # Calculate rewards\n        self._calculate_rewards(agent_id, action, self.prev_state, current_state,\n                              goal_result, ball_touched_by)\n\n        # Update step count\n        self.step_count += 1\n\n        # Check termination conditions\n        self._check_termination()\n\n        # Move to next agent\n        self.agent_selection = self._agent_selector.next()\n\n        # Update previous state\n        self.prev_state = current_state\n\n    def _calculate_rewards(self, agent_id: int, action: np.ndarray,\n                          prev_state: Dict, current_state: Dict,\n                          goal_result: Optional[str], ball_touched_by: Optional[int]):\n        \"\"\"Calculate rewards for all agents\"\"\"\n        # Reset rewards for this step\n        self.rewards = {agent: 0.0 for agent in self.agents}\n\n        # Calculate rewards for each agent\n        for i, agent in enumerate(self.agents):\n            reward = self.reward_shaper.shaped_reward(\n                i, action if i == agent_id else np.zeros(5),\n                prev_state, current_state,\n                goal_scored=goal_result,\n                ball_touched_by=ball_touched_by,\n                scores=tuple(self.scores)\n            )\n            self.rewards[agent] = reward\n\n    def _check_termination(self):\n        \"\"\"Check if episode should terminate\"\"\"\n        # Game ends if max steps reached\n        if self.step_count >= self.config.MAX_STEPS:\n            self.episode_truncated = True\n\n        # Game ends if goal difference is too large (optional)\n        goal_diff = abs(self.scores[0] - self.scores[1])\n        if goal_diff >= 5:  # End early if one team is dominating\n            self.episode_terminated = True\n\n        # Update termination/truncation for all agents\n        if self.episode_terminated or self.episode_truncated:\n            for agent in self.agents:\n                self.terminations[agent] = self.episode_terminated\n                self.truncations[agent] = self.episode_truncated\n\n    def _get_observations(self) -> Dict[str, np.ndarray]:\n        \"\"\"Get observations for all agents\"\"\"\n        current_state = self.physics.get_state()\n        observations = {}\n\n        for i, agent in enumerate(self.agents):\n            obs = self.observation_space_handler.create_observation(\n                i, current_state, tuple(self.scores), self.step_count,\n                self.config.MAX_STEPS, self.ball_possession, self.last_touch\n            )\n            observations[agent] = obs\n\n        return observations\n\n    def observe(self, agent: str) -> np.ndarray:\n        \"\"\"Get observation for specific agent\"\"\"\n        agent_id = int(agent.split('_')[1])\n        current_state = self.physics.get_state()\n\n        return self.observation_space_handler.create_observation(\n            agent_id, current_state, tuple(self.scores), self.step_count,\n            self.config.MAX_STEPS, self.ball_possession, self.last_touch\n        )\n\n    def render(self):\n        \"\"\"Render the environment\"\"\"\n        if self.render_mode == \"human\" and self.renderer:\n            current_state = self.physics.get_state()\n            return self.renderer.render(current_state, tuple(self.scores), self.step_count)\n        elif self.render_mode == \"rgb_array\":\n            # Return RGB array for recording\n            if not self.renderer:\n                self.renderer = SoccerRenderer(self.config)\n            current_state = self.physics.get_state()\n            self.renderer.render(current_state, tuple(self.scores), self.step_count)\n            # Convert pygame surface to numpy array\n            import pygame\n            rgb_array = pygame.surfarray.array3d(self.renderer.screen)\n            return np.transpose(rgb_array, (1, 0, 2))\n\n    def close(self):\n        \"\"\"Clean up resources\"\"\"\n        if self.renderer:\n            self.renderer.close()\n\n    def state(self) -> np.ndarray:\n        \"\"\"Get global state (concatenated observations)\"\"\"\n        observations = self._get_observations()\n        return np.concatenate([observations[agent] for agent in self.agents])\n\n    def _was_dead_step(self, action):\n        \"\"\"Handle action taken when episode is over\"\"\"\n        # This method is required by PettingZoo but not used in our implementation\n        pass\n\n# Wrapper functions for easier usage\n\ndef make_soccer_env(config: SoccerEnvironmentConfig = None,\n                   render_mode: str = None,\n                   action_type: str = \"continuous\") -> SoccerEnvironment:\n    \"\"\"Create soccer environment with default settings\"\"\"\n    return SoccerEnvironment(config, render_mode=render_mode, action_type=action_type)\n\ndef make_parallel_soccer_env(config: SoccerEnvironmentConfig = None,\n                            render_mode: str = None,\n                            action_type: str = \"continuous\"):\n    \"\"\"Create parallel version of soccer environment\"\"\"\n    from pettingzoo.utils import parallel_to_aec\n    env = make_soccer_env(config, render_mode, action_type)\n    return parallel_to_aec(env)\n\n# Compatibility with stable-baselines3\nclass SB3SoccerEnv:\n    \"\"\"Stable-Baselines3 compatible wrapper\"\"\"\n    def __init__(self, config: SoccerEnvironmentConfig = None,\n                 action_type: str = \"continuous\"):\n        self.env = make_soccer_env(config, action_type=action_type)\n        self.agents = self.env.agents\n        self.num_agents = len(self.agents)\n\n        # For SB3 compatibility\n        self.observation_space = self.env.observation_spaces[self.agents[0]]\n        self.action_space = self.env.action_spaces[self.agents[0]]\n\n    def reset(self):\n        observations = self.env.reset()\n        return np.array([observations[agent] for agent in self.agents])\n\n    def step(self, actions):\n        # Execute actions for all agents simultaneously\n        rewards = []\n        done = False\n        infos = []\n\n        for i, agent in enumerate(self.agents):\n            if not done:\n                self.env.step(actions[i])\n                rewards.append(self.env.rewards[agent])\n                done = self.env.terminations[agent] or self.env.truncations[agent]\n                infos.append(self.env.infos[agent])\n\n        obs = [self.env.observe(agent) for agent in self.agents]\n        return np.array(obs), np.array(rewards), done, infos\n\nprint(\"✅ Soccer environment implemented successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agents_header"
      },
      "source": [
        "## 🤖 9. エージェント実装 (Agents)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agents_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nAgent implementations for soccer environment\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom collections import deque\nimport random\n\n\nclass BaseAgent(ABC):\n    \"\"\"Base class for all agents\"\"\"\n\n    def __init__(self, agent_id: int, action_space_size: int):\n        self.agent_id = agent_id\n        self.action_space_size = action_space_size\n\n    @abstractmethod\n    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Select action based on observation\"\"\"\n        pass\n\n    @abstractmethod\n    def learn(self, experiences: List) -> Dict[str, float]:\n        \"\"\"Learn from experiences\"\"\"\n        pass\n\n    def save(self, filepath: str):\n        \"\"\"Save agent model\"\"\"\n        pass\n\n    def load(self, filepath: str):\n        \"\"\"Load agent model\"\"\"\n        pass\n\nclass RandomAgent(BaseAgent):\n    \"\"\"Random agent for baseline and testing\"\"\"\n\n    def __init__(self, agent_id: int, action_space_size: int = 5,\n                 action_type: str = \"continuous\"):\n        super().__init__(agent_id, action_space_size)\n        self.action_type = action_type\n\n    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Select random action\"\"\"\n        if self.action_type == \"continuous\":\n            # Continuous action: [move_x, move_y, kick_power, kick_dir_x, kick_dir_y]\n            action = np.array([\n                np.random.uniform(-1, 1),  # move_x\n                np.random.uniform(-1, 1),  # move_y\n                np.random.uniform(0, 1),   # kick_power\n                np.random.uniform(-1, 1),  # kick_dir_x\n                np.random.uniform(-1, 1),  # kick_dir_y\n            ], dtype=np.float32)\n        else:\n            # Discrete action\n            action = np.random.randint(0, 9)  # 9 possible actions\n\n        return action\n\n    def learn(self, experiences: List) -> Dict[str, float]:\n        \"\"\"Random agent doesn't learn\"\"\"\n        return {\"loss\": 0.0}\n\nclass MLPNetwork(nn.Module):\n    \"\"\"Multi-layer perceptron network\"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n        super().__init__()\n\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            prev_dim = hidden_dim\n\n        layers.append(nn.Linear(prev_dim, output_dim))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.network(x)\n\nclass DQNAgent(BaseAgent):\n    \"\"\"Deep Q-Network agent\"\"\"\n\n    def __init__(self, agent_id: int, obs_dim: int, action_dim: int = 9,\n                 hidden_dims: Tuple[int, ...] = (256, 128),\n                 lr: float = 1e-3, gamma: float = 0.99,\n                 epsilon: float = 1.0, epsilon_decay: float = 0.995,\n                 epsilon_min: float = 0.01, buffer_size: int = 10000,\n                 batch_size: int = 64):\n        super().__init__(agent_id, action_dim)\n\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.batch_size = batch_size\n\n        # Neural networks\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.q_network = MLPNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n        self.target_network = MLPNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n        # Experience replay buffer\n        self.replay_buffer = deque(maxlen=buffer_size)\n\n        # Copy weights to target network\n        self.update_target_network()\n\n    def select_action(self, observation: np.ndarray, training: bool = True) -> int:\n        \"\"\"Select action using epsilon-greedy policy\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.action_dim)\n\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n            q_values = self.q_network(obs_tensor)\n            action = q_values.argmax(dim=1).item()\n\n        return action\n\n    def store_experience(self, state: np.ndarray, action: int, reward: float,\n                        next_state: np.ndarray, done: bool):\n        \"\"\"Store experience in replay buffer\"\"\"\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\n    def learn(self, experiences: List = None) -> Dict[str, float]:\n        \"\"\"Learn from experiences in replay buffer\"\"\"\n        if len(self.replay_buffer) < self.batch_size:\n            return {\"loss\": 0.0}\n\n        # Sample batch from replay buffer\n        batch = random.sample(self.replay_buffer, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.BoolTensor(dones).to(self.device)\n\n        # Compute current Q values\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n\n        # Compute target Q values\n        with torch.no_grad():\n            next_q_values = self.target_network(next_states).max(1)[0]\n            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        # Compute loss and update\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n\n        # Update epsilon\n        if training:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n        return {\"loss\": loss.item(), \"epsilon\": self.epsilon}\n\n    def update_target_network(self):\n        \"\"\"Copy weights from main network to target network\"\"\"\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def save(self, filepath: str):\n        \"\"\"Save model\"\"\"\n        torch.save({\n            'q_network': self.q_network.state_dict(),\n            'target_network': self.target_network.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'epsilon': self.epsilon\n        }, filepath)\n\n    def load(self, filepath: str):\n        \"\"\"Load model\"\"\"\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.q_network.load_state_dict(checkpoint['q_network'])\n        self.target_network.load_state_dict(checkpoint['target_network'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.epsilon = checkpoint['epsilon']\n\nclass Actor(nn.Module):\n    \"\"\"Actor network for MADDPG\"\"\"\n\n    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n        super().__init__()\n        self.network = MLPNetwork(obs_dim, action_dim, hidden_dims)\n\n    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n        return torch.tanh(self.network(obs))\n\nclass Critic(nn.Module):\n    \"\"\"Critic network for MADDPG\"\"\"\n\n    def __init__(self, global_obs_dim: int, global_action_dim: int,\n                 hidden_dims: Tuple[int, ...]):\n        super().__init__()\n        self.network = MLPNetwork(global_obs_dim + global_action_dim, 1, hidden_dims)\n\n    def forward(self, global_obs: torch.Tensor, global_actions: torch.Tensor) -> torch.Tensor:\n        x = torch.cat([global_obs, global_actions], dim=1)\n        return self.network(x)\n\nclass OUNoise:\n    \"\"\"Ornstein-Uhlenbeck noise for exploration\"\"\"\n\n    def __init__(self, action_dim: int, mu: float = 0.0, theta: float = 0.15,\n                 sigma: float = 0.2):\n        self.action_dim = action_dim\n        self.mu = mu\n        self.theta = theta\n        self.sigma = sigma\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset noise\"\"\"\n        self.state = np.ones(self.action_dim) * self.mu\n\n    def sample(self) -> np.ndarray:\n        \"\"\"Sample noise\"\"\"\n        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.action_dim)\n        self.state += dx\n        return self.state\n\nclass MADDPGAgent(BaseAgent):\n    \"\"\"Multi-Agent Deep Deterministic Policy Gradient agent\"\"\"\n\n    def __init__(self, agent_id: int, config: MADDPGConfig):\n        super().__init__(agent_id, config.action_dim)\n\n        self.config = config\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Networks\n        self.actor = Actor(config.obs_dim, config.action_dim, config.hidden_dims).to(self.device)\n        self.critic = Critic(config.global_obs_dim, config.global_action_dim,\n                           config.hidden_dims).to(self.device)\n        self.target_actor = Actor(config.obs_dim, config.action_dim, config.hidden_dims).to(self.device)\n        self.target_critic = Critic(config.global_obs_dim, config.global_action_dim,\n                                  config.hidden_dims).to(self.device)\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)\n\n        # Copy weights to target networks\n        self.target_actor.load_state_dict(self.actor.state_dict())\n        self.target_critic.load_state_dict(self.critic.state_dict())\n\n        # Noise for exploration\n        self.noise = OUNoise(config.action_dim, sigma=config.noise_scale)\n\n    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Select action using actor network\"\"\"\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n            action = self.actor(obs_tensor).cpu().data.numpy().flatten()\n\n        if training:\n            action += self.noise.sample()\n            action = np.clip(action, -1, 1)\n\n        return action\n\n    def learn(self, experiences: Dict) -> Dict[str, float]:\n        \"\"\"Learn from experiences (implemented in trainer)\"\"\"\n        # This will be implemented in the MADDPG trainer\n        return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n\n    def soft_update(self, local_model: nn.Module, target_model: nn.Module):\n        \"\"\"Soft update of target network\"\"\"\n        for target_param, local_param in zip(target_model.parameters(),\n                                           local_model.parameters()):\n            target_param.data.copy_(\n                self.config.tau * local_param.data + (1.0 - self.config.tau) * target_param.data\n            )\n\n    def save(self, filepath: str):\n        \"\"\"Save model\"\"\"\n        torch.save({\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'target_actor': self.target_actor.state_dict(),\n            'target_critic': self.target_critic.state_dict(),\n            'actor_optimizer': self.actor_optimizer.state_dict(),\n            'critic_optimizer': self.critic_optimizer.state_dict(),\n        }, filepath)\n\n    def load(self, filepath: str):\n        \"\"\"Load model\"\"\"\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.actor.load_state_dict(checkpoint['actor'])\n        self.critic.load_state_dict(checkpoint['critic'])\n        self.target_actor.load_state_dict(checkpoint['target_actor'])\n        self.target_critic.load_state_dict(checkpoint['target_critic'])\n        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n\ndef create_agent(agent_type: str, agent_id: int, config: Dict) -> BaseAgent:\n    \"\"\"Factory function to create agents\"\"\"\n    if agent_type == \"random\":\n        return RandomAgent(agent_id)\n    elif agent_type == \"dqn\":\n        return DQNAgent(agent_id, **config)\n    elif agent_type == \"maddpg\":\n        return MADDPGAgent(agent_id, **config)\n    else:\n        raise ValueError(f\"Unknown agent type: {agent_type}\")\n\nprint(\"✅ Agents implemented successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainers_header"
      },
      "source": [
        "## 📚 10. トレーニングフレームワーク (Training Frameworks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trainers_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nTraining frameworks for multi-agent soccer environment\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom collections import deque, defaultdict\nimport random\nimport time\nfrom abc import ABC, abstractmethod\n\n\nclass ReplayBuffer:\n    \"\"\"Experience replay buffer for multi-agent learning\"\"\"\n\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, experience: Tuple):\n        \"\"\"Add experience to buffer\"\"\"\n        self.buffer.append(experience)\n\n    def sample(self, batch_size: int) -> List[Tuple]:\n        \"\"\"Sample batch from buffer\"\"\"\n        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n\n    def __len__(self):\n        return len(self.buffer)\n\nclass BaseTrainer(ABC):\n    \"\"\"Base class for all training frameworks\"\"\"\n\n    def __init__(self, env_config: SoccerEnvironmentConfig,\n                 training_config: TrainingConfig):\n        self.env_config = env_config\n        self.training_config = training_config\n        self.env = make_soccer_env(env_config, render_mode=None)\n\n        # Training statistics\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.scores_history = []\n        self.training_metrics = defaultdict(list)\n\n    @abstractmethod\n    def train(self, num_episodes: int) -> Dict[str, Any]:\n        \"\"\"Train agents for specified number of episodes\"\"\"\n        pass\n\n    def evaluate(self, num_episodes: int = 10) -> Dict[str, float]:\n        \"\"\"Evaluate current agent performance\"\"\"\n        total_rewards = []\n        total_lengths = []\n        team_scores = [[], []]\n\n        for episode in range(num_episodes):\n            observations = self.env.reset()\n            episode_reward = 0\n            steps = 0\n\n            while not all(self.env.terminations.values()) and not all(self.env.truncations.values()):\n                actions = {}\n                for agent in self.env.agents:\n                    if not self.env.terminations.get(agent, False) and not self.env.truncations.get(agent, False):\n                        obs = self.env.observe(agent)\n                        actions[agent] = self._get_agent_action(agent, obs, training=False)\n\n                for agent, action in actions.items():\n                    self.env.step(action)\n                    episode_reward += self.env.rewards.get(agent, 0)\n                    steps += 1\n\n                    if self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False):\n                        break\n\n            total_rewards.append(episode_reward)\n            total_lengths.append(steps)\n            team_scores[0].append(self.env.scores[0])\n            team_scores[1].append(self.env.scores[1])\n\n        return {\n            'avg_reward': np.mean(total_rewards),\n            'avg_length': np.mean(total_lengths),\n            'team_0_avg_score': np.mean(team_scores[0]),\n            'team_1_avg_score': np.mean(team_scores[1]),\n            'win_rate_team_0': sum(1 for i in range(num_episodes) if team_scores[0][i] > team_scores[1][i]) / num_episodes,\n            'win_rate_team_1': sum(1 for i in range(num_episodes) if team_scores[1][i] > team_scores[0][i]) / num_episodes,\n        }\n\n    @abstractmethod\n    def _get_agent_action(self, agent: str, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Get action from agent\"\"\"\n        pass\n\nclass IndependentLearningTrainer(BaseTrainer):\n    \"\"\"Independent learning trainer where each agent learns separately\"\"\"\n\n    def __init__(self, env_config: SoccerEnvironmentConfig,\n                 training_config: TrainingConfig,\n                 agent_type: str = \"dqn\",\n                 agent_configs: Dict = None):\n        super().__init__(env_config, training_config)\n\n        self.agent_type = agent_type\n        self.agent_configs = agent_configs or {}\n\n        # Create agents\n        self.agents = {}\n        for i, agent_name in enumerate(self.env.agents):\n            if agent_type == \"dqn\":\n                self.agents[agent_name] = DQNAgent(\n                    agent_id=i,\n                    obs_dim=28,  # From observation space\n                    **self.agent_configs\n                )\n            elif agent_type == \"random\":\n                self.agents[agent_name] = RandomAgent(i)\n            else:\n                raise ValueError(f\"Unknown agent type: {agent_type}\")\n\n    def train(self, num_episodes: int) -> Dict[str, Any]:\n        \"\"\"Train agents independently\"\"\"\n        print(f\"Starting independent learning training with {self.agent_type} agents\")\n\n        for episode in range(num_episodes):\n            observations = self.env.reset()\n            episode_rewards = {agent: 0 for agent in self.env.agents}\n            episode_length = 0\n\n            # Store previous observations for experience replay\n            prev_observations = {}\n\n            while not all(self.env.terminations.values()) and not all(self.env.truncations.values()):\n                actions = {}\n\n                # Get actions from all agents\n                for agent in self.env.agents:\n                    if not self.env.terminations.get(agent, False) and not self.env.truncations.get(agent, False):\n                        obs = self.env.observe(agent)\n                        action = self.agents[agent].select_action(obs, training=True)\n                        actions[agent] = action\n                        prev_observations[agent] = obs\n\n                # Execute actions\n                for agent, action in actions.items():\n                    self.env.step(action)\n                    reward = self.env.rewards.get(agent, 0)\n                    episode_rewards[agent] += reward\n                    episode_length += 1\n\n                    # Store experience for DQN agents\n                    if self.agent_type == \"dqn\" and agent in prev_observations:\n                        next_obs = self.env.observe(agent)\n                        done = self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False)\n\n                        if isinstance(action, np.ndarray):\n                            action = int(action[0]) if len(action) > 0 else 0\n\n                        self.agents[agent].store_experience(\n                            prev_observations[agent], action, reward, next_obs, done\n                        )\n\n                        # Learn from experience\n                        metrics = self.agents[agent].learn()\n                        if metrics and metrics['loss'] > 0:\n                            self.training_metrics[f'{agent}_loss'].append(metrics['loss'])\n\n                    if self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False):\n                        break\n\n            # Update target networks for DQN agents\n            if self.agent_type == \"dqn\" and episode % 100 == 0:\n                for agent_name, agent in self.agents.items():\n                    agent.update_target_network()\n\n            # Record episode statistics\n            total_reward = sum(episode_rewards.values())\n            self.episode_rewards.append(total_reward)\n            self.episode_lengths.append(episode_length)\n            self.scores_history.append(self.env.scores.copy())\n\n            # Print progress\n            if episode % 100 == 0:\n                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n                print(f\"Episode {episode}: Avg Reward (last 100): {avg_reward:.2f}, Scores: {self.env.scores}\")\n\n                # Evaluate current performance\n                if episode % 500 == 0 and episode > 0:\n                    eval_metrics = self.evaluate(num_episodes=10)\n                    print(f\"Evaluation: {eval_metrics}\")\n\n        return {\n            'episode_rewards': self.episode_rewards,\n            'episode_lengths': self.episode_lengths,\n            'scores_history': self.scores_history,\n            'training_metrics': dict(self.training_metrics)\n        }\n\n    def _get_agent_action(self, agent: str, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Get action from specific agent\"\"\"\n        if self.agent_type == \"dqn\":\n            discrete_action = self.agents[agent].select_action(observation, training)\n            # Convert discrete action to continuous for environment\n            action_space = ActionSpace(\"discrete\")\n            return action_space.convert_discrete_to_continuous(discrete_action)\n        else:\n            return self.agents[agent].select_action(observation, training)\n\nclass MADDPGTrainer(BaseTrainer):\n    \"\"\"MADDPG trainer with centralized critic\"\"\"\n\n    def __init__(self, env_config: SoccerEnvironmentConfig,\n                 training_config: TrainingConfig,\n                 maddpg_config: MADDPGConfig):\n        super().__init__(env_config, training_config)\n        self.maddpg_config = maddpg_config\n\n        # Create MADDPG agents\n        self.agents = {}\n        for i, agent_name in enumerate(self.env.agents):\n            self.agents[agent_name] = MADDPGAgent(i, maddpg_config)\n\n        # Shared replay buffer\n        self.replay_buffer = ReplayBuffer(maddpg_config.buffer_size)\n\n    def train(self, num_episodes: int) -> Dict[str, Any]:\n        \"\"\"Train MADDPG agents\"\"\"\n        print(\"Starting MADDPG training\")\n\n        for episode in range(num_episodes):\n            observations = self.env.reset()\n            episode_rewards = {agent: 0 for agent in self.env.agents}\n            episode_length = 0\n\n            # Episode experience\n            episode_experiences = []\n\n            while not all(self.env.terminations.values()) and not all(self.env.truncations.values()):\n                # Get global observation and actions\n                global_obs = []\n                actions = {}\n\n                for agent in self.env.agents:\n                    if not self.env.terminations.get(agent, False) and not self.env.truncations.get(agent, False):\n                        obs = self.env.observe(agent)\n                        action = self.agents[agent].select_action(obs, training=True)\n                        actions[agent] = action\n                        global_obs.append(obs)\n\n                # Execute actions and collect rewards\n                global_actions = list(actions.values())\n                step_experience = {\n                    'global_obs': np.concatenate(global_obs),\n                    'actions': actions.copy(),\n                    'global_actions': np.concatenate(global_actions),\n                    'rewards': {},\n                    'next_global_obs': None,\n                    'dones': {}\n                }\n\n                for agent, action in actions.items():\n                    self.env.step(action)\n                    reward = self.env.rewards.get(agent, 0)\n                    episode_rewards[agent] += reward\n                    step_experience['rewards'][agent] = reward\n                    step_experience['dones'][agent] = self.env.terminations.get(agent, False) or self.env.truncations.get(agent, False)\n                    episode_length += 1\n\n                    if step_experience['dones'][agent]:\n                        break\n\n                # Get next global observation\n                next_global_obs = []\n                for agent in self.env.agents:\n                    next_obs = self.env.observe(agent)\n                    next_global_obs.append(next_obs)\n                step_experience['next_global_obs'] = np.concatenate(next_global_obs)\n\n                episode_experiences.append(step_experience)\n\n            # Store experiences in replay buffer\n            for exp in episode_experiences:\n                self.replay_buffer.push(exp)\n\n            # Train agents if enough experiences\n            if len(self.replay_buffer) > self.maddpg_config.batch_size:\n                self._train_maddpg_step()\n\n            # Record statistics\n            total_reward = sum(episode_rewards.values())\n            self.episode_rewards.append(total_reward)\n            self.episode_lengths.append(episode_length)\n            self.scores_history.append(self.env.scores.copy())\n\n            # Print progress\n            if episode % 100 == 0:\n                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n                print(f\"Episode {episode}: Avg Reward: {avg_reward:.2f}, Scores: {self.env.scores}\")\n\n        return {\n            'episode_rewards': self.episode_rewards,\n            'episode_lengths': self.episode_lengths,\n            'scores_history': self.scores_history,\n            'training_metrics': dict(self.training_metrics)\n        }\n\n    def _train_maddpg_step(self):\n        \"\"\"Perform one MADDPG training step\"\"\"\n        batch = self.replay_buffer.sample(self.maddpg_config.batch_size)\n\n        for i, (agent_name, agent) in enumerate(self.agents.items()):\n            # Extract data for this agent\n            states = torch.FloatTensor([exp['global_obs'] for exp in batch]).to(agent.device)\n            actions = torch.FloatTensor([exp['global_actions'] for exp in batch]).to(agent.device)\n            rewards = torch.FloatTensor([exp['rewards'][agent_name] for exp in batch]).to(agent.device)\n            next_states = torch.FloatTensor([exp['next_global_obs'] for exp in batch]).to(agent.device)\n            dones = torch.BoolTensor([exp['dones'][agent_name] for exp in batch]).to(agent.device)\n\n            # Get agent-specific observations\n            agent_obs = states[:, i*28:(i+1)*28]  # 28D observation per agent\n            next_agent_obs = next_states[:, i*28:(i+1)*28]\n\n            # Update critic\n            with torch.no_grad():\n                next_actions = torch.cat([\n                    self.agents[list(self.agents.keys())[j]].target_actor(next_states[:, j*28:(j+1)*28])\n                    for j in range(len(self.agents))\n                ], dim=1)\n                target_q = agent.target_critic(next_states, next_actions)\n                target_q = rewards + (self.maddpg_config.gamma * target_q * ~dones)\n\n            current_q = agent.critic(states, actions)\n            critic_loss = F.mse_loss(current_q.squeeze(), target_q.squeeze())\n\n            agent.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), max_norm=0.5)\n            agent.critic_optimizer.step()\n\n            # Update actor\n            agent_actions = agent.actor(agent_obs)\n            full_actions = actions.clone()\n            full_actions[:, i*5:(i+1)*5] = agent_actions  # 5D action per agent\n\n            actor_loss = -agent.critic(states, full_actions).mean()\n\n            agent.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            torch.nn.utils.clip_grad_norm_(agent.actor.parameters(), max_norm=0.5)\n            agent.actor_optimizer.step()\n\n            # Soft update target networks\n            agent.soft_update(agent.actor, agent.target_actor)\n            agent.soft_update(agent.critic, agent.target_critic)\n\n            # Record metrics\n            self.training_metrics[f'{agent_name}_critic_loss'].append(critic_loss.item())\n            self.training_metrics[f'{agent_name}_actor_loss'].append(actor_loss.item())\n\n    def _get_agent_action(self, agent: str, observation: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Get action from MADDPG agent\"\"\"\n        return self.agents[agent].select_action(observation, training)\n\ndef create_trainer(trainer_type: str, env_config: SoccerEnvironmentConfig,\n                  training_config: TrainingConfig, **kwargs) -> BaseTrainer:\n    \"\"\"Factory function to create trainers\"\"\"\n    if trainer_type == \"independent\":\n        return IndependentLearningTrainer(env_config, training_config, **kwargs)\n    elif trainer_type == \"maddpg\":\n        maddpg_config = kwargs.get('maddpg_config', MADDPGConfig())\n        return MADDPGTrainer(env_config, training_config, maddpg_config)\n    else:\n        raise ValueError(f\"Unknown trainer type: {trainer_type}\")\n\nprint(\"✅ Training frameworks implemented successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_header"
      },
      "source": [
        "## 🧪 11. テスト関数 (Test Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nTest script for soccer environment with random agents\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict\nimport time\n\n\ndef test_basic_environment():\n    \"\"\"Test basic environment functionality\"\"\"\n    print(\"Testing basic environment functionality...\")\n\n    # Create environment\n    config = SoccerEnvironmentConfig()\n    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n    print(f\"Environment created with {len(env.agents)} agents\")\n    print(f\"Agents: {env.agents}\")\n    print(f\"Observation space: {env.observation_spaces[env.agents[0]]}\")\n    print(f\"Action space: {env.action_spaces[env.agents[0]]}\")\n\n    # Test reset\n    observations = env.reset()\n    print(f\"Reset successful, observations shape: {[obs.shape for obs in observations.values()]}\")\n\n    # Test step\n    for agent in env.agents:\n        action = env.action_spaces[agent].sample()\n        print(f\"Agent {agent} taking action: {action}\")\n        env.step(action)\n\n    print(\"Basic environment test completed successfully!\")\n    return True\n\ndef test_random_agents_episode():\n    \"\"\"Test full episode with random agents\"\"\"\n    print(\"Testing full episode with random agents...\")\n\n    # Create environment and agents\n    config = SoccerEnvironmentConfig()\n    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n    # Create random agents\n    agents = {}\n    for i, agent_name in enumerate(env.agents):\n        agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n\n    # Run episode\n    observations = env.reset()\n    episode_rewards = {agent: 0 for agent in env.agents}\n    episode_length = 0\n\n    print(\"Running episode...\")\n    start_time = time.time()\n\n    while not all(env.terminations.values()) and not all(env.truncations.values()):\n        for agent in env.agents:\n            if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n                # Get action from agent\n                obs = env.observe(agent)\n                action = agents[agent].select_action(obs, training=False)\n\n                # Take step\n                env.step(action)\n\n                # Accumulate reward\n                episode_rewards[agent] += env.rewards.get(agent, 0)\n\n                episode_length += 1\n\n                # Break if episode is done\n                if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n                    break\n\n    elapsed_time = time.time() - start_time\n\n    print(f\"Episode completed in {elapsed_time:.2f} seconds\")\n    print(f\"Episode length: {episode_length} steps\")\n    print(f\"Final scores: {env.scores}\")\n    print(f\"Episode rewards: {episode_rewards}\")\n\n    env.close()\n    return True\n\ndef test_multiple_episodes(num_episodes: int = 5):\n    \"\"\"Test multiple episodes and collect statistics\"\"\"\n    print(f\"Testing {num_episodes} episodes for performance analysis...\")\n\n    config = SoccerEnvironmentConfig()\n    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n    # Create random agents\n    agents = {}\n    for i, agent_name in enumerate(env.agents):\n        agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n\n    # Statistics\n    episode_lengths = []\n    episode_rewards = []\n    final_scores = []\n\n    for episode in range(num_episodes):\n        print(f\"Episode {episode + 1}/{num_episodes}\")\n\n        observations = env.reset()\n        episode_reward = {agent: 0 for agent in env.agents}\n        steps = 0\n\n        while not all(env.terminations.values()) and not all(env.truncations.values()):\n            for agent in env.agents:\n                if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n                    obs = env.observe(agent)\n                    action = agents[agent].select_action(obs, training=False)\n                    env.step(action)\n                    episode_reward[agent] += env.rewards.get(agent, 0)\n                    steps += 1\n\n                    if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n                        break\n\n        episode_lengths.append(steps)\n        episode_rewards.append(episode_reward)\n        final_scores.append(env.scores.copy())\n\n        print(f\"  Steps: {steps}, Scores: {env.scores}, Avg Reward: {np.mean(list(episode_reward.values())):.2f}\")\n\n    # Print statistics\n    print(f\"\\n=== Statistics over {num_episodes} episodes ===\")\n    print(f\"Average episode length: {np.mean(episode_lengths):.2f} ± {np.std(episode_lengths):.2f}\")\n\n    # Team scores\n    team_0_scores = [score[0] for score in final_scores]\n    team_1_scores = [score[1] for score in final_scores]\n\n    print(f\"Team 0 (Blue) average score: {np.mean(team_0_scores):.2f} ± {np.std(team_0_scores):.2f}\")\n    print(f\"Team 1 (Red) average score: {np.mean(team_1_scores):.2f} ± {np.std(team_1_scores):.2f}\")\n\n    # Average rewards per agent\n    for agent in env.agents:\n        agent_rewards = [ep_reward[agent] for ep_reward in episode_rewards]\n        print(f\"{agent} average reward: {np.mean(agent_rewards):.2f} ± {np.std(agent_rewards):.2f}\")\n\n    env.close()\n    return True\n\ndef run_all_tests():\n    \"\"\"Run all tests\"\"\"\n    print(\"=\" * 60)\n    print(\"Multi-Agent Soccer Environment Test Suite\")\n    print(\"=\" * 60)\n\n    tests = [\n        (\"Basic Environment\", test_basic_environment),\n        (\"Random Agents Episode\", test_random_agents_episode),\n        (\"Multiple Episodes\", lambda: test_multiple_episodes(3))\n    ]\n\n    results = []\n    for test_name, test_func in tests:\n        print(f\"\\n[TEST] {test_name}\")\n        print(\"-\" * 40)\n        try:\n            result = test_func()\n            results.append(result)\n            print(f\"✓ {test_name} PASSED\")\n        except Exception as e:\n            print(f\"✗ {test_name} FAILED: {e}\")\n            results.append(False)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TEST SUMMARY\")\n    print(\"=\" * 60)\n    for i, (test_name, _) in enumerate(tests):\n        status = \"PASSED\" if results[i] else \"FAILED\"\n        print(f\"{test_name}: {status}\")\n\n    success_rate = sum(results) / len(results)\n    print(f\"\\nSuccess Rate: {success_rate:.1%} ({sum(results)}/{len(results)})\")\n\n    return success_rate == 1.0\n\nif __name__ == \"__main__\":\n    run_all_tests()\n\nprint(\"✅ Test functions implemented successfully!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "execution_header"
      },
      "source": [
        "## 🚀 12. 実行セクション (Execution Section)\n\n",
        "以下のセルで環境をテストし、エージェントを訓練できます。\n",
        "**重要**: 上記のすべてのコードセルを実行してから、以下のセルを実行してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_env_header"
      },
      "source": [
        "### 12.1 環境テスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_env"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 環境のテストを実行\n",
        "print(\"🧪 Running environment tests...\")\n",
        "print(\"=\" * 60)\n\n",
        "test_passed = run_all_tests()\n\n",
        "if test_passed:\n",
        "    print(\"\\n✅ All environment tests passed! Ready to proceed with training.\")\n",
        "else:\n",
        "    print(\"\\n❌ Some tests failed. Please check the implementation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baseline_header"
      },
      "source": [
        "### 12.2 環境作成とベースライン実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baseline_code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 環境の作成\n",
        "config = SoccerEnvironmentConfig()\n",
        "env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n\n",
        "print(\"📊 Environment Configuration:\")\n",
        "print(f\"  Field Size: {config.FIELD_SIZE}\")\n",
        "print(f\"  Max Steps: {config.MAX_STEPS}\")\n",
        "print(f\"  Players per Team: {config.NUM_PLAYERS_PER_TEAM}\")\n",
        "print(f\"  Agents: {env.agents}\")\n",
        "print(f\"  Observation space: {env.observation_spaces[env.agents[0]].shape}\")\n",
        "print(f\"  Action space: {env.action_spaces[env.agents[0]].shape}\")\n\n",
        "# ランダムエージェントでベースライン実行\n",
        "print(\"\\n🎲 Running baseline with random agents...\")\n",
        "print(\"=\" * 60)\n\n",
        "random_agents = {}\n",
        "for i, agent_name in enumerate(env.agents):\n",
        "    random_agents[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n\n",
        "# 5エピソード実行\n",
        "episode_stats = []\n",
        "for episode in range(5):\n",
        "    env.reset()\n",
        "    episode_reward = {agent: 0 for agent in env.agents}\n",
        "    steps = 0\n",
        "    \n",
        "    while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
        "        for agent in env.agents:\n",
        "            if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n",
        "                obs = env.observe(agent)\n",
        "                action = random_agents[agent].select_action(obs, training=False)\n",
        "                env.step(action)\n",
        "                episode_reward[agent] += env.rewards.get(agent, 0)\n",
        "                steps += 1\n",
        "                \n",
        "                if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n",
        "                    break\n",
        "    \n",
        "    print(f\"Episode {episode + 1}: {steps} steps, Scores: {env.scores}, \"\n",
        "          f\"Total Reward: {sum(episode_reward.values()):.2f}\")\n",
        "    \n",
        "    episode_stats.append({\n",
        "        'episode': episode,\n",
        "        'steps': steps,\n",
        "        'scores': env.scores.copy(),\n",
        "        'total_reward': sum(episode_reward.values())\n",
        "    })\n\n",
        "# 統計表示\n",
        "avg_steps = np.mean([stat['steps'] for stat in episode_stats])\n",
        "avg_reward = np.mean([stat['total_reward'] for stat in episode_stats])\n",
        "team_0_wins = sum(1 for stat in episode_stats if stat['scores'][0] > stat['scores'][1])\n",
        "team_1_wins = sum(1 for stat in episode_stats if stat['scores'][1] > stat['scores'][0])\n",
        "draws = len(episode_stats) - team_0_wins - team_1_wins\n\n",
        "print(f\"\\n📈 Baseline Statistics (Random Agents):\")\n",
        "print(f\"  Average Steps per Episode: {avg_steps:.1f}\")\n",
        "print(f\"  Average Total Reward: {avg_reward:.2f}\")\n",
        "print(f\"  Team 0 (Blue) Wins: {team_0_wins}\")\n",
        "print(f\"  Team 1 (Red) Wins: {team_1_wins}\")\n",
        "print(f\"  Draws: {draws}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_header"
      },
      "source": [
        "### 12.3 簡単な訓練デモ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "simple_training"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 簡単な訓練デモ（Random Agentsの性能測定）\n",
        "print(\"📊 Performance measurement over multiple episodes...\")\n",
        "print(\"=\" * 60)\n\n",
        "num_test_episodes = 20\n",
        "all_rewards = []\n",
        "all_scores = {'team_0': [], 'team_1': []}\n\n",
        "for ep in range(num_test_episodes):\n",
        "    env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
        "        for agent in env.agents:\n",
        "            if not env.terminations.get(agent, False) and not env.truncations.get(agent, False):\n",
        "                obs = env.observe(agent)\n",
        "                action = random_agents[agent].select_action(obs, training=False)\n",
        "                env.step(action)\n",
        "                episode_reward += env.rewards.get(agent, 0)\n",
        "                \n",
        "                if env.terminations.get(agent, False) or env.truncations.get(agent, False):\n",
        "                    break\n",
        "    \n",
        "    all_rewards.append(episode_reward)\n",
        "    all_scores['team_0'].append(env.scores[0])\n",
        "    all_scores['team_1'].append(env.scores[1])\n",
        "    \n",
        "    if (ep + 1) % 5 == 0:\n",
        "        print(f\"Episodes {ep-3}-{ep+1}: Avg Reward = {np.mean(all_rewards[-5:]):.2f}, \"\n",
        "              f\"Scores = Blue:{np.mean(all_scores['team_0'][-5:]):.1f}, \"\n",
        "              f\"Red:{np.mean(all_scores['team_1'][-5:]):.1f}\")\n\n",
        "# 結果のプロット\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n",
        "# Rewards over time\n",
        "ax1.plot(all_rewards, alpha=0.5)\n",
        "ax1.plot(np.convolve(all_rewards, np.ones(5)/5, mode='valid'), linewidth=2)\n",
        "ax1.set_title('Total Rewards per Episode')\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Total Reward')\n",
        "ax1.grid(True)\n\n",
        "# Team scores\n",
        "ax2.plot(all_scores['team_0'], label='Team 0 (Blue)', alpha=0.7)\n",
        "ax2.plot(all_scores['team_1'], label='Team 1 (Red)', alpha=0.7)\n",
        "ax2.set_title('Team Scores Over Episodes')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Goals Scored')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n\n",
        "plt.tight_layout()\n",
        "plt.show()\n\n",
        "print(f\"\\n📈 Final Statistics ({num_test_episodes} episodes):\")\n",
        "print(f\"  Mean Total Reward: {np.mean(all_rewards):.2f} ± {np.std(all_rewards):.2f}\")\n",
        "print(f\"  Team 0 Mean Score: {np.mean(all_scores['team_0']):.2f} ± {np.std(all_scores['team_0']):.2f}\")\n",
        "print(f\"  Team 1 Mean Score: {np.mean(all_scores['team_1']):.2f} ± {np.std(all_scores['team_1']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## 📝 まとめ\n\n",
        "このノートブックでは、以下を実装しました：\n\n",
        "### ✅ 実装済み機能\n",
        "1. **完全な物理エンジン**: 衝突検出、ボール物理、プレイヤー移動\n",
        "2. **PettingZoo互換環境**: マルチエージェント強化学習の標準インターフェース\n",
        "3. **複数のエージェント実装**:\n",
        "   - Random Agent (ベースライン)\n",
        "   - DQN Agent (独立学習)\n",
        "   - MADDPG Agent (協調学習)\n",
        "4. **豊富な観測空間**: 28次元の観測（位置、速度、ゲーム状態）\n",
        "5. **多目的報酬システム**: ゴール、ボール制御、チームワーク\n",
        "6. **訓練フレームワーク**: 独立学習とMADDPG\n\n",
        "### 🚀 使用方法\n",
        "1. **すべてのコードセルを上から順番に実行**\n",
        "2. 環境テストで動作確認\n",
        "3. ベースライン実行で基本性能を確認\n",
        "4. 必要に応じて訓練パラメータを調整\n\n",
        "### ⚠️ トラブルシューティング\n",
        "- **ModuleNotFoundError**: すべてのコードセルを実行したか確認\n",
        "- **NameError**: セルの実行順序を確認（上から順番に）\n",
        "- **GPU関連エラー**: Runtime → Change runtime type → GPU\n\n",
        "### 📚 参考文献\n",
        "- [PettingZoo Documentation](https://pettingzoo.farama.org/)\n",
        "- [MADDPG Paper](https://arxiv.org/abs/1706.02275)\n",
        "- [DQN Paper](https://arxiv.org/abs/1312.5602)\n\n",
        "**Happy Training! 🎮**"
      ]
    }
  ]
}