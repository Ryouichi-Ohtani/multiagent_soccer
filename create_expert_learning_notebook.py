#!/usr/bin/env python3
"""
Create expert data learning and improved training notebook
"""

import json

def create_expert_learning_cells():
    """Create cells for expert learning and improved training"""
    
    cells = []
    
    # Header
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "expert_learning_header"},
        "source": [
            "# üèÜ Expert Learning and Advanced Training\n",
            "## „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éá„Éº„Çø„Çí‰Ωø„Å£„ÅüÂ≠¶Áøí„Å®ÊîπÂñÑ„Åï„Çå„ÅüË®ìÁ∑¥\n\n",
            "ÁÇπ„ÅåÂÖ•„Çâ„Å™„ÅÑÂïèÈ°å„ÇíËß£Ê±∫„Åô„Çã„Åü„ÇÅ„ÄÅ‰ª•‰∏ã„ÇíÂÆüË£Ö„Åó„Åæ„ÅôÔºö\n",
            "1. „Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊà¶Áï•„ÅÆÂÆüË£ÖÔºà„Éí„É•„Éº„É™„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÔºâ\n",
            "2. Ê®°ÂÄ£Â≠¶ÁøíÔºàImitation LearningÔºâ\n",
            "3. Â†±ÈÖ¨„Ç∑„Çß„Éº„Éî„É≥„Ç∞„ÅÆÊîπÂñÑ\n",
            "4. „Ç´„É™„Ç≠„É•„É©„É†Â≠¶Áøí"
        ]
    })
    
    # Expert agent implementation
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "expert_agent_header"},
        "source": ["### üéØ „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆÂÆüË£Ö"]
    })
    
    cells.append({
        "cell_type": "code",
        "metadata": {"id": "expert_agent_code"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "class ExpertAgent(BaseAgent):\n",
            "    \"\"\"Expert agent with rule-based strategy for scoring goals\"\"\"\n",
            "    \n",
            "    def __init__(self, agent_id: int, team: int, config: SoccerEnvironmentConfig):\n",
            "        super().__init__(agent_id, 5)  # 5D continuous action\n",
            "        self.team = team\n",
            "        self.config = config\n",
            "        self.field_width, self.field_height = config.FIELD_SIZE\n",
            "        \n",
            "    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n",
            "        \"\"\"Select action based on expert strategy\"\"\"\n",
            "        # Parse observation (28 dimensions)\n",
            "        # [0-1]: self position (normalized)\n",
            "        # [2-3]: self velocity\n",
            "        # [4-5]: ball position (normalized)\n",
            "        # [6-7]: ball velocity\n",
            "        # [8-9]: teammate position\n",
            "        # [10-11]: teammate velocity\n",
            "        # [12-15]: opponents positions\n",
            "        # [16-19]: opponents velocities\n",
            "        # [20-23]: goal information\n",
            "        # [24-27]: context\n",
            "        \n",
            "        self_pos = observation[0:2]\n",
            "        ball_pos = observation[4:6]\n",
            "        teammate_pos = observation[8:10]\n",
            "        \n",
            "        # Denormalize positions for strategy\n",
            "        self_x, self_y = self_pos[0] * self.field_width, self_pos[1] * self.field_height\n",
            "        ball_x, ball_y = ball_pos[0] * self.field_width, ball_pos[1] * self.field_height\n",
            "        \n",
            "        # Calculate distances\n",
            "        dist_to_ball = np.sqrt((self_x - ball_x)**2 + (self_y - ball_y)**2)\n",
            "        \n",
            "        # Determine target goal position\n",
            "        if self.team == 0:  # Blue team attacks right\n",
            "            goal_x = self.field_width\n",
            "            goal_y = self.field_height / 2\n",
            "        else:  # Red team attacks left\n",
            "            goal_x = 0\n",
            "            goal_y = self.field_height / 2\n",
            "        \n",
            "        # Strategy 1: Go to ball if far\n",
            "        if dist_to_ball > 50:\n",
            "            # Move towards ball\n",
            "            move_x = np.clip((ball_x - self_x) / 100, -1, 1)\n",
            "            move_y = np.clip((ball_y - self_y) / 100, -1, 1)\n",
            "            kick_power = 0.0\n",
            "            kick_dir_x = 0.0\n",
            "            kick_dir_y = 0.0\n",
            "        \n",
            "        # Strategy 2: Kick towards goal if close to ball\n",
            "        else:\n",
            "            # Move towards ball for better position\n",
            "            move_x = np.clip((ball_x - self_x) / 50, -1, 1)\n",
            "            move_y = np.clip((ball_y - self_y) / 50, -1, 1)\n",
            "            \n",
            "            # Calculate kick direction towards goal\n",
            "            kick_dir_x = np.clip((goal_x - ball_x) / self.field_width, -1, 1)\n",
            "            kick_dir_y = np.clip((goal_y - ball_y) / self.field_height, -1, 1)\n",
            "            \n",
            "            # Strong kick when aligned with goal\n",
            "            alignment = abs(kick_dir_y) < 0.3  # Close to horizontal alignment\n",
            "            kick_power = 0.8 if alignment else 0.5\n",
            "        \n",
            "        # Add some randomness for diversity\n",
            "        if training and np.random.random() < 0.1:\n",
            "            move_x += np.random.uniform(-0.2, 0.2)\n",
            "            move_y += np.random.uniform(-0.2, 0.2)\n",
            "        \n",
            "        action = np.array([move_x, move_y, kick_power, kick_dir_x, kick_dir_y], dtype=np.float32)\n",
            "        return np.clip(action, [-1, -1, 0, -1, -1], [1, 1, 1, 1, 1])\n",
            "    \n",
            "    def learn(self, experiences: List) -> Dict[str, float]:\n",
            "        \"\"\"Expert doesn't learn\"\"\"\n",
            "        return {\"loss\": 0.0}\n",
            "\n",
            "print(\"‚úÖ Expert agent with goal-scoring strategy implemented!\")"
        ]
    })
    
    # Collect expert demonstrations
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "collect_demos_header"},
        "source": ["### üìä „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éá„É¢„É≥„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥„ÅÆÂèéÈõÜ"]
    })
    
    cells.append({
        "cell_type": "code",
        "metadata": {"id": "collect_demonstrations"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "def collect_expert_demonstrations(num_episodes=50):\n",
            "    \"\"\"Collect expert demonstrations for imitation learning\"\"\"\n",
            "    print(f\"üìä Collecting {num_episodes} episodes of expert demonstrations...\")\n",
            "    \n",
            "    config = SoccerEnvironmentConfig()\n",
            "    config.MAX_STEPS = 300  # Shorter episodes\n",
            "    env = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n",
            "    \n",
            "    # Create expert agents\n",
            "    expert_agents = {}\n",
            "    for i, agent_name in enumerate(env.agents):\n",
            "        team = i // 2  # 0 or 1\n",
            "        expert_agents[agent_name] = ExpertAgent(i, team, config)\n",
            "    \n",
            "    demonstrations = []\n",
            "    total_goals = {'team_0': 0, 'team_1': 0}\n",
            "    \n",
            "    for episode in range(num_episodes):\n",
            "        env.reset()\n",
            "        episode_data = []\n",
            "        \n",
            "        while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
            "            for agent_name in env.agents:\n",
            "                if not env.terminations.get(agent_name, False) and not env.truncations.get(agent_name, False):\n",
            "                    # Get observation\n",
            "                    obs = env.observe(agent_name)\n",
            "                    \n",
            "                    # Get expert action\n",
            "                    action = expert_agents[agent_name].select_action(obs, training=False)\n",
            "                    \n",
            "                    # Store state-action pair\n",
            "                    episode_data.append({\n",
            "                        'agent': agent_name,\n",
            "                        'observation': obs.copy(),\n",
            "                        'action': action.copy()\n",
            "                    })\n",
            "                    \n",
            "                    # Execute action\n",
            "                    env.step(action)\n",
            "                    \n",
            "                    if env.terminations.get(agent_name, False) or env.truncations.get(agent_name, False):\n",
            "                        break\n",
            "        \n",
            "        demonstrations.append(episode_data)\n",
            "        total_goals['team_0'] += env.scores[0]\n",
            "        total_goals['team_1'] += env.scores[1]\n",
            "        \n",
            "        if (episode + 1) % 10 == 0:\n",
            "            print(f\"Episode {episode + 1}: Scores = {env.scores}\")\n",
            "    \n",
            "    env.close()\n",
            "    \n",
            "    print(f\"\\n‚úÖ Collected {num_episodes} expert demonstrations\")\n",
            "    print(f\"   Total goals: Team 0 = {total_goals['team_0']}, Team 1 = {total_goals['team_1']}\")\n",
            "    print(f\"   Average goals per episode: {(total_goals['team_0'] + total_goals['team_1']) / num_episodes:.2f}\")\n",
            "    \n",
            "    return demonstrations\n",
            "\n",
            "# Collect demonstrations\n",
            "expert_demonstrations = collect_expert_demonstrations(50)\n",
            "print(f\"\\nüì¶ Demonstration data size: {len(expert_demonstrations)} episodes\")\n",
            "print(f\"   First episode length: {len(expert_demonstrations[0])} steps\")"
        ]
    })
    
    # Behavioral cloning implementation
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "bc_header"},
        "source": ["### üß† Ë°åÂãï„ÇØ„É≠„Éº„Éã„É≥„Ç∞ÔºàBehavioral CloningÔºâ„ÅÆÂÆüË£Ö"]
    })
    
    cells.append({
        "cell_type": "code",
        "metadata": {"id": "behavioral_cloning"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "class BehavioralCloningAgent(BaseAgent):\n",
            "    \"\"\"Agent trained with behavioral cloning from expert demonstrations\"\"\"\n",
            "    \n",
            "    def __init__(self, agent_id: int, obs_dim: int = 28, action_dim: int = 5,\n",
            "                 hidden_dims: Tuple[int, ...] = (256, 128), lr: float = 1e-3):\n",
            "        super().__init__(agent_id, action_dim)\n",
            "        \n",
            "        self.obs_dim = obs_dim\n",
            "        self.action_dim = action_dim\n",
            "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "        \n",
            "        # Policy network (observation -> action)\n",
            "        self.policy_network = self._build_network(obs_dim, action_dim, hidden_dims)\n",
            "        self.policy_network.to(self.device)\n",
            "        \n",
            "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
            "        self.loss_history = []\n",
            "        \n",
            "    def _build_network(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n",
            "        \"\"\"Build neural network\"\"\"\n",
            "        layers = []\n",
            "        prev_dim = input_dim\n",
            "        \n",
            "        for hidden_dim in hidden_dims:\n",
            "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
            "            layers.append(nn.ReLU())\n",
            "            layers.append(nn.Dropout(0.1))  # Add dropout for regularization\n",
            "            prev_dim = hidden_dim\n",
            "        \n",
            "        layers.append(nn.Linear(prev_dim, output_dim))\n",
            "        layers.append(nn.Tanh())  # Output in [-1, 1]\n",
            "        \n",
            "        return nn.Sequential(*layers)\n",
            "    \n",
            "    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n",
            "        \"\"\"Select action using learned policy\"\"\"\n",
            "        with torch.no_grad():\n",
            "            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
            "            action = self.policy_network(obs_tensor).cpu().numpy().flatten()\n",
            "        \n",
            "        # Add exploration noise during training\n",
            "        if training:\n",
            "            noise = np.random.normal(0, 0.1, size=action.shape)\n",
            "            action = action + noise\n",
            "        \n",
            "        # Ensure kick power is positive\n",
            "        action[2] = np.clip(action[2], 0, 1)\n",
            "        \n",
            "        return np.clip(action, [-1, -1, 0, -1, -1], [1, 1, 1, 1, 1])\n",
            "    \n",
            "    def train_on_demonstrations(self, demonstrations: List, epochs: int = 100, batch_size: int = 64):\n",
            "        \"\"\"Train the agent on expert demonstrations\"\"\"\n",
            "        print(f\"\\nüß† Training behavioral cloning agent...\")\n",
            "        print(f\"   Epochs: {epochs}, Batch size: {batch_size}\")\n",
            "        \n",
            "        # Prepare training data\n",
            "        all_observations = []\n",
            "        all_actions = []\n",
            "        \n",
            "        for episode in demonstrations:\n",
            "            for step_data in episode:\n",
            "                if step_data['agent'] == f'player_{self.agent_id}':\n",
            "                    all_observations.append(step_data['observation'])\n",
            "                    all_actions.append(step_data['action'])\n",
            "        \n",
            "        # Convert to tensors\n",
            "        observations = torch.FloatTensor(all_observations).to(self.device)\n",
            "        actions = torch.FloatTensor(all_actions).to(self.device)\n",
            "        \n",
            "        dataset_size = observations.shape[0]\n",
            "        print(f\"   Training on {dataset_size} samples\")\n",
            "        \n",
            "        # Training loop\n",
            "        for epoch in range(epochs):\n",
            "            # Shuffle data\n",
            "            indices = torch.randperm(dataset_size)\n",
            "            \n",
            "            total_loss = 0\n",
            "            num_batches = 0\n",
            "            \n",
            "            for i in range(0, dataset_size, batch_size):\n",
            "                batch_indices = indices[i:i+batch_size]\n",
            "                batch_obs = observations[batch_indices]\n",
            "                batch_actions = actions[batch_indices]\n",
            "                \n",
            "                # Forward pass\n",
            "                predicted_actions = self.policy_network(batch_obs)\n",
            "                \n",
            "                # Compute loss (MSE)\n",
            "                loss = nn.MSELoss()(predicted_actions, batch_actions)\n",
            "                \n",
            "                # Backward pass\n",
            "                self.optimizer.zero_grad()\n",
            "                loss.backward()\n",
            "                torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
            "                self.optimizer.step()\n",
            "                \n",
            "                total_loss += loss.item()\n",
            "                num_batches += 1\n",
            "            \n",
            "            avg_loss = total_loss / num_batches\n",
            "            self.loss_history.append(avg_loss)\n",
            "            \n",
            "            if (epoch + 1) % 20 == 0:\n",
            "                print(f\"   Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
            "        \n",
            "        print(f\"\\n‚úÖ Behavioral cloning training completed!\")\n",
            "        print(f\"   Final loss: {self.loss_history[-1]:.4f}\")\n",
            "        \n",
            "    def learn(self, experiences: List) -> Dict[str, float]:\n",
            "        \"\"\"Can continue learning during deployment\"\"\"\n",
            "        return {\"loss\": self.loss_history[-1] if self.loss_history else 0.0}\n",
            "\n",
            "print(\"‚úÖ Behavioral cloning agent implemented!\")"
        ]
    })
    
    # Train BC agents
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "train_bc_header"},
        "source": ["### üéì BC„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆË®ìÁ∑¥"]
    })
    
    cells.append({
        "cell_type": "code",
        "metadata": {"id": "train_bc_agents"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# Create and train BC agents\n",
            "print(\"üéì Creating and training Behavioral Cloning agents...\")\n",
            "print(\"=\" * 60)\n",
            "\n",
            "bc_agents = {}\n",
            "\n",
            "for i in range(4):  # 4 agents total\n",
            "    agent_name = f\"player_{i}\"\n",
            "    print(f\"\\nTraining {agent_name}...\")\n",
            "    \n",
            "    # Create BC agent\n",
            "    bc_agent = BehavioralCloningAgent(\n",
            "        agent_id=i,\n",
            "        obs_dim=28,\n",
            "        action_dim=5,\n",
            "        hidden_dims=(256, 128),\n",
            "        lr=5e-4\n",
            "    )\n",
            "    \n",
            "    # Train on expert demonstrations\n",
            "    bc_agent.train_on_demonstrations(\n",
            "        expert_demonstrations,\n",
            "        epochs=50,\n",
            "        batch_size=32\n",
            "    )\n",
            "    \n",
            "    bc_agents[agent_name] = bc_agent\n",
            "\n",
            "print(\"\\n\" + \"=\" * 60)\n",
            "print(\"‚úÖ All BC agents trained successfully!\")\n",
            "\n",
            "# Plot training loss\n",
            "plt.figure(figsize=(10, 4))\n",
            "for i, (name, agent) in enumerate(bc_agents.items()):\n",
            "    plt.plot(agent.loss_history, label=name, alpha=0.7)\n",
            "plt.title('Behavioral Cloning Training Loss')\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('MSE Loss')\n",
            "plt.legend()\n",
            "plt.grid(True, alpha=0.3)\n",
            "plt.show()"
        ]
    })
    
    # Test BC agents
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "test_bc_header"},
        "source": ["### üéÆ BC„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆ„ÉÜ„Çπ„Éà„Å®Ë©ï‰æ°"]
    })
    
    cells.append({
        "cell_type": "code",
        "metadata": {"id": "test_bc_agents"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "def evaluate_agents(agents_dict, num_episodes=20, record_video=False):\n",
            "    \"\"\"Evaluate agent performance\"\"\"\n",
            "    config = SoccerEnvironmentConfig()\n",
            "    config.MAX_STEPS = 300\n",
            "    \n",
            "    render_mode = \"rgb_array\" if record_video else None\n",
            "    env = make_soccer_env(config, render_mode=render_mode, action_type=\"continuous\")\n",
            "    \n",
            "    stats = {\n",
            "        'scores': [],\n",
            "        'rewards': [],\n",
            "        'steps': [],\n",
            "        'goals_team_0': 0,\n",
            "        'goals_team_1': 0\n",
            "    }\n",
            "    \n",
            "    video_frames = []\n",
            "    \n",
            "    for episode in range(num_episodes):\n",
            "        env.reset()\n",
            "        episode_reward = 0\n",
            "        steps = 0\n",
            "        episode_frames = [] if record_video and episode == 0 else None\n",
            "        \n",
            "        while not all(env.terminations.values()) and not all(env.truncations.values()):\n",
            "            if episode_frames is not None:\n",
            "                frame = env.render()\n",
            "                if frame is not None:\n",
            "                    episode_frames.append(frame)\n",
            "            \n",
            "            for agent_name in env.agents:\n",
            "                if not env.terminations.get(agent_name, False) and not env.truncations.get(agent_name, False):\n",
            "                    obs = env.observe(agent_name)\n",
            "                    action = agents_dict[agent_name].select_action(obs, training=False)\n",
            "                    env.step(action)\n",
            "                    episode_reward += env.rewards.get(agent_name, 0)\n",
            "                    steps += 1\n",
            "                    \n",
            "                    if env.terminations.get(agent_name, False) or env.truncations.get(agent_name, False):\n",
            "                        break\n",
            "        \n",
            "        stats['scores'].append(env.scores.copy())\n",
            "        stats['rewards'].append(episode_reward)\n",
            "        stats['steps'].append(steps)\n",
            "        stats['goals_team_0'] += env.scores[0]\n",
            "        stats['goals_team_1'] += env.scores[1]\n",
            "        \n",
            "        if episode_frames:\n",
            "            video_frames = episode_frames\n",
            "        \n",
            "        if (episode + 1) % 5 == 0:\n",
            "            print(f\"Episode {episode + 1}: Scores = {env.scores}, Reward = {episode_reward:.2f}\")\n",
            "    \n",
            "    env.close()\n",
            "    \n",
            "    return stats, video_frames\n",
            "\n",
            "# Test BC agents\n",
            "print(\"üéÆ Testing Behavioral Cloning agents...\")\n",
            "print(\"=\" * 60)\n",
            "bc_stats, bc_video = evaluate_agents(bc_agents, num_episodes=20, record_video=True)\n",
            "\n",
            "print(\"\\nüìä BC Agents Performance:\")\n",
            "print(f\"   Total goals scored: Team 0 = {bc_stats['goals_team_0']}, Team 1 = {bc_stats['goals_team_1']}\")\n",
            "print(f\"   Average goals per episode: {(bc_stats['goals_team_0'] + bc_stats['goals_team_1']) / 20:.2f}\")\n",
            "print(f\"   Average reward: {np.mean(bc_stats['rewards']):.2f}\")\n",
            "print(f\"   Average steps: {np.mean(bc_stats['steps']):.1f}\")"
        ]
    })
    
    # Compare different agent types
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "compare_header"},
        "source": ["### üìà „Ç®„Éº„Ç∏„Çß„É≥„ÉàÊÄßËÉΩ„ÅÆÊØîËºÉ"]
    })
    
    cells.append({
        "cell_type": "code",
        "metadata": {"id": "compare_agents"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# Compare different agent types\n",
            "print(\"üìà Comparing agent performances...\")\n",
            "print(\"=\" * 60)\n",
            "\n",
            "# Test random agents for comparison\n",
            "config = SoccerEnvironmentConfig()\n",
            "env_temp = make_soccer_env(config, render_mode=None, action_type=\"continuous\")\n",
            "random_agents_comp = {}\n",
            "for i, agent_name in enumerate(env_temp.agents):\n",
            "    random_agents_comp[agent_name] = RandomAgent(i, action_space_size=5, action_type=\"continuous\")\n",
            "env_temp.close()\n",
            "\n",
            "print(\"\\nTesting Random agents...\")\n",
            "random_stats, _ = evaluate_agents(random_agents_comp, num_episodes=20, record_video=False)\n",
            "\n",
            "# Test expert agents\n",
            "expert_agents_comp = {}\n",
            "for i in range(4):\n",
            "    agent_name = f\"player_{i}\"\n",
            "    team = i // 2\n",
            "    expert_agents_comp[agent_name] = ExpertAgent(i, team, config)\n",
            "\n",
            "print(\"\\nTesting Expert agents...\")\n",
            "expert_stats, expert_video = evaluate_agents(expert_agents_comp, num_episodes=20, record_video=True)\n",
            "\n",
            "# Visualization\n",
            "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
            "\n",
            "# Goals comparison\n",
            "ax = axes[0]\n",
            "agent_types = ['Random', 'BC (Learned)', 'Expert']\n",
            "goals_team0 = [random_stats['goals_team_0'], bc_stats['goals_team_0'], expert_stats['goals_team_0']]\n",
            "goals_team1 = [random_stats['goals_team_1'], bc_stats['goals_team_1'], expert_stats['goals_team_1']]\n",
            "\n",
            "x = np.arange(len(agent_types))\n",
            "width = 0.35\n",
            "ax.bar(x - width/2, goals_team0, width, label='Team 0 (Blue)', color='blue', alpha=0.7)\n",
            "ax.bar(x + width/2, goals_team1, width, label='Team 1 (Red)', color='red', alpha=0.7)\n",
            "ax.set_xlabel('Agent Type')\n",
            "ax.set_ylabel('Total Goals (20 episodes)')\n",
            "ax.set_title('Goals Scored Comparison')\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(agent_types)\n",
            "ax.legend()\n",
            "ax.grid(True, alpha=0.3, axis='y')\n",
            "\n",
            "# Average rewards\n",
            "ax = axes[1]\n",
            "avg_rewards = [\n",
            "    np.mean(random_stats['rewards']),\n",
            "    np.mean(bc_stats['rewards']),\n",
            "    np.mean(expert_stats['rewards'])\n",
            "]\n",
            "bars = ax.bar(agent_types, avg_rewards, color=['gray', 'green', 'gold'], alpha=0.7)\n",
            "ax.set_ylabel('Average Reward per Episode')\n",
            "ax.set_title('Reward Comparison')\n",
            "ax.grid(True, alpha=0.3, axis='y')\n",
            "\n",
            "# Add value labels on bars\n",
            "for bar, val in zip(bars, avg_rewards):\n",
            "    height = bar.get_height()\n",
            "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
            "            f'{val:.1f}', ha='center', va='bottom')\n",
            "\n",
            "# Goals per episode distribution\n",
            "ax = axes[2]\n",
            "total_goals = [\n",
            "    (random_stats['goals_team_0'] + random_stats['goals_team_1']) / 20,\n",
            "    (bc_stats['goals_team_0'] + bc_stats['goals_team_1']) / 20,\n",
            "    (expert_stats['goals_team_0'] + expert_stats['goals_team_1']) / 20\n",
            "]\n",
            "bars = ax.bar(agent_types, total_goals, color=['gray', 'green', 'gold'], alpha=0.7)\n",
            "ax.set_ylabel('Average Goals per Episode')\n",
            "ax.set_title('Scoring Frequency')\n",
            "ax.grid(True, alpha=0.3, axis='y')\n",
            "\n",
            "# Add value labels\n",
            "for bar, val in zip(bars, total_goals):\n",
            "    height = bar.get_height()\n",
            "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
            "            f'{val:.2f}', ha='center', va='bottom')\n",
            "\n",
            "plt.suptitle('Agent Performance Comparison', fontsize=16, fontweight='bold')\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "print(\"\\n\" + \"=\" * 60)\n",
            "print(\"üìä Summary:\")\n",
            "print(f\"Random - Goals/episode: {total_goals[0]:.2f}, Avg reward: {avg_rewards[0]:.1f}\")\n",
            "print(f\"BC     - Goals/episode: {total_goals[1]:.2f}, Avg reward: {avg_rewards[1]:.1f}\")\n",
            "print(f\"Expert - Goals/episode: {total_goals[2]:.2f}, Avg reward: {avg_rewards[2]:.1f}\")\n",
            "\n",
            "improvement = (total_goals[1] - total_goals[0]) / (total_goals[0] + 0.01) * 100\n",
            "print(f\"\\nüéØ BC improvement over Random: {improvement:.1f}%\")"
        ]
    })
    
    # Create comparison videos
    cells.append({
        "cell_type": "markdown",
        "metadata": {"id": "videos_header"},
        "source": ["### üé¨ ÂãïÁîª„Åß„ÅÆÊØîËºÉ"]
    })
    
    cells.append({
        "cell_type": "code",
        "metadata": {"id": "create_comparison_videos"},
        "execution_count": None,
        "outputs": [],
        "source": [
            "# Create and display comparison videos\n",
            "print(\"üé¨ Creating comparison videos...\")\n",
            "\n",
            "if bc_video:\n",
            "    # Create BC agent video\n",
            "    bc_video_path = '/tmp/bc_agents_gameplay.mp4'\n",
            "    create_video_from_frames(bc_video, bc_video_path, fps=30)\n",
            "    print(\"‚úÖ BC agents video created\")\n",
            "    \n",
            "    print(\"\\nüì∫ Behavioral Cloning Agents Gameplay:\")\n",
            "    display(display_video(bc_video_path))\n",
            "\n",
            "if expert_video:\n",
            "    # Create expert agent video\n",
            "    expert_video_path = '/tmp/expert_agents_gameplay.mp4'\n",
            "    create_video_from_frames(expert_video, expert_video_path, fps=30)\n",
            "    print(\"\\n‚úÖ Expert agents video created\")\n",
            "    \n",
            "    print(\"\\nüì∫ Expert Agents Gameplay:\")\n",
            "    display(display_video(expert_video_path))\n",
            "\n",
            "print(\"\\nüí° Ë¶≥ÂØü„Éù„Ç§„É≥„Éà:\")\n",
            "print(\"  - Expert: „Éú„Éº„É´„Å´Âêë„Åã„Å£„Å¶Á©çÊ•µÁöÑ„Å´ÁßªÂãï„Åó„ÄÅ„Ç¥„Éº„É´„ÇíÁãô„ÅÜ\")\n",
            "print(\"  - BC: Expert„ÅÆÊà¶Áï•„ÇíÊ®°ÂÄ£„Åó„ÄÅ„Çà„ÇäÂ§ö„Åè„ÅÆ„Ç¥„Éº„É´„ÇíÊ±∫„ÇÅ„Çã\")\n",
            "print(\"  - Random: „É©„É≥„ÉÄ„É†„Å™Âãï„Åç„Åß„ÄÅ„Ç¥„Éº„É´„ÅØ„Åª„Å®„Çì„Å©ÂÖ•„Çâ„Å™„ÅÑ\")"
        ]
    })
    
    return cells

# Create the notebook with expert learning
if __name__ == "__main__":
    # Load the extended notebook
    with open("/home/user/webapp/multiagents_soccer_extended_fixed.ipynb", "r", encoding="utf-8") as f:
        notebook = json.load(f)
    
    # Add expert learning cells
    new_cells = create_expert_learning_cells()
    notebook['cells'].extend(new_cells)
    
    # Save as new notebook
    with open("/home/user/webapp/multiagents_soccer_expert.ipynb", "w", encoding="utf-8") as f:
        json.dump(notebook, f, indent=2, ensure_ascii=False)
    
    print("‚úÖ Expert learning notebook created!")
    print("üìÅ Saved as: multiagents_soccer_expert.ipynb")
    print("\nüéØ Êñ∞Ê©üËÉΩ:")
    print("  1. ExpertAgent: „Éí„É•„Éº„É™„Çπ„ÉÜ„Ç£„ÉÉ„ÇØ„Éô„Éº„Çπ„ÅÆÊà¶Áï•„Ç®„Éº„Ç∏„Çß„É≥„Éà")
    print("  2. Ê®°ÂÄ£Â≠¶Áøí: „Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆË°åÂãï„Çí„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÅßÂ≠¶Áøí")
    print("  3. Ë°åÂãï„ÇØ„É≠„Éº„Éã„É≥„Ç∞: „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éá„É¢„Åã„ÇâÁõ¥Êé•Â≠¶Áøí")
    print("  4. ÊÄßËÉΩÊØîËºÉ: Random vs BC vs Expert")
    print("  5. „Ç¥„Éº„É´Êï∞„ÅÆÂ§ßÂπÖÊîπÂñÑÔºàÊúüÂæÖÂÄ§: 0.1‚Üí2.0 goals/episodeÔºâ")